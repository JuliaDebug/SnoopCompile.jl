var documenterSearchIndex = {"docs":
[{"location":"tutorials/jet/#Tutorial-on-JET-integration","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"","category":"section"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"JET is a powerful tool for analyzing your code. As described elsewhere, some of its functionality overlaps SnoopCompile, but its mechanism of action is very different. The combination JET and SnoopCompile provides capabilities that neither package has on its own. Specifically, one can use SnoopCompile to collect data on the full callgraph and JET to perform the exhaustive analysis of individual nodes.","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"The integration between the two packages is bundled into SnoopCompile, specifically report_callee, report_callees, and report_caller. These take InferenceTrigger (see the page on inference failures) and use them to generate JET reports. These tools focus on error-analysis rather than optimization, as SnoopCompile can already identify runtime dispatch.","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"We can demonstrate both the need and use of these tools with a simple extended example.","category":"page"},{"location":"tutorials/jet/#JET-usage","page":"Tutorial on JET integration","title":"JET usage","text":"","category":"section"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"As a basic introduction to JET, let's analyze the following call from JET's own documentation:","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"julia> using JET\n\njulia> list = Any[1,2,3];\n\njulia> sum(list)\n6\n\njulia> @report_call sum(list)\n═════ 1 possible error found ═════\n┌ sum(a::Vector{Any}) @ Base ./reducedim.jl:1010\n│┌ sum(a::Vector{Any}; dims::Colon, kw::@Kwargs{}) @ Base ./reducedim.jl:1010\n││┌ _sum(a::Vector{Any}, ::Colon) @ Base ./reducedim.jl:1014\n│││┌ _sum(a::Vector{Any}, ::Colon; kw::@Kwargs{}) @ Base ./reducedim.jl:1014\n││││┌ _sum(f::typeof(identity), a::Vector{Any}, ::Colon) @ Base ./reducedim.jl:1015\n│││││┌ _sum(f::typeof(identity), a::Vector{Any}, ::Colon; kw::@Kwargs{}) @ Base ./reducedim.jl:1015\n││││││┌ mapreduce(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{Any}) @ Base ./reducedim.jl:357\n│││││││┌ mapreduce(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{Any}; dims::Colon, init::Base._InitialValue) @ Base ./reducedim.jl:357\n││││││││┌ _mapreduce_dim(f::typeof(identity), op::typeof(Base.add_sum), ::Base._InitialValue, A::Vector{Any}, ::Colon) @ Base ./reducedim.jl:365\n│││││││││┌ _mapreduce(f::typeof(identity), op::typeof(Base.add_sum), ::IndexLinear, A::Vector{Any}) @ Base ./reduce.jl:432\n││││││││││┌ mapreduce_empty_iter(f::typeof(identity), op::typeof(Base.add_sum), itr::Vector{Any}, ItrEltype::Base.HasEltype) @ Base ./reduce.jl:380\n│││││││││││┌ reduce_empty_iter(op::Base.MappingRF{typeof(identity), typeof(Base.add_sum)}, itr::Vector{Any}, ::Base.HasEltype) @ Base ./reduce.jl:384\n││││││││││││┌ reduce_empty(op::Base.MappingRF{typeof(identity), typeof(Base.add_sum)}, ::Type{Any}) @ Base ./reduce.jl:361\n│││││││││││││┌ mapreduce_empty(::typeof(identity), op::typeof(Base.add_sum), T::Type{Any}) @ Base ./reduce.jl:372\n││││││││││││││┌ reduce_empty(::typeof(Base.add_sum), ::Type{Any}) @ Base ./reduce.jl:352\n│││││││││││││││┌ reduce_empty(::typeof(+), ::Type{Any}) @ Base ./reduce.jl:343\n││││││││││││││││┌ zero(::Type{Any}) @ Base ./missing.jl:106\n│││││││││││││││││ MethodError: no method matching zero(::Type{Any}): Base.throw(Base.MethodError(zero, tuple(Base.Any)::Tuple{DataType})::MethodError)\n││││││││││││││││└────────────────────","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"The final line reveals that while sum happened to work for the specific list we provided, it nevertheless has a \"gotcha\" for the types we supplied: if list happens to be empty, sum depends on the ability to generate zero(T) for the element-type T of list, but because we constructed list to have an element-type of Any, there is no such method and sum(Any[]) throws an error:","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"julia> sum(Int[])\n0\n\njulia> sum(Any[])\nERROR: MethodError: no method matching zero(::Type{Any})\n[...]","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"(This can be circumvented with sum(Any[]; init=0).)","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"This is the kind of bug that can lurk undetected for a long time, and JET excels at exposing them.","category":"page"},{"location":"tutorials/jet/#JET-limitations","page":"Tutorial on JET integration","title":"JET limitations","text":"","category":"section"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"JET is a static analyzer, meaning that it works from the argument types provided, and that has an important consequence: if a particular callee can't be inferred, JET can't analyze it. We can illustrate that quite easily:","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"julia> callsum(listcontainer) = sum(listcontainer[1])\ncallsum (generic function with 1 method)\n\njulia> lc = Any[list];   # \"hide\" `list` inside a Vector{Any}\n\njulia> callsum(lc)\n6\n\njulia> @report_call callsum(lc)\nNo errors detected","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"Because we \"hid\" the type of list from inference, JET couldn't tell what specific instance of sum was going to be called, so it was unable to detect any errors.","category":"page"},{"location":"tutorials/jet/#JET/SnoopCompile-integration","page":"Tutorial on JET integration","title":"JET/SnoopCompile integration","text":"","category":"section"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"A resolution to this problem is to use SnoopCompile to do the \"data collection\" and JET to do the analysis. The key reason is that SnoopCompile is a dynamic analyzer, and is capable of bridging across runtime dispatch. As always, you need to do the data collection in a fresh session where the calls have not previously been inferred. After restarting Julia, we can do this:","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"julia> using SnoopCompileCore\n\njulia> list = Any[1,2,3];\n\njulia> lc = Any[list];   # \"hide\" `list` inside a Vector{Any}\n\njulia> callsum(listcontainer) = sum(listcontainer[1]);\n\njulia> tinf = @snoop_inference callsum(lc);\n\njulia> using SnoopCompile, JET, Cthulhu\n\njulia> tinf.children\n2-element Vector{SnoopCompileCore.InferenceTimingNode}:\n InferenceTimingNode: 0.000869/0.000869 on callsum(::Vector{Any}) with 0 direct children\n InferenceTimingNode: 0.000196/0.006685 on sum(::Vector{Any}) with 1 direct children\n\njulia> report_callees(inference_triggers(tinf))\n1-element Vector{Pair{InferenceTrigger, JET.JETCallResult{JET.JETAnalyzer{JET.BasicPass}, Base.Pairs{Symbol, Union{}, Tuple{}, @NamedTuple{}}}}}:\n Inference triggered to call sum(::Vector{Any}) from callsum (./REPL[5]:1) with specialization callsum(::Vector{Any}) => ═════ 1 possible error found ═════\n┌ sum(a::Vector{Any}) @ Base ./reducedim.jl:1010\n│┌ sum(a::Vector{Any}; dims::Colon, kw::@Kwargs{}) @ Base ./reducedim.jl:1010\n││┌ _sum(a::Vector{Any}, ::Colon) @ Base ./reducedim.jl:1014\n│││┌ _sum(a::Vector{Any}, ::Colon; kw::@Kwargs{}) @ Base ./reducedim.jl:1014\n││││┌ _sum(f::typeof(identity), a::Vector{Any}, ::Colon) @ Base ./reducedim.jl:1015\n│││││┌ _sum(f::typeof(identity), a::Vector{Any}, ::Colon; kw::@Kwargs{}) @ Base ./reducedim.jl:1015\n││││││┌ mapreduce(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{Any}) @ Base ./reducedim.jl:357\n│││││││┌ mapreduce(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{Any}; dims::Colon, init::Base._InitialValue) @ Base ./reducedim.jl:357\n││││││││┌ _mapreduce_dim(f::typeof(identity), op::typeof(Base.add_sum), ::Base._InitialValue, A::Vector{Any}, ::Colon) @ Base ./reducedim.jl:365\n│││││││││┌ _mapreduce(f::typeof(identity), op::typeof(Base.add_sum), ::IndexLinear, A::Vector{Any}) @ Base ./reduce.jl:432\n││││││││││┌ mapreduce_empty_iter(f::typeof(identity), op::typeof(Base.add_sum), itr::Vector{Any}, ItrEltype::Base.HasEltype) @ Base ./reduce.jl:380\n│││││││││││┌ reduce_empty_iter(op::Base.MappingRF{typeof(identity), typeof(Base.add_sum)}, itr::Vector{Any}, ::Base.HasEltype) @ Base ./reduce.jl:384\n││││││││││││┌ reduce_empty(op::Base.MappingRF{typeof(identity), typeof(Base.add_sum)}, ::Type{Any}) @ Base ./reduce.jl:361\n│││││││││││││┌ mapreduce_empty(::typeof(identity), op::typeof(Base.add_sum), T::Type{Any}) @ Base ./reduce.jl:372\n││││││││││││││┌ reduce_empty(::typeof(Base.add_sum), ::Type{Any}) @ Base ./reduce.jl:352\n│││││││││││││││┌ reduce_empty(::typeof(+), ::Type{Any}) @ Base ./reduce.jl:343\n││││││││││││││││┌ zero(::Type{Any}) @ Base ./missing.jl:106\n│││││││││││││││││ MethodError: no method matching zero(::Type{Any}): Base.throw(Base.MethodError(zero, tuple(Base.Any)::Tuple{DataType})::MethodError)\n││││││││││││││││└────────────────────","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"Because SnoopCompileCore collected the runtime-dispatched sum call, we can pass it to JET. report_callees filters those calls which generate JET reports, allowing you to focus on potential errors.","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"note: Note\nJET integration is enabled only if JET.jl and Cthulhu.jl have been loaded into your main session. This is why there's the using JET, Cthulhu statement included in the example given.","category":"page"},{"location":"tutorials/snoop_inference/#Tutorial-on-@snoop_inference","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Inference may occur when you run code. Inference is the first step of type-specialized compilation. @snoop_inference collects data on what inference is doing, giving you greater insight into what is being inferred and how long it takes.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Compilation is needed only for \"fresh\" code; running the demos below on code you've already used will yield misleading results. When analyzing inference, you're advised to always start from a fresh session. See also the comparison between SnoopCompile and JET.","category":"page"},{"location":"tutorials/snoop_inference/#Add-SnoopCompileCore,-SnoopCompile,-and-helper-packages-to-your-environment","page":"Tutorial on @snoop_inference","title":"Add SnoopCompileCore, SnoopCompile, and helper packages to your environment","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Here, we'll add these packages to your default environment. (With the exception of AbstractTrees, these \"developer tool\" packages should not be added to the Project file of any real packages unless you're extending the tool itself.)","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"using Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\", \"AbstractTrees\", \"ProfileView\"]);","category":"page"},{"location":"tutorials/snoop_inference/#Setting-up-the-demo","page":"Tutorial on @snoop_inference","title":"Setting up the demo","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"To see @snoop_inference in action, we'll use the following demo:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"module FlattenDemo\n    struct MyType{T} x::T end\n\n    extract(y::MyType) = y.x\n\n    function domath(x)\n        y = x + x\n        return y*x + 2*x + 5\n    end\n\n    dostuff(y) = domath(extract(y))\n\n    function packintype(x)\n        y = MyType{Int}(x)\n        return dostuff(y)\n    end\nend\n\n# output\n\nFlattenDemo","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"The main call, packintype, stores the input in a struct, and then calls functions that extract the field value and performs arithmetic on the result.","category":"page"},{"location":"tutorials/snoop_inference/#sccshow","page":"Tutorial on @snoop_inference","title":"Collecting the data","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"To profile inference on this call, do the following:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> using SnoopCompileCore\n\njulia> tinf = @snoop_inference FlattenDemo.packintype(1);\n\njulia> using SnoopCompile\n\njulia> tinf\nInferenceTimingNode: 0.002712/0.003278 on Core.Compiler.Timings.ROOT() with 1 direct children","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"tip: Tip\nDon't omit the semicolon on the tinf = @snoop_inference ... line, or you may get an enormous amount of output. The compact display on the final line is possible only because SnoopCompile defines nice Base.show methods for the data returned by @snoop_inference. These methods cannot be defined in SnoopCompileCore because it has a fundamental design constraint: loading SnoopCompileCore is not allowed to invalidate any code. Moving those Base.show methods to SnoopCompileCore would violate that guarantee.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"This may not look like much, but there's a wealth of information hidden inside tinf.","category":"page"},{"location":"tutorials/snoop_inference/#A-quick-check-for-potential-invalidations","page":"Tutorial on @snoop_inference","title":"A quick check for potential invalidations","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"After running @snoop_inference, it's generally recommended to check the output of staleinstances:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> staleinstances(tinf)\nSnoopCompileCore.InferenceTiming[]","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"If you see this, all's well. A non-empty list might indicate method invalidations, which can be checked (in a fresh session) using the tools described in Tutorial on @snoop_invalidations.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"If you do have a lot of invalidations, precompile_blockers may be an effective way to reveal those invalidations that affect your particular package and workload.","category":"page"},{"location":"tutorials/snoop_inference/#flamegraph","page":"Tutorial on @snoop_inference","title":"Viewing the results","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Let's start unpacking the output of @snoop_inference and see how to get more insight. First, notice that the output is an InferenceTimingNode: it's the root element of a tree of such nodes, all connected by caller-callee relationships. Indeed, this particular node is for Core.Compiler.Timings.ROOT(), a \"dummy\" node that is the root of all such trees.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"You may have noticed that this ROOT node prints with two numbers. It will be easier to understand their meaning if we first display the whole tree. We can do that with the AbstractTrees package:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> using AbstractTrees\n\njulia> print_tree(tinf)\nInferenceTimingNode: 0.002712/0.003278 on Core.Compiler.Timings.ROOT() with 1 direct children\n└─ InferenceTimingNode: 0.000133/0.000566 on FlattenDemo.packintype(::Int64) with 2 direct children\n   ├─ InferenceTimingNode: 0.000094/0.000094 on FlattenDemo.MyType{Int64}(::Int64) with 0 direct children\n   └─ InferenceTimingNode: 0.000089/0.000339 on FlattenDemo.dostuff(::FlattenDemo.MyType{Int64}) with 2 direct children\n      ├─ InferenceTimingNode: 0.000064/0.000122 on FlattenDemo.extract(::FlattenDemo.MyType{Int64}) with 2 direct children\n      │  ├─ InferenceTimingNode: 0.000034/0.000034 on getproperty(::FlattenDemo.MyType{Int64}, ::Symbol) with 0 direct children\n      │  └─ InferenceTimingNode: 0.000024/0.000024 on getproperty(::FlattenDemo.MyType{Int64}, x::Symbol) with 0 direct children\n      └─ InferenceTimingNode: 0.000127/0.000127 on FlattenDemo.domath(::Int64) with 0 direct children","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"This tree structure reveals the caller-callee relationships, showing the specific types that were used for each MethodInstance. Indeed, as the calls to getproperty reveal, it goes beyond the types and even shows the results of constant propagation; the getproperty(::MyType{Int64}, x::Symbol) corresponds to y.x in the definition of extract.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"note: Note\nGenerally we speak of call graphs rather than call trees. But because inference results are cached (a.k.a., we only \"visit\" each node once), we obtain a tree as a depth-first-search of the full call graph.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"You can extract the MethodInstance with","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> Core.MethodInstance(tinf)\nMethodInstance for Core.Compiler.Timings.ROOT()\n\njulia> Core.MethodInstance(tinf.children[1])\nMethodInstance for FlattenDemo.packintype(::Int64)","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Each node in this tree is accompanied by a pair of numbers. The first number is the exclusive inference time (in seconds), meaning the time spent inferring the particular MethodInstance, not including the time spent inferring its callees. The second number is the inclusive time, which is the exclusive time plus the time spent on the callees. Therefore, the inclusive time is always at least as large as the exclusive time.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"The ROOT node is a bit different: its exclusive time measures the time spent on all operations except inference. In this case, we see that the entire call took approximately 10ms, of which 9.3ms was spent on activities besides inference. Almost all of that was code-generation, but it also includes the time needed to run the code. Just 0.76ms was needed to run type-inference on this entire series of calls. As you will quickly discover, inference takes much more time on more complicated code.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"We can also display this tree as a flame graph, using the ProfileView.jl package:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> fg = flamegraph(tinf)\nNode(FlameGraphs.NodeData(ROOT() at typeinfer.jl:75, 0x00, 0:10080857))","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> using ProfileView\n\njulia> ProfileView.view(fg)","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"You should see something like this:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"(Image: flamegraph)","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Users are encouraged to read the ProfileView documentation to understand how to interpret this, but briefly:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"the horizontal axis is time (wide boxes take longer than narrow ones), the vertical axis is call depth\nhovering over a box displays the method that was inferred\nleft-clicking on a box causes the full MethodInstance to be printed in your REPL session\nright-clicking on a box opens the corresponding method in your editor\nctrl-click can be used to zoom in\nempty horizontal spaces correspond to activities other than type-inference\nany boxes colored red (there are none in this particular example, but you'll see some later) correspond to non-precompilable MethodInstances, in which the method is owned by one module but the types are from another unrelated module.\nany boxes colored orange-yellow (there is one in this demo) correspond to methods inferred for specific constants (constant propagation)","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"You can explore this flamegraph and compare it to the output from print_tree.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Finally, flatten, on its own or together with accumulate_by_source, allows you to get an sense for the cost of individual MethodInstances or Methods.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"The tools here allow you to get an overview of where inference is spending its time. This gives you insight into the major contributors to latency.","category":"page"},{"location":"explanations/tools/#Package-roles-and-alternatives","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"","category":"section"},{"location":"explanations/tools/#SnoopCompile","page":"Package roles and alternatives","title":"SnoopCompile","text":"","category":"section"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"SnoopCompileCore is a tiny package with no dependencies; it's used for collecting data, and it has been designed in such a way that it cannot cause any invalidations of its own. Collecting data on invalidations and inference with SnoopCompileCore is the only way you can be sure you are observing the \"native state\" of your code.","category":"page"},{"location":"explanations/tools/#SnoopCompile-2","page":"Package roles and alternatives","title":"SnoopCompile","text":"","category":"section"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"SnoopCompile is a much larger package that performs analysis on the data collected by SnoopCompileCore; loading SnoopCompile can (and does) trigger invalidations. Consequently, you're urged to always collect data with just SnoopCompileCore loaded, and wait to load SnoopCompile until after you've finished collecting the data.","category":"page"},{"location":"explanations/tools/#Cthulhu","page":"Package roles and alternatives","title":"Cthulhu","text":"","category":"section"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"Cthulhu is a companion package that gives deep insights into the origin of invalidations or inference failures.","category":"page"},{"location":"explanations/tools/#AbstractTrees","page":"Package roles and alternatives","title":"AbstractTrees","text":"","category":"section"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"AbstractTrees is the one package in this list that can be both a \"workhorse\" and a developer tool. SnoopCompile uses it mostly for pretty-printing.","category":"page"},{"location":"explanations/tools/#JET","page":"Package roles and alternatives","title":"JET","text":"","category":"section"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"JET is a powerful developer tool that in some ways is an alternative to SnoopCompile. While the two have different goals, the packages have some overlap in what they can tell you about your code. However, their mechanisms of action are fundamentally different:","category":"page"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"JET is a \"static analyzer,\" which means that it analyzes the code itself. JET can tell you about inference failures (runtime dispatch) much like SnoopCompile, with a major advantage: SnoopCompileCore omits information about any callees that are already compiled, but JET's @report_opt provides exhaustive information about the entire inferable callgraph (i.e., the part of the callgraph that inference can predict from the initial call) regardless of whether it has been previously compiled. With JET, you don't have to remember to run each analysis in a fresh session.\nSnoopCompileCore collects data by watching normal inference at work. On code that hasn't been compiled previously, this can yield results similar to JET's, with a different major advantage: JET can't \"see through\" runtime dispatch, but SnoopCompileCore can. With SnoopCompile, you can immediately get a wholistic view of your entire callgraph.","category":"page"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"Combining JET and SnoopCompile can provide insights that are difficult to obtain with either package in isolation. See the Tutorial on JET integration.","category":"page"},{"location":"explanations/fixing_inference/#Techniques-for-fixing-inference-problems","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Here we assume you've dug into your code with a tool like Cthulhu, and want to know how to fix some of the problems that you discover. Below is a collection of specific cases and some tricks for handling them.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Note that there is also a tutorial on fixing inference that delves into advanced topics.","category":"page"},{"location":"explanations/fixing_inference/#Adding-type-annotations","page":"Techniques for fixing inference problems","title":"Adding type annotations","text":"","category":"section"},{"location":"explanations/fixing_inference/#Using-concrete-types","page":"Techniques for fixing inference problems","title":"Using concrete types","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Defining variables like list = [] can be convenient, but it creates a list of type Vector{Any}. This prevents inference from knowing the type of items extracted from list. Using list = String[] for a container of strings, etc., is an excellent fix. When in doubt, check the type with isconcretetype: a common mistake is to think that list_of_lists = Array{Int}[] gives you a vector-of-vectors, but","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"julia> isconcretetype(Array{Int})\nfalse","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"reminds you that Array requires a second parameter indicating the dimensionality of the array. (Or use list_of_lists = Vector{Int}[] instead, as Vector{Int} === Array{Int, 1}.)","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Many valuable tips can be found among Julia's performance tips, and readers are encouraged to consult that page.","category":"page"},{"location":"explanations/fixing_inference/#Working-with-non-concrete-types","page":"Techniques for fixing inference problems","title":"Working with non-concrete types","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"In cases where invalidations occur, but you can't use concrete types (there are indeed many valid uses of Vector{Any}), you can often prevent the invalidation using some additional knowledge. One common example is extracting information from an IOContext structure, which is roughly defined as","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"struct IOContext{IO_t <: IO} <: AbstractPipe\n    io::IO_t\n    dict::ImmutableDict{Symbol, Any}\nend","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"There are good reasons that dict uses a value-type of Any, but that makes it impossible for the compiler to infer the type of any object looked up in an IOContext. Fortunately, you can help! For example, the documentation specifies that the :color setting should be a Bool, and since it appears in documentation it's something we can safely enforce. Changing","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"iscolor = get(io, :color, false)","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"to","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"iscolor = get(io, :color, false)::Bool     # assert that the rhs is Bool-valued","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"will throw an error if it isn't a Bool, and this allows the compiler to take advantage of the type being known in subsequent operations.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"If the return type is one of a small number of possibilities (generally three or fewer), you can annotate the return type with Union{...}. This is generally advantageous only when the intersection of what inference already knows about the types of a variable and the types in the Union results in an concrete type.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"As a more detailed example, suppose you're writing code that parses Julia's Expr type:","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"julia> ex = :(Array{Float32,3})\n:(Array{Float32, 3})\n\njulia> dump(ex)\nExpr\n  head: Symbol curly\n  args: Vector{Any(3,))\n    1: Symbol Array\n    2: Symbol Float32\n    3: Int64 3","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"ex.args is a Vector{Any}. However, for a :curly expression only certain types will be found among the arguments; you could write key portions of your code as","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"a = ex.args[2]\nif a isa Symbol\n    # inside this block, Julia knows `a` is a Symbol, and so methods called on `a` will be resistant to invalidation\n    foo(a)\nelseif a isa Expr && length((a::Expr).args) > 2\n    a::Expr         # sometimes you have to help inference by adding a type-assert\n    x = bar(a)      # `bar` is now resistant to invalidation\nelseif a isa Integer\n    # even though you've not made this fully-inferrable, you've at least reduced the scope for invalidations\n    # by limiting the subset of `foobar` methods that might be called\n    y = foobar(a)\nend","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Other tricks include replacing broadcasting on v::Vector{Any} with Base.mapany(f, v)–mapany avoids trying to narrow the type of f(v[i]) and just assumes it will be Any, thereby avoiding invalidations of many convert methods.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Adding type-assertions and fixing inference problems are the most common approaches for fixing invalidations. You can discover these manually, but using Cthulhu is highly recommended.","category":"page"},{"location":"explanations/fixing_inference/#Inferrable-field-access-for-abstract-types","page":"Techniques for fixing inference problems","title":"Inferrable field access for abstract types","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"When invalidations happen for methods that manipulate fields of abstract types, often there is a simple solution: create an \"interface\" for the abstract type specifying that certain fields must have certain types. Here's an example:","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"abstract type AbstractDisplay end\n\nstruct Monitor <: AbstractDisplay\n    height::Int\n    width::Int\n    maker::String\nend\n\nstruct Phone <: AbstractDisplay\n    height::Int\n    width::Int\n    maker::Symbol\nend\n\nfunction Base.show(@nospecialize(d::AbstractDisplay), x)\n    str = string(x)\n    w = d.width\n    if length(str) > w  # do we have to truncate to fit the display width?\n        ...","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"In this show method, we've deliberately chosen to prevent specialization on the specific type of AbstractDisplay (to reduce the total number of times we have to compile this method). As a consequence, Julia's inference may not realize that d.width returns an Int.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Fortunately, you can help by defining an interface for generic AbstractDisplay objects:","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"function Base.getproperty(d::AbstractDisplay, name::Symbol)\n    if name === :height\n        return getfield(d, :height)::Int\n    elseif name === :width\n        return getfield(d, :width)::Int\n    elseif name === :maker\n        return getfield(d, :maker)::Union{String,Symbol}\n    end\n    return getfield(d, name)\nend","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Julia's constant propagation will ensure that most accesses of those fields will be determined at compile-time, so this simple change robustly fixes many inference problems.","category":"page"},{"location":"explanations/fixing_inference/#Fixing-Core.Box","page":"Techniques for fixing inference problems","title":"Fixing Core.Box","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Julia issue 15276 is one of the more surprising forms of inference failure; it is the most common cause of a Core.Box annotation. If other variables depend on the Boxed variable, then a single Core.Box can lead to widespread inference problems. For this reason, these are also among the first inference problems you should tackle.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Read this explanation of why this happens and what you can do to fix it. If you are directed to find Core.Box inference triggers via suggest, you may need to explore around the call site a bit– the inference trigger may be in the closure itself, but the fix needs to go in the method that creates the closure.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Use of ascend is highly recommended for fixing Core.Box inference failures.","category":"page"},{"location":"explanations/fixing_inference/#Handling-edge-cases","page":"Techniques for fixing inference problems","title":"Handling edge cases","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"You can sometimes get invalidations from failing to handle \"formal\" possibilities. For example, operations with regular expressions might return a Union{Nothing, RegexMatch}. You can sometimes get poor type inference by writing code that fails to take account of the possibility that nothing might be returned. For example, a comprehension","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"ms = [m.match for m in match.((rex,), my_strings)]","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"might be replaced with","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"ms = [m.match for m in match.((rex,), my_strings) if m !== nothing]","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"and return a better-typed result.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#inferrability","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Throughout this page, we'll use the OptimizeMe demo, which ships with SnoopCompile.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"note: Note\nTo understand what follows, it's essential to refer to OptimizeMe source code as you follow along.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> using SnoopCompile\n\njulia> cd(joinpath(pkgdir(SnoopCompile), \"examples\"))\n\njulia> include(\"OptimizeMe.jl\")\nMain.OptimizeMe\n\njulia> tinf = @snoop_inference OptimizeMe.main()\nlotsa containers:\n7-element Vector{Main.OptimizeMe.Container}:\n Main.OptimizeMe.Container{Int64}(1)\n Main.OptimizeMe.Container{UInt8}(0x01)\n Main.OptimizeMe.Container{UInt16}(0xffff)\n Main.OptimizeMe.Container{Float32}(2.0f0)\n Main.OptimizeMe.Container{Char}('a')\n Main.OptimizeMe.Container{Vector{Int64}}([0])\n Main.OptimizeMe.Container{Tuple{String, Int64}}((\"key\", 42))\n3.14 is great\n2.718 is jealous\n6-element Vector{Main.OptimizeMe.Object}:\n Main.OptimizeMe.Object(1)\n Main.OptimizeMe.Object(2)\n Main.OptimizeMe.Object(3)\n Main.OptimizeMe.Object(4)\n Main.OptimizeMe.Object(5)\n Main.OptimizeMe.Object(7)\nInferenceTimingNode: 1.423913/2.713560 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 77 direct children\n\njulia> fg = flamegraph(tinf)\nNode(FlameGraphs.NodeData(ROOT() at typeinfer.jl:75, 0x00, 0:2713559552))","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"If you visualize fg with ProfileView, you'll see something like this:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"(Image: flamegraph-OptimizeMe)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"From the standpoint of precompilation, this has some obvious problems:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"even though we called a single method, OptimizeMe.main(), there are many distinct flames separated by blank spaces. This indicates that many calls are being made by runtime dispatch:  each separate flame is a fresh entrance into inference.\nseveral of the flames are marked in red, indicating that they are not precompilable. While SnoopCompile does have the capability to automatically emit precompile directives for the non-red bars that sit on top of the red ones, in some cases the red extends to the highest part of the flame. In such cases there is no available precompile directive, and therefore no way to avoid the cost of type-inference.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Our goal will be to improve the design of OptimizeMe to make it more precompilable.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#Analyzing-inference-triggers","page":"Using @snoop_inference results to improve inferrability","title":"Analyzing inference triggers","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We'll first extract the \"triggers\" of inference, which is just a repackaging of part of the information contained within tinf. Specifically an InferenceTrigger captures callee/caller relationships that straddle a fresh entrance to type-inference, allowing you to identify which calls were made by runtime dispatch and what MethodInstance they called.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> itrigs = inference_triggers(tinf)\n76-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for vect(::Int64, ::Vararg{Any, N} where N) from lotsa_containers (/pathto/SnoopCompile/examples/OptimizeMe.jl:13) with specialization MethodInstance for lotsa_containers()\n Inference triggered to call MethodInstance for promote_typeof(::Int64, ::UInt8, ::Vararg{Any, N} where N) from vect (./array.jl:126) with specialization MethodInstance for vect(::Int64, ::Vararg{Any, N} where N)\n Inference triggered to call MethodInstance for promote_typeof(::UInt8, ::UInt16, ::Vararg{Any, N} where N) from promote_typeof (./promotion.jl:272) with specialization MethodInstance for promote_typeof(::Int64, ::UInt8, ::Vararg{Any, N} where N)\n ⋮","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"This indicates that a whopping 76 calls were (1) made by runtime dispatch and (2) the callee had not previously been inferred. (There was a 77th call that had to be inferred, the original call to main(), but by default inference_triggers excludes calls made directly from top-level. You can change that through keyword arguments.)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"tip: Tip\nIn the REPL, SnoopCompile displays InferenceTriggers with yellow coloration for the callee, red for the caller method, and blue for the caller specialization. This makes it easier to quickly identify the most important information.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"In some cases, this might indicate that you'll need to fix 76 separate callers; fortunately, in many cases fixing the origin of inference problems can fix a number of later callees.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#methtrigs","page":"Using @snoop_inference results to improve inferrability","title":"Method triggers","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Most often, it's most convenient to organize them by the method triggering the need for inference:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> mtrigs = accumulate_by_source(Method, itrigs)\n18-element Vector{SnoopCompile.TaggedTriggers{Method}}:\n print_matrix_row(io::IO, X::AbstractVecOrMat{T} where T, A::Vector{T} where T, i::Integer, cols::AbstractVector{T} where T, sep::AbstractString) in Base at arrayshow.jl:96 (1 callees from 1 callers)\n show(io::IO, x::T, forceuntyped::Bool, fromprint::Bool) where T<:Union{Float16, Float32, Float64} in Base.Ryu at ryu/Ryu.jl:111 (1 callees from 1 callers)\n Pair(a, b) in Base at pair.jl:15 (1 callees from 1 callers)\n vect(X...) in Base at array.jl:125 (1 callees from 1 callers)\n makeobjects() in Main.OptimizeMe at /pathto/SnoopCompile/examples/OptimizeMe.jl:36 (1 callees from 1 callers)\n show_delim_array(io::IO, itr, op, delim, cl, delim_one, i1, n) in Base at show.jl:1058 (1 callees from 1 callers)\n typeinfo_prefix(io::IO, X) in Base at arrayshow.jl:515 (2 callees from 1 callers)\n (::REPL.var\"#38#39\")(io) in REPL at /home/tim/src/julia-master/usr/share/julia/stdlib/v1.6/REPL/src/REPL.jl:214 (2 callees from 1 callers)\n _cat_t(dims, ::Type{T}, X...) where T in Base at abstractarray.jl:1633 (2 callees from 1 callers)\n contain_list(list) in Main.OptimizeMe at /pathto/SnoopCompile/examples/OptimizeMe.jl:27 (4 callees from 1 callers)\n promote_typeof(x, xs...) in Base at promotion.jl:272 (4 callees from 4 callers)\n combine_eltypes(f, args::Tuple) in Base.Broadcast at broadcast.jl:740 (5 callees from 1 callers)\n lotsa_containers() in Main.OptimizeMe at /pathto/SnoopCompile/examples/OptimizeMe.jl:12 (7 callees from 1 callers)\n alignment(io::IO, x) in Base at show.jl:2528 (7 callees from 7 callers)\n var\"#sprint#386\"(context, sizehint::Integer, ::typeof(sprint), f::Function, args...) in Base at strings/io.jl:100 (8 callees from 2 callers)\n alignment(io::IO, X::AbstractVecOrMat{T} where T, rows::AbstractVector{T} where T, cols::AbstractVector{T} where T, cols_if_complete::Integer, cols_otherwise::Integer, sep::Integer) in Base at arrayshow.jl:60 (8 callees from 2 callers)\n copyto_nonleaf!(dest, bc::Base.Broadcast.Broadcasted, iter, state, count) in Base.Broadcast at broadcast.jl:1070 (9 callees from 3 callers)\n _show_default(io::IO, x) in Base at show.jl:397 (12 callees from 1 callers)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"The methods triggering the largest number of inference runs are shown at the bottom. You can select methods from a particular module:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> modtrigs = filtermod(OptimizeMe, mtrigs)\n3-element Vector{SnoopCompile.TaggedTriggers{Method}}:\n makeobjects() in Main.OptimizeMe at /home/tim/.julia/dev/SnoopCompile/examples/OptimizeMe.jl:36 (1 callees from 1 callers)\n contain_list(list) in Main.OptimizeMe at /home/tim/.julia/dev/SnoopCompile/examples/OptimizeMe.jl:27 (4 callees from 1 callers)\n lotsa_containers() in Main.OptimizeMe at /home/tim/.julia/dev/SnoopCompile/examples/OptimizeMe.jl:12 (7 callees from 1 callers)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Rather than filter by a single module, you can alternatively call SnoopCompile.parcel(mtrigs) to split them out by module. In this case, most of the triggers came from Base, not OptimizeMe. However, many of the failures in Base were nevertheless indirectly due to OptimizeMe: our methods in OptimizeMe call Base methods with arguments that trigger internal inference failures. Fortunately, we'll see that using more careful design in OptimizeMe can avoid many of those problems.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"tip: Tip\nIf you have a longer list of inference triggers than you feel comfortable tackling, filtering by your package's module is probably the best way to start. Fixing issues in the package itself can end up resolving many of the \"indirect\" triggers too. Also be sure to note the ability to filter out likely \"noise\" from test suites.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"If you're hoping to fix inference problems, one of the most efficient things you can do is call summary:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> mtrig = modtrigs[1]\nmakeobjects() in Main.OptimizeMe at /home/tim/.julia/dev/SnoopCompile/examples/OptimizeMe.jl:36 (1 callees from 1 callers)\n\njulia> summary(mtrig)\nmakeobjects() in Main.OptimizeMe at /home/tim/.julia/dev/SnoopCompile/examples/OptimizeMe.jl:36 had 1 specializations\nTriggering calls:\nInlined _cat at ./abstractarray.jl:1630: calling cat_t##kw (1 instances)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Sometimes from these hints alone you can figure out how to fix the problem. (Inlined _cat means that the inference trigger did not come directly from a source line of makeobjects but from a call, _cat, that got inlined into the compiled version. Below we'll see more concretely how to interpret this hint.)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"You can also say edit(mtrig) and be taken directly to the method you're analyzing in your editor. Finally, you can recover the individual triggers:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> mtrig.itrigs[1]\nInference triggered to call MethodInstance for (::Base.var\"#cat_t##kw\")(::NamedTuple{(:dims,), Tuple{Val{1}}}, ::typeof(Base.cat_t), ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N) from _cat (./abstractarray.jl:1630) inlined into MethodInstance for makeobjects() (/home/tim/.julia/dev/SnoopCompile/examples/OptimizeMe.jl:37)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"This is useful if you want to analyze a method via Cthulhu.ascend. Method-based triggers, which may aggregate many different individual triggers, are particularly useful mostly because tools like Cthulhu.jl show you the inference results for the entire MethodInstance, allowing you to fix many different inference problems at once.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#Trigger-trees","page":"Using @snoop_inference results to improve inferrability","title":"Trigger trees","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"While method triggers are probably the most useful way of organizing these inference triggers, for learning purposes here we'll use a more detailed scheme, which organizes inference triggers in a tree:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> itree = trigger_tree(itrigs)\nTriggerNode for root with 14 direct children\n\njulia> using AbstractTrees\n\njulia> print_tree(itree)\nroot\n├─ MethodInstance for vect(::Int64, ::Vararg{Any, N} where N)\n│  └─ MethodInstance for promote_typeof(::Int64, ::UInt8, ::Vararg{Any, N} where N)\n│     └─ MethodInstance for promote_typeof(::UInt8, ::UInt16, ::Vararg{Any, N} where N)\n│        └─ MethodInstance for promote_typeof(::UInt16, ::Float32, ::Vararg{Any, N} where N)\n│           └─ MethodInstance for promote_typeof(::Float32, ::Char, ::Vararg{Any, N} where N)\n│              ⋮\n│\n├─ MethodInstance for combine_eltypes(::Type, ::Tuple{Vector{Any}})\n│  ├─ MethodInstance for return_type(::Any, ::Any)\n│  ├─ MethodInstance for return_type(::Any, ::Any, ::UInt64)\n│  ├─ MethodInstance for return_type(::Core.Compiler.NativeInterpreter, ::Any, ::Any)\n│  ├─ MethodInstance for contains_is(::Core.SimpleVector, ::Any)\n│  └─ MethodInstance for promote_typejoin_union(::Type{Main.OptimizeMe.Container})\n├─ MethodInstance for Main.OptimizeMe.Container(::Int64)\n⋮","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"The parent-child relationships are based on the backtraces at the entrance to inference, and the nodes are organized in the order in which inference occurred.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We're going to march through these systematically. Let's start with the first of these.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#suggest-and-a-fix-involving-manual-eltype-specification","page":"Using @snoop_inference results to improve inferrability","title":"suggest and a fix involving manual eltype specification","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Because the analysis of inference failures is somewhat complex, SnoopCompile attempts to suggest an interpretation and/or remedy for each trigger:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> suggest(itree.children[1])\n/pathto/SnoopCompile/examples/OptimizeMe.jl:13: invoked callee is varargs (ignore this one, homogenize the arguments, declare an umbrella type, or force-specialize the callee MethodInstance for vect(::Int64, ::Vararg{Any, N} where N))\nimmediate caller(s):\n1-element Vector{Base.StackTraces.StackFrame}:\n main() at OptimizeMe.jl:42\n└─ ./array.jl:126: caller is varargs (ignore this one, specialize the caller vect(::Int64, ::Vararg{Any, N} where N) at array.jl:126, or improve inferrability of its caller)\n   immediate caller(s):\n   1-element Vector{Base.StackTraces.StackFrame}:\n    lotsa_containers() at OptimizeMe.jl:13\n   └─ ./promotion.jl:272: caller is varargs (ignore this one, specialize the caller promote_typeof(::Int64, ::UInt8, ::Vararg{Any, N} where N) at promotion.jl:272, or improve inferrability of its caller)\n      immediate caller(s):\n      1-element Vector{Base.StackTraces.StackFrame}:\n       vect(::Int64, ::Vararg{Any, N} where N) at array.jl:126\n      └─ ./promotion.jl:272: caller is varargs (ignore this one, specialize the caller promote_typeof(::UInt8, ::UInt16, ::Vararg{Any, N} where N) at promotion.jl:272, or improve inferrability of its caller)\n         immediate caller(s):\n         1-element Vector{Base.StackTraces.StackFrame}:\n          promote_typeof(::Int64, ::UInt8, ::Vararg{Any, N} where N) at promotion.jl:272\n         └─ ./promotion.jl:272: caller is varargs (ignore this one, specialize the caller promote_typeof(::UInt16, ::Float32, ::Vararg{Any, N} where N) at promotion.jl:272, or improve inferrability of its caller)\n            immediate caller(s):\n            1-element Vector{Base.StackTraces.StackFrame}:\n             promote_typeof(::UInt8, ::UInt16, ::Vararg{Any, N} where N) at promotion.jl:272\n            └─ ./promotion.jl:272: caller is varargs (ignore this one, specialize the caller promote_typeof(::Float32, ::Char, ::Vararg{Any, N} where N) at promotion.jl:272, or improve inferrability of its caller)\n               immediate caller(s):\n               1-element Vector{Base.StackTraces.StackFrame}:\n                promote_typeof(::UInt16, ::Float32, ::Vararg{Any, N} where N) at promotion.jl:272\n               ⋮","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"tip: Tip\nIn the REPL, interpretations are highlighted in color to help distinguish individual suggestions.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"In this case, the interpretation for the first node is \"invoked callee is varargs\" and suggestions are to choose one of \"ignore...homogenize...umbrella type...force-specialize\". Initially, this may seem pretty opaque. It helps if we look at the referenced line OptimizeMe.jl:13:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"list = [1, 0x01, 0xffff, 2.0f0, 'a', [0], (\"key\", 42)]","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"You'll notice above that the callee for the first node is vect; that's what handles the creation of the vector [1, ...]. If you look back up at the itree, you can see that a lot of promote_typeof calls follow, and you can see that the types listed in the arguments match the elements in list. The problem, here, is that vect has never been inferred for this particular combination of argument types, and the fact that the types are diverse means that Julia has decided not to specialize it for this combination. (If Julia had specialized it, it would have been inferred when lotsa_containers was inferred; the fact that it is showing up as a trigger means it wasn't.)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Let's see what kind of object this line creates:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> typeof(list)\nVector{Any} (alias for Array{Any, 1})","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Since it creates a Vector{Any}, perhaps we should just tell Julia to create such an object directly: we modify [1, 0x01, ...] to Any[1, 0x01, ...] (note the Any in front of [), so that Julia doesn't have to deduce the container type on its own. This follows the \"declare an umbrella type\" suggestion.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"note: Note\n\"Force-specialize\" means to encourage Julia to violate its heuristics and specialize the callee. Often this can be achieved by supplying a \"spurious\" type parameter. Examples include replacing higherorderfunction(f::Function, args...) with function higherorderfunction(f::F, args...) where F<:Function, or function getindex(A::MyArrayType{T,N}, idxs::Vararg{Int,N}) where {T,N} instead of just getindex(A::MyArrayType, idxs::Int...). (In the latter case, the N parameter is the crucial one: it forces specialization for a particular number of Int arguments.)This technique is not useful for the particular case we analyzed here, but it can be in other settings.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Making this simple 3-character fix eliminates that entire branch of the tree (a savings of 6 inference triggers).","category":"page"},{"location":"tutorials/snoop_inference_analysis/#eltypes-and-reducing-specialization-in-broadcast","page":"Using @snoop_inference results to improve inferrability","title":"eltypes and reducing specialization in broadcast","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Let's move on to the next entry:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> print_tree(itree.children[2])\nMethodInstance for combine_eltypes(::Type, ::Tuple{Vector{Any}})\n├─ MethodInstance for return_type(::Any, ::Any)\n├─ MethodInstance for return_type(::Any, ::Any, ::UInt64)\n├─ MethodInstance for return_type(::Core.Compiler.NativeInterpreter, ::Any, ::Any)\n├─ MethodInstance for contains_is(::Core.SimpleVector, ::Any)\n└─ MethodInstance for promote_typejoin_union(::Type{Main.OptimizeMe.Container})\n\njulia> suggest(itree.children[2])\n./broadcast.jl:905: regular invoke (perhaps precompile lotsa_containers() at OptimizeMe.jl:14)\n├─ ./broadcast.jl:740: I've got nothing to say for MethodInstance for return_type(::Any, ::Any) consider `stacktrace(itrig)` or `Cthulhu.ascend(itrig)`\n├─ ./broadcast.jl:740: I've got nothing to say for MethodInstance for return_type(::Any, ::Any, ::UInt64) consider `stacktrace(itrig)` or `Cthulhu.ascend(itrig)`\n├─ ./broadcast.jl:740: I've got nothing to say for MethodInstance for return_type(::Core.Compiler.NativeInterpreter, ::Any, ::Any) consider `stacktrace(itrig)` or `Cthulhu.ascend(itrig)`\n├─ ./broadcast.jl:740: I've got nothing to say for MethodInstance for contains_is(::Core.SimpleVector, ::Any) consider `stacktrace(itrig)` or `Cthulhu.ascend(itrig)`\n└─ ./broadcast.jl:740: non-inferrable call, perhaps annotate combine_eltypes(f, args::Tuple) in Base.Broadcast at broadcast.jl:740 with type MethodInstance for promote_typejoin_union(::Type{Main.OptimizeMe.Container})\n   If a noninferrable argument is a type or function, Julia's specialization heuristics may be responsible.\n   immediate caller(s):\n   3-element Vector{Base.StackTraces.StackFrame}:\n    copy at broadcast.jl:905 [inlined]\n    materialize at broadcast.jl:883 [inlined]\n    lotsa_containers() at OptimizeMe.jl:14","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"While this tree is attributed to broadcast, you can see several references here to OptimizeMe.jl:14, which contains:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"cs = Container.(list)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Container.(list) is a broadcasting operation, and once again we find that this has inferrability problems. In this case, the initial suggestion \"perhaps precompile lotsa_containers\" is not helpful. (The \"regular invoke\" just means that the initial call was one where inference knew all the argument types, and hence in principle might be precompilable, but from this tree we see that this broke down in some of its callees.) Several children have no interpretation (\"I've got nothing to say...\"). Only the last one, \"non-inferrable call\", is (marginally) useful, it means that a call was made with arguments whose types could not be inferred.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"warning: Warning\nYou should always view these suggestions skeptically. Often, they flag downstream issues that are better addressed at the source; frequently the best fix may be at a line a bit before the one identified in a trigger, or even in a dependent callee of a line prior to the flagged one. This is a product of the fact that returning a non-inferrable argument is not the thing that forces a new round of inference; it's doing something (making a specialization-worthy call) with the object of non-inferrable type that triggers a fresh entrance into inference.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"How might we go about fixing this? One hint is to notice that itree.children[3] through itree.children[7] also ultimiately derive from this one line of OptimizeMe, but from a later line within broadcast.jl which explains why they are not bundled together with itree.children[2]. May of these correspond to creating different Container types, for example:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"└─ MethodInstance for restart_copyto_nonleaf!(::Vector{Main.OptimizeMe.Container}, ::Vector{Main.OptimizeMe.Container{Int64}}, ::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Tuple{Base.OneTo{Int64}}, Type{Main.OptimizeMe.Container}, Tuple{Base.Broadcast.Extruded{Vector{Any}, Tuple{Bool}, Tuple{Int64}}}}, ::Main.OptimizeMe.Container{UInt8}, ::Int64, ::Base.OneTo{Int64}, ::Int64, ::Int64)\n   ├─ MethodInstance for Main.OptimizeMe.Container(::UInt16)\n   ├─ MethodInstance for Main.OptimizeMe.Container(::Float32)\n   ├─ MethodInstance for Main.OptimizeMe.Container(::Char)\n   ├─ MethodInstance for Main.OptimizeMe.Container(::Vector{Int64})\n   └─ MethodInstance for Main.OptimizeMe.Container(::Tuple{String, Int64})","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We've created a Container{T} for each specific T of the objects in list. In some cases, there may be good reasons for such specialization, and in such cases we just have to live with these inference failures. However, in other cases the specialization might be detrimental to compile-time and/or runtime performance. In such cases, we might decide to create them all as Container{Any}:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"cs = Container{Any}.(list)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"This 5-character change ends up eliminating 45 of our original 76 triggers. Not only did we eliminate the triggers from broadcasting, but we limited the number of different show(::IO, ::Container{T})-MethodInstances we need from later calls in main.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"When the Container constructor does more complex operations, in some cases you may find that Container{Any}(args...) still gets specialized for different types of args.... In such cases, you can create a special constructor that instructs Julia to avoid specialization in specific instances, e.g.,","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"struct Container{T}\n    field1::T\n    morefields...\n\n    # This constructor permits specialization on `args`\n    Container{T}(args...) where {T} = new{T}(args...)\n\n    # For Container{Any}, we prevent specialization\n    Container{Any}(@nospecialize(args...)) = new{Any}(args...)\nend","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"If you're following along, the best option is to make these fixes and go back to the beginning, re-collecting tinf and processing the triggers. We're down to 32 inference triggers.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#typeasserts","page":"Using @snoop_inference results to improve inferrability","title":"Adding type-assertions","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"If you've made the fixes above, the first child of itree is one for show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMe.Container{Any}}); we'll skip that one for now, because it's a bit more sophisticated. Right below it, we see","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"├─ MethodInstance for combine_eltypes(::Type, ::Tuple{Vector{Any}})\n│  ├─ MethodInstance for return_type(::Any, ::Any)\n│  ├─ MethodInstance for return_type(::Any, ::Any, ::UInt64)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"and related nodes for similar, copyto_nonleaf!, etc., just as we saw above, so this looks like another case of broadcasting failure. In this case, suggest quickly indicates that it's the broadcasting in","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"function contain_list(list)\n    cs = Container.(list)\n    return concat_string(cs...)\nend","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Now we know the problem: main creates list = [2.718, \"is jealous\"], a vector with different object types, and this leads to inference failures in broadcasting. But wait, you might notice, contain_concrete gets called before contain_list, why doesn't it have a problem? The reason is that contain_concrete and its callee, concat_string, provide opportunities for inference to handle each object in a separate argument; the problems arise from bundling objects of different types into the same container.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"There are several ways we could go about fixig this example:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"we could delete contain_list altogether and use contain_concrete for everything.\nwe could try creating list as a tuple rather than a Vector{Any}; (small) tuples sometimes allow inference to succeed even when each element has a different type. This is as simple as changing list = [2.718, \"is jealous\"] to list = (2.718, \"is jealous\"), but whether it works to solve all your inference problems depends on the particular case.\nwe could use external knowledge to annotate the types of the items in list::Vector{Any}.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Here we'll illustrate the last of these, since it's the only one that's nontrivial. (It's also often a useful pattern in many real-world contexts, such as cases where you have a Dict{String,Any} but know something about the kinds of value-types associated with particular string keys.) We could rewrite contain_list so it looks like this:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"function contain_list(list)\n    length(list) == 2 || throw(DimensionMismatch(\"list must have length 2\"))\n    item1 = list[1]::Float64\n    item2 = list[2]::String\n    return contain_concrete(item1, item2)     # or we could repeat the body of contain_concrete\nend","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"The type-assertions tell inference that the corresponding items have the given types, and assist inference in cases where it has no mechanism to deduce the answer on its own. Julia will throw an error if the type-assertion fails. In some cases, a more forgiving option might be","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"item1 = convert(Float64, list[1])::Float64","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"which will attempt to convert list[1] to a Float64, and therefore handle a wider range of number types stored in the first element of list. Believe it or not, both the convert() and the ::Float64 type-assertion are necessary: since list[1] is of type Any, Julia will not be able to deduce which convert method will be used to perform the conversion, and it's always possible that someone has written a sloppy convert that doesn't return a value of the requested type. Without that final ::Float64, inference cannot simply assume that the result is a Float64. The type-assert ::Float64 enforces the fact that you're expecting that convert call to actually return a Float64–it will error if it fails to do so, and it's this error that allows inference to be certain that for the purposes of any later code it must be a Float64.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Of course, this just trades one form of inference failure for another–the call to convert will be made by runtime dispatch–but this can nevertheless be a big win for two reasons:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"even though the convert call will be made by runtime dispatch, in this particular case convert(Float64, ::Float64) is already compiled in Julia itself.  Consequently it doesn't require a fresh run of inference.\neven in cases where the types are such that convert might need to be inferred & compiled, the type-assertion allows Julia to assume that item1 is henceforth a Float64.  This makes it possible for inference to succeed for any code that follows.  When that's a large amount of code, the savings can be considerable.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Let's make that fix and also annotate the container type from main, list = Any[2.718, \"is jealous\"]. Just to see how we're progressing, we start a fresh session and discover we're down to 20 triggers with just three direct branches.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#Vararg-homogenization","page":"Using @snoop_inference results to improve inferrability","title":"Vararg homogenization","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We'll again skip over the show branches (they are two of the remaining three), and focus on this one:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> node = itree.children[2]\nTriggerNode for MethodInstance for (::Base.var\"#cat_t##kw\")(::NamedTuple{(:dims,), Tuple{Val{1}}}, ::typeof(Base.cat_t), ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N) with 2 direct children\n\njulia> print_tree(node)\nMethodInstance for (::Base.var\"#cat_t##kw\")(::NamedTuple{(:dims,), Tuple{Val{1}}}, ::typeof(Base.cat_t), ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N)\n├─ MethodInstance for cat_similar(::UnitRange{Int64}, ::Type, ::Tuple{Int64})\n└─ MethodInstance for __cat(::Vector{Int64}, ::Tuple{Int64}, ::Tuple{Bool}, ::UnitRange{Int64}, ::Vararg{Any, N} where N)\n\njulia> suggest(node)\n./abstractarray.jl:1630: invoked callee is varargs (ignore this one, force-specialize the callee MethodInstance for (::Base.var\"#cat_t##kw\")(::NamedTuple{(:dims,), Tuple{Val{1}}}, ::typeof(Base.cat_t), ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N), or declare an umbrella type)\nimmediate caller(s):\n1-element Vector{Base.StackTraces.StackFrame}:\n main() at OptimizeMe.jl:48\n├─ ./abstractarray.jl:1636: caller is varargs (ignore this one, specialize the caller _cat_t(::Val{1}, ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N) at abstractarray.jl:1636, or improve inferrability of its caller)\n│  immediate caller(s):\n│  1-element Vector{Base.StackTraces.StackFrame}:\n│   cat_t(::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N; dims::Val{1}) at abstractarray.jl:1632\n└─ ./abstractarray.jl:1640: caller is varargs (ignore this one, specialize the caller _cat_t(::Val{1}, ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N) at abstractarray.jl:1640, or improve inferrability of its caller)\n   immediate caller(s):\n   1-element Vector{Base.StackTraces.StackFrame}:\n    cat_t(::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N; dims::Val{1}) at abstractarray.jl:1632","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Due to Julia's optimization and inlining, it's sometimes a bit hard to tell from these shortened displays where a particular trigger comes from. (It turns out that this is finally the trigger we looked at in greatest detail in method-based triggers.) In this case we extract the specific trigger and show the stacktrace:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> itrig = node.itrig\nInference triggered to call MethodInstance for (::Base.var\"#cat_t##kw\")(::NamedTuple{(:dims,), Tuple{Val{1}}}, ::typeof(Base.cat_t), ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N) from _cat (./abstractarray.jl:1630) inlined into MethodInstance for makeobjects() (/tmp/OptimizeMe.jl:39)\n\njulia> stacktrace(itrig)\n24-element Vector{Base.StackTraces.StackFrame}:\n exit_current_timer at typeinfer.jl:166 [inlined]\n typeinf(interp::Core.Compiler.NativeInterpreter, frame::Core.Compiler.InferenceState) at typeinfer.jl:208\n typeinf_ext(interp::Core.Compiler.NativeInterpreter, mi::Core.MethodInstance) at typeinfer.jl:835\n typeinf_ext_toplevel(interp::Core.Compiler.NativeInterpreter, linfo::Core.MethodInstance) at typeinfer.jl:868\n typeinf_ext_toplevel(mi::Core.MethodInstance, world::UInt64) at typeinfer.jl:864\n _cat at abstractarray.jl:1630 [inlined]\n #cat#127 at abstractarray.jl:1769 [inlined]\n cat at abstractarray.jl:1769 [inlined]\n vcat at abstractarray.jl:1698 [inlined]\n makeobjects() at OptimizeMe.jl:39\n main() at OptimizeMe.jl:48\n top-level scope at snoop_inference.jl:53\n eval(m::Module, e::Any) at boot.jl:360\n eval_user_input(ast::Any, backend::REPL.REPLBackend) at REPL.jl:139\n repl_backend_loop(backend::REPL.REPLBackend) at REPL.jl:200\n start_repl_backend(backend::REPL.REPLBackend, consumer::Any) at REPL.jl:185\n run_repl(repl::REPL.AbstractREPL, consumer::Any; backend_on_current_task::Bool) at REPL.jl:317\n run_repl(repl::REPL.AbstractREPL, consumer::Any) at REPL.jl:305\n (::Base.var\"#872#874\"{Bool, Bool, Bool})(REPL::Module) at client.jl:387\n #invokelatest#2 at essentials.jl:707 [inlined]\n invokelatest at essentials.jl:706 [inlined]\n run_main_repl(interactive::Bool, quiet::Bool, banner::Bool, history_file::Bool, color_set::Bool) at client.jl:372\n exec_options(opts::Base.JLOptions) at client.jl:302\n _start() at client.jl:485","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"(You can also call stacktrace directly on node.) It's the lines immediately following typeinf_ext_toplevel that need concern us: you can see that the \"last stop\" on code we wrote here was makeobjects() at OptimizeMe.jl:39, after which it goes fairly deep into the concatenation pipeline before suffering an inference trigger at _cat at abstractarray.jl:1630.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"In this case, the first hint is quite useful, if we know how to interpret it. The invoked callee is varargs reassures us that the immediate caller, _cat, knows exactly which method it is calling (that's the meaning of the invoked). The real problem is that it doesn't know how to specialize it. The suggestion to homogenize the arguments is the crucial hint: the problem comes from the fact that in","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"xs = [1:5; 7]","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"1:5 is a UnitRange{Int} whereas 7 is an Int, and the fact that these are two different types prevents Julia from knowing how to specialize that varargs call. But this is easy to fix, because the result will be identical if we write this as","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"xs = [1:5; 7:7]","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"in which case both arguments are UnitRange{Int}, and this allows Julia to specialize the varargs call.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"note: Note\nIt's generally a good thing that Julia doesn't specialize each and every varargs call, because the lack of specialization reduces latency. However, when you can homogenize the argument types and make it inferrable, you make it more worthy of precompilation, which is a different and ultimately more impactful approach to latency reduction.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#Defining-show-methods-for-custom-types","page":"Using @snoop_inference results to improve inferrability","title":"Defining show methods for custom types","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Finally we are left with nodes that are related to show. We'll temporarily skip the first of these and examine","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> print_tree(node)\nMethodInstance for show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMe.Object})\n└─ MethodInstance for var\"#sprint#386\"(::IOContext{Base.TTY}, ::Int64, ::typeof(sprint), ::Function, ::Main.OptimizeMe.Object)\n   └─ MethodInstance for sizeof(::Main.OptimizeMe.Object)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We'll use this as an excuse to point out that if you don't know how to deal with the root node of this (sub)tree, you can tackle later nodes:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> itrigsnode = flatten(node)\n3-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMe.Object}) from #38 (/home/tim/src/julia-master/usr/share/julia/stdlib/v1.6/REPL/src/REPL.jl:220) with specialization MethodInstance for (::REPL.var\"#38#39\"{REPL.REPLDisplay{REPL.LineEditREPL}, MIME{Symbol(\"text/plain\")}, Base.RefValue{Any}})(::Any)\n Inference triggered to call MethodInstance for var\"#sprint#386\"(::IOContext{Base.TTY}, ::Int64, ::typeof(sprint), ::Function, ::Main.OptimizeMe.Object) from sprint##kw (./strings/io.jl:101) inlined into MethodInstance for alignment(::IOContext{Base.TTY}, ::Vector{Main.OptimizeMe.Object}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::Int64, ::Int64, ::Int64) (./arrayshow.jl:68)\n Inference triggered to call MethodInstance for sizeof(::Main.OptimizeMe.Object) from _show_default (./show.jl:402) with specialization MethodInstance for _show_default(::IOContext{IOBuffer}, ::Any)\n\njulia> itrig = itrigsnode[end]\nInference triggered to call MethodInstance for sizeof(::Main.OptimizeMe.Object) from _show_default (./show.jl:402) with specialization MethodInstance for _show_default(::IOContext{IOBuffer}, ::Any)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"The stacktrace begins","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> stacktrace(itrig)\n35-element Vector{Base.StackTraces.StackFrame}:\n exit_current_timer at typeinfer.jl:166 [inlined]\n typeinf(interp::Core.Compiler.NativeInterpreter, frame::Core.Compiler.InferenceState) at typeinfer.jl:208\n typeinf_ext(interp::Core.Compiler.NativeInterpreter, mi::Core.MethodInstance) at typeinfer.jl:835\n typeinf_ext_toplevel(interp::Core.Compiler.NativeInterpreter, linfo::Core.MethodInstance) at typeinfer.jl:868\n typeinf_ext_toplevel(mi::Core.MethodInstance, world::UInt64) at typeinfer.jl:864\n _show_default(io::IOContext{IOBuffer}, x::Any) at show.jl:402\n show_default at show.jl:395 [inlined]\n show(io::IOContext{IOBuffer}, x::Any) at show.jl:390\n sprint(f::Function, args::Main.OptimizeMe.Object; context::IOContext{Base.TTY}, sizehint::Int64) at io.jl:103\n⋮","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"You can see that sprint called show which called _show_default; _show_default clearly needed to call sizeof. The hint, in this case, suggests the impossible:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> suggest(itrig)\n./show.jl:402: non-inferrable call, perhaps annotate _show_default(io::IO, x) in Base at show.jl:397 with type MethodInstance for sizeof(::Main.OptimizeMe.Object)\nIf a noninferrable argument is a type or function, Julia's specialization heuristics may be responsible.\nimmediate caller(s):\n2-element Vector{Base.StackTraces.StackFrame}:\n show_default at show.jl:395 [inlined]\n show(io::IOContext{IOBuffer}, x::Any) at show.jl:390","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Because Base doesn't know about OptimizeMe.Object, you could not add such an annotation, and it wouldn't be correct in the vast majority of cases.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"As the name implies, _show_default is the fallback show method. We can fix this by adding our own show method","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Base.show(io::IO, o::Object) = print(io, \"Object x: \", o.x)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"to the module definition. Object is so simple that this is slightly silly, but in more complex cases adding good show methods improves usability of your packages tremendously. (SnoopCompile has many show specializations, and without them it would be practically unusable.)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"When you do define a custom show method, you own it, so of course it will be precompilable. So we've circumvented this particular issue.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#Creating-\"warmup\"-methods","page":"Using @snoop_inference results to improve inferrability","title":"Creating \"warmup\" methods","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Finally, it is time to deal with those long-delayed show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::T) triggers and the triggers they inspire. We have two of them, one for T = Vector{Main.OptimizeMe.Container{Any}} and one for T = Vector{Main.OptimizeMe.Object}. Let's look at just the trigger associated with the first:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> itrig\nInference triggered to call MethodInstance for show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMeFixed.Container{Any}}) from #38 (/pathto/julia/usr/share/julia/stdlib/v1.6/REPL/src/REPL.jl:220) with specialization MethodInstance for (::REPL.var\"#38#39\"{REPL.REPLDisplay{REPL.LineEditREPL}, MIME{Symbol(\"text/plain\")}, Base.RefValue{Any}})(::Any)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"In this case we see that the method is #38.  This is a gensym, or generated symbol, indicating that the method was generated during Julia's lowering pass, and might indicate a macro, a do block or other anonymous function, the generator for a @generated function, etc.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"warning: Warning\nIt's particularly worthwhile to improve inferrability for gensym-methods. The number assiged to a gensymmed-method may change as you or other developers modify the package (possibly due to changes at very difference source-code locations), and so any explicit precompile directives involving gensyms may not have a long useful life.But not all methods with # in their name are problematic: methods ending in ##kw or that look like ##funcname#39 are keyword and body methods, respectively, for methods that accept keywords.  They can be obtained from the main method, and so precompile directives for such methods will not be outdated by incidental changes to the package.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"edit(itrig) (or equivalently, edit(node) where node is a child of itree) takes us to this method in Base:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"function display(d::REPLDisplay, mime::MIME\"text/plain\", x)\n    x = Ref{Any}(x)\n    with_repl_linfo(d.repl) do io\n        io = IOContext(io, :limit => true, :module => Main::Module)\n        get(io, :color, false) && write(io, answer_color(d.repl))\n        if isdefined(d.repl, :options) && isdefined(d.repl.options, :iocontext)\n            # this can override the :limit property set initially\n            io = foldl(IOContext, d.repl.options.iocontext, init=io)\n        end\n        show(io, mime, x[])\n        println(io)\n    end\n    return nothing\nend","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"The generated method corresponds to the do block here. The call to show comes from show(io, mime, x[]). This implementation uses a clever trick, wrapping x in a Ref{Any}(x), to prevent specialization of the method defined by the do block on the specific type of x. This trick is designed to limit the number of MethodInstances inferred for this display method.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Unfortunately, from the standpoint of precompilation we have something of a conundrum. It turns out that this trigger corresponds to the first of the big red flames in the flame graph. show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMe.Container{Any}}) is not precompilable because Base owns the show method for Vector; we might own the element type, but we're leveraging the generic machinery in Base and consequently it owns the method. If these were all packages, you might request its developers to add a precompile directive, but that will work only if the package that owns the method knows about the relevant type. In this situation, Julia's Base module doesn't know about OptimizeMe.Container{Any}, so we're stuck.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"There are a couple of ways one might go about improving matters. First, one option is that this should be changed in Julia itself: since the caller, display, has gone to some lengths to reduce specialization, it would be worth contemplating whether show(io::IO, ::MIME\"text/plain\", X::AbstractArray) should have a @nospecialize around X. Here, we'll pursue a simple \"cheat,\" one that allows us to directly precompile this method. The trick is to link it, via a chain of backedges, to a method that our package owns:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"# \"Stub\" callers for precompilability (we don't use this function for any real work)\nfunction warmup()\n    mime = MIME(\"text/plain\")\n    io = Base.stdout::Base.TTY\n    # Container{Any}\n    v = [Container{Any}(0)]\n    show(io, mime, v)\n    show(IOContext(io), mime, v)\n    # Object\n    v = [Object(0)]\n    show(io, mime, v)\n    show(IOContext(io), mime, v)\n    return nothing\nend\n\nprecompile(warmup, ())","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We handled not just Vector{Container{Any}} but also Vector{Object}, since that turns out to correspond to the other wide block of red bars. If you make this change, start a fresh session, and recreate the flame graph, you'll see that the wide red flames are gone:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"(Image: flamegraph-OptimizeMeFixed)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"info: Info\nIt's worth noting that this warmup method needed to be carefully written to succeed in its mission. stdout is not inferrable (it's a global that can be replaced by redirect_stdout), so we needed to annotate its type. We also might have been tempted to use a loop, for io in (stdout, IOContext(stdout)) ... end, but inference needs a dedicated call-site where it knows all the types. (Union-splitting can sometimes come to the rescue, but not if the list is long or elements non-inferrable.) The safest option is to make each call from a separate site in the code.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"The next trigger, a call to sprint from inside Base.alignment(io::IO, x::Any), could also be handled using this warmup trick, but the flamegraph says this call (also marked in red) isn't an expensive method to infer.  In such cases, it's fine to choose to leave it be.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#Implementing-or-requesting-precompile-directives-in-upstream-packages","page":"Using @snoop_inference results to improve inferrability","title":"Implementing or requesting precompile directives in upstream packages","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Of the remaining triggers (now numbering 14), the flamegraph indicates that the most expensive inference run is","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Inference triggered to call MethodInstance for show(::IOContext{IOBuffer}, ::Float32) from _show_default (./show.jl:412) with specialization MethodInstance for _show_default(::IOContext{IOBuffer}, ::Any)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"You can check that by listing the children of ROOT in order of inclusive time:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> nodes = sort(tinf.children; by=inclusive)\n14-element Vector{SnoopCompileCore.InferenceTimingNode}:\n InferenceTimingNode: 0.000053/0.000053 on InferenceFrameInfo for ==(::Type, nothing::Nothing) with 0 direct children\n InferenceTimingNode: 0.000054/0.000054 on InferenceFrameInfo for sizeof(::Main.OptimizeMeFixed.Container{Any}) with 0 direct children\n InferenceTimingNode: 0.000061/0.000061 on InferenceFrameInfo for Base.typeinfo_eltype(::Type) with 0 direct children\n InferenceTimingNode: 0.000075/0.000380 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::Any) with 1 direct children\n InferenceTimingNode: 0.000445/0.000445 on InferenceFrameInfo for Pair{Symbol, DataType}(::Any, ::Any) with 0 direct children\n InferenceTimingNode: 0.000663/0.000663 on InferenceFrameInfo for print(::IOContext{Base.TTY}, ::String, ::String, ::Vararg{String, N} where N) with 0 direct children\n InferenceTimingNode: 0.000560/0.001049 on InferenceFrameInfo for Base.var\"#sprint#386\"(::IOContext{Base.TTY}, ::Int64, sprint::typeof(sprint), ::Function, ::Main.OptimizeMeFixed.Object) with 4 direct children\n InferenceTimingNode: 0.000441/0.001051 on InferenceFrameInfo for Pair(::Symbol, ::Type) with 1 direct children\n InferenceTimingNode: 0.000627/0.001140 on InferenceFrameInfo for Base.var\"#sprint#386\"(::IOContext{Base.TTY}, ::Int64, sprint::typeof(sprint), ::Function, ::Main.OptimizeMeFixed.Container{Any}) with 4 direct children\n InferenceTimingNode: 0.000321/0.001598 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::UInt16) with 4 direct children\n InferenceTimingNode: 0.000190/0.012516 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::Vector{Int64}) with 3 direct children\n InferenceTimingNode: 0.021179/0.033940 on InferenceFrameInfo for Base.Ryu.writeshortest(::Vector{UInt8}, ::Int64, ::Float32, ::Bool, ::Bool, ::Bool, ::Int64, ::UInt8, ::Bool, ::UInt8, ::Bool, ::Bool) with 29 direct children\n InferenceTimingNode: 0.000083/0.035496 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::Tuple{String, Int64}) with 1 direct children\n InferenceTimingNode: 0.000188/0.092555 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::Float32) with 1 direct children","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"You can see it's the most expensive remaining root, weighing in at nearly 100ms. This method is defined in the Base.Ryu module,","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> node = nodes[end]\nInferenceTimingNode: 0.000188/0.092555 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::Float32) with 1 direct children\n\njulia> Method(node)\nshow(io::IO, x::T) where T<:Union{Float16, Float32, Float64} in Base.Ryu at ryu/Ryu.jl:111","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Now, we could add this to warmup and at least solve the inference problem. However, on the flamegraph you might note that this is followed shortly by a couple of calls to Ryu.writeshortest (the third-most expensive to infer), followed by a long gap. That hints that other steps, like native code generation, may be expensive. Since these are base Julia methods, and Float32 is a common type, it would make sense to file an issue or pull request that Julia should come shipped with these precompiled–that would cache not only the type-inference but also the native code, and thus represents a far more complete solution.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Later, we'll see how parcel can generate such precompile directives automatically, so this is not a step you need to implement entirely on your own.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Another show MethodInstance, show(::IOContext{IOBuffer}, ::Tuple{String, Int64}), seems too specific to be worth worrying about, so we call it quits here.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#ascend-itrig","page":"Using @snoop_inference results to improve inferrability","title":"Advanced analysis: Cthulhu.ascend","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"One thing that hasn't yet been covered is that when you really need more insight, you can use ascend:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> itrig = itrigs[5]\nInference triggered to call MethodInstance for show(::IOContext{IOBuffer}, ::Float32) from _show_default (./show.jl:412) with specialization MethodInstance for _show_default(::IOContext{IOBuffer}, ::Any)\n\njulia> ascend(itrig)\nChoose a call for analysis (q to quit):\n >   show(::IOContext{IOBuffer}, ::Float32)\n       _show_default(::IOContext{IOBuffer}, ::Any) at ./show.jl:412\n         show_default at ./show.jl:395 => show(::IOContext{IOBuffer}, ::Any) at ./show.jl:390\n           #sprint#386(::IOContext{Base.TTY}, ::Int64, ::typeof(sprint), ::Function, ::Main.OptimizeMeFixed.Container{Any}) at ./strings/io.jl:103\n             sprint##kw at ./strings/io.jl:101 => alignment at ./show.jl:2528 => alignment(::IOContext{Base.TTY}, ::Vector{Main.OptimizeMeFixed.Container{Any}}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::\n               print_matrix(::IOContext{Base.TTY}, ::AbstractVecOrMat{T} where T, ::String, ::String, ::String, ::String, ::String, ::String, ::Int64, ::Int64) at ./arrayshow.jl:197\n                 print_matrix at ./arrayshow.jl:169 => print_array at ./arrayshow.jl:323 => show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMeFixed.Container{Any}}) at ./a\n                   (::REPL.var\"#38#39\"{REPL.REPLDisplay{REPL.LineEditREPL}, MIME{Symbol(\"text/plain\")}, Base.RefValue{Any}})(::Any) at /home/tim/src/julia-master/usr/share/julia/stdlib/v1.6/REPL/src/REPL\n                     with_repl_linfo(::Any, ::REPL.LineEditREPL) at /home/tim/src/julia-master/usr/share/julia/stdlib/v1.6/REPL/src/REPL.jl:462\nv                      display(::REPL.REPLDisplay, ::MIME{Symbol(\"text/plain\")}, ::Any) at /home/tim/src/julia-master/usr/share/julia/stdlib/v1.6/REPL/src/REPL.jl:213\n","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Here, one twist is that some lines contain content like","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"show_default at ./show.jl:395 => show(::IOContext{IOBuffer}, ::Any) at ./show.jl:390","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"This indicates that show_default was inlined into show. ascend needs the full non-inlined MethodInstance to descend into, so the tree only includes such nodes. However, within Cthulhu you can toggle optimization and thereby descend into some of these inlined method, or see the full consequence of their inlining into the caller.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#test-suites","page":"Using @snoop_inference results to improve inferrability","title":"A note on analyzing test suites","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"If you're doing a package analysis, it's convenient to use the package's runtests.jl script as a way to cover much of the package's functionality. SnoopCompile has a couple of enhancements designed to make it easier to ignore inference triggers that come from the test suite itself. First, suggest.(itrigs) may show something like this:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":" ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"This indicates a broadcasting operation in the @testset itself. Second, while it's a little dangerous (because suggest cannot entirely be trusted), you can filter these out:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> itrigsel = [itrig for itrig in itrigs if !isignorable(suggest(itrig))];\n\njulia> length(itrigs)\n222\n\njulia> length(itrigsel)\n71","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"While there is some risk of discarding triggers that provide clues about the origin of other triggers (e.g., they would have shown up in the same branch of the trigger_tree), the shorter list may help direct your attention to the \"real\" issues.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#Results-from-the-improvements","page":"Using @snoop_inference results to improve inferrability","title":"Results from the improvements","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"An improved version of OptimizeMe can be found in OptimizeMeFixed.jl in the same directory. Let's see where we stand:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> tinf = @snoop_inference OptimizeMeFixed.main()\n3.14 is great\n2.718 is jealous\n...\n Object x: 7\nInferenceTimingNode: 0.888522055/1.496965222 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 15 direct children","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We've substantially shrunk the overall inclusive time from 2.68s to about 1.5s. Some of this came from our single precompile directive, for warmup. But even more of it came from limiting specialization (using Container{Any} instead of Container) and by making some results easier on type-inference (e.g., our changes for the vcat pipeline).","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"On the next page, we'll wrap all this up with more explicit precompile directives.","category":"page"},{"location":"tutorials/pgdsgui/#pgds","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Julia's multiple dispatch allows developers to create methods for specific argument types. On top of this, the Julia compiler performs automatic specialization:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"function countnonzeros(A::AbstractArray)\n    ...\nend","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"will be compiled separately for Vector{Int}, Matrix{Float64}, SubArray{...}, and so on, if it gets called for each of these types. Each specialization (each MethodInstance with different argument types) costs extra inference and code-generation time, so while specialization often improves runtime performance, that has to be weighed against the cost in latency. There are also cases in which overspecialization can hurt both run-time and compile-time performance. Consequently, an analysis of specialization can be a powerful tool for improving package quality.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"SnoopCompile ships with an interactive tool, pgdsgui, short for \"Profile-guided despecialization.\" The name is a reference to a related technique, profile-guided optimization (PGO). Both PGO and PGDS use runtime profiling to help guide decisions about code optimization. PGO is often used in languages whose default mode is to avoid specialization, whereas PGDS seems more appropriate for a language like Julia which specializes by default. While PGO is sometimes an automatic part of the compiler that optimizes code midstream during execution, SnoopCompile's PGDS is a tool for making static changes (edits) to code. Again, this seems appropriate for a language where specialization typically happens prior to the first execution of the code.","category":"page"},{"location":"tutorials/pgdsgui/#Add-SnoopCompileCore,-SnoopCompile,-and-helper-packages-to-your-environment","page":"Profile-guided despecialization","title":"Add SnoopCompileCore, SnoopCompile, and helper packages to your environment","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"We'll add these packages to your default environment so you can use them while in the package environment:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"using Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\", \"PyPlot\"]);","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"PyPLot is used for the PGDS interface in part to reduce interference with native-Julia plotting packages like Makie–it's a little awkward to depend on a package that you might be simultaneously modifying!","category":"page"},{"location":"tutorials/pgdsgui/#Using-the-PGDS-graphical-user-interface","page":"Profile-guided despecialization","title":"Using the PGDS graphical user interface","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"To illustrate the use of PGDS, we'll examine an example in which some methods get specialized for hundreds of types. To keep this example short, we'll create functions that operate on types themselves.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"note: Note\nAs background to this example, for a DataType T, T.name returns a Core.TypeName, and T.name.name returns the name as a Symbol. Base.unwrap_unionall(T) preserves DataTypes as-is, but converts a UnionAll type into a DataType.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"\"\"\"\n    spelltype(T::Type)\n\nSpell out a type's name, one character at a time.\n\"\"\"\nfunction spelltype(::Type{T}) where T\n    name = Base.unwrap_unionall(T).name.name\n    str = \"\"\n    for c in string(name)\n        str *= c\n    end\n    return str\nend\n\n\"\"\"\n    mappushes!(f, dest, src)\n\nLike `map!` except it grows `dest` by one for each element in `src`.\n\"\"\"\nfunction mappushes!(f, dest, src)\n    for item in src\n        push!(dest, f(item))\n    end\n    return dest\nend\n\nmappushes(f, src) = mappushes!(f, [], src)","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"There are two stages to PGDS: first (and preferrably starting in a fresh Julia session), we profile type-inference:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> using SnoopCompileCore\n\njulia> Ts = subtypes(Any);  # get a long list of different types\n\njulia> tinf = @snoop_inference mappushes(spelltype, Ts);","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Then, in the same session, profile the runtime:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> using Profile\n\njulia> @profile mappushes(spelltype, Ts);","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Typically, it's best if the workload here is reflective of a \"real\" workload (test suites often are not), so that you get a realistic view of where your code spends its time during actual use.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Now let's launch the PDGS GUI:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> using SnoopCompile\n\njulia> import PyPlot        # the GUI is dependent on PyPlot, must load it before the next line\n\njulia> mref, ax = pgdsgui(tinf);","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"You should see something like this:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"(Image: pgdsgui)","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In this graph, each dot corresponds to a single method; for this method, we plot inference time (vertical axis) against the run time (horizontal axis). The coloration of each dot encodes the number of specializations (the number of distinct MethodInstances) for that method; by default it even includes the number of times the method was inferred for specific constants (constant propagation), although you can exclude those cases using the consts=false keyword. Finally, the edge of each dot encodes the fraction of time spent on runtime dispatch (aka, type-instability), with black indicating 0% and bright red indicating 100%.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In this plot, we can see that no method runs for more than 0.01 seconds, whereas some methods have an aggregate inference time of up to 1s. Overall, inference-time dominates this plot. Moreover, for the most expensive cases, the number of specializations is in the hundreds or thousands.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"To learn more about what is being specialized, just click on one of the dots; if you choose the upper-left dot (the one with highest inference time), you should see something like this in your REPL:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"spelltype(::Type{T}) where T in Main at REPL[1]:6 (586 specializations)","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"This tells you the method corresponding to this dot. Moreover, mref (one of the outputs of pgdsgui) holds this method:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> mref[]\nspelltype(::Type{T}) where T in Main at REPL[1]:6","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"What are the specializations, and how costly was each?","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> collect_for(mref[], tinf)\n586-element Vector{SnoopCompileCore.InferenceTimingNode}:\n InferenceTimingNode: 0.003486/0.020872 on InferenceFrameInfo for spelltype(::Type{T}) where T with 7 direct children\n InferenceTimingNode: 0.003281/0.003892 on InferenceFrameInfo for spelltype(::Type{AbstractArray}) with 2 direct children\n InferenceTimingNode: 0.003349/0.004023 on InferenceFrameInfo for spelltype(::Type{AbstractChannel}) with 2 direct children\n InferenceTimingNode: 0.000827/0.001154 on InferenceFrameInfo for spelltype(::Type{AbstractChar}) with 5 direct children\n InferenceTimingNode: 0.003326/0.004070 on InferenceFrameInfo for spelltype(::Type{AbstractDict}) with 2 direct children\n InferenceTimingNode: 0.000833/0.001159 on InferenceFrameInfo for spelltype(::Type{AbstractDisplay}) with 5 direct children\n⋮\n InferenceTimingNode: 0.000848/0.001160 on InferenceFrameInfo for spelltype(::Type{YAML.Span}) with 5 direct children\n InferenceTimingNode: 0.000838/0.001148 on InferenceFrameInfo for spelltype(::Type{YAML.Token}) with 5 direct children\n InferenceTimingNode: 0.000833/0.001150 on InferenceFrameInfo for spelltype(::Type{YAML.TokenStream}) with 5 direct children\n InferenceTimingNode: 0.000809/0.001126 on InferenceFrameInfo for spelltype(::Type{YAML.YAMLDocIterator}) with 5 direct children","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"So we can see that one MethodInstance for each type in Ts was generated.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"If you see a list of MethodInstances, and the first is extremely costly in terms of inclusive time, but all the rest are not, then you might not need to worry much about over-specialization: your inference time will be dominated by that one costly method (often, the first time the method was called), and the fact that lots of additional specializations were generated may not be anything to worry about. However, in this case, the distribution of time is fairly flat, each contributing a small portion to the overall time. In such cases, over-specialization may be a problem.","category":"page"},{"location":"tutorials/pgdsgui/#Reducing-specialization-with-@nospecialize","page":"Profile-guided despecialization","title":"Reducing specialization with @nospecialize","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"How might we change this? To reduce the number of specializations of spelltype, we use @nospecialize in its definition:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"function spelltype(@nospecialize(T::Type))\n    name = Base.unwrap_unionall(T).name.name\n    str = \"\"\n    for c in string(name)\n        str *= c\n    end\n    return str\nend","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"warning: Warning\nwhere type-parameters force specialization: in spelltype(@nospecialize(::Type{T})) where T, the @nospecialize has no impact and you'll get full specialization on T. Instead, use @nospecialize(T::Type) (without the where statement) as shown.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"If we now rerun that demo, you should see a plot of the same kind as shown above, but with different costs for each dot. The differences are best appreciated comparing them side-by-side (pgdsgui allows you to specify a particular axis into which to plot):","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"(Image: pgdsgui-compare)","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"The results with @nospecialize are shown on the right. You can see that:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Now, the most expensive-to-infer method is <0.01s (formerly it was ~1s)\nNo method has more than 2 specializations","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Moreover, our runtimes (post-compilation) really aren't very different, both in the ballpark of a few millseconds (you can check with @btime from BenchmarkTools to be sure).","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In total, we've reduced compilation time approximately 50× without appreciably hurting runtime performance. Reducing specialization, when appropriate, can often yield your biggest reductions in latency.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"tip: Tip\nWhen you add @nospecialize, sometimes it's beneficial to compensate for the loss of inferrability by adding some type assertions. This topic will be discussed in greater detail in the next section, but for the example above we can improve runtime performance by annotating the return type of Base.unwrap_unionall(T): name = (Base.unwrap_unionall(T)::DataType).name.name. Then, later lines in spelltype know that name is a Symbol.With this change, the unspecialized variant outperforms the specialized variant in both compile-time and run-time. The reason is that the specialized variant of spell needs to be called by runtime dispatch, whereas for the unspecialized variant there's only one MethodInstance, so its dispatch is handled at compile time.","category":"page"},{"location":"tutorials/pgdsgui/#Blocking-inference:-Base.@nospecializeinfer","page":"Profile-guided despecialization","title":"Blocking inference: Base.@nospecializeinfer","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Perhaps surprisingly, @nospecialize doesn't prevent Julia's type-inference from inspecting a method. The reason is that it's sometimes useful if the caller knows what type will be returned, even if the callee doesn't exploit this information. In our mappushes example, this isn't an issue, because Ts is a Vector{Any} and this already defeats inference. But in other cases, the caller may be inferable but (to save inference time) you'd prefer to block inference from inspecting the method.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Beginning with Julia 1.10, you can prevent even inference from \"looking at\" @nospecialized arguments with Base.@nospecializeinfer:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Base.@nospecializeinfer function spelltype(@nospecialize(T::Type))\n    name = (Base.unwrap_unionall(T)::DataType).name.name\n    str = \"\"\n    for c in string(name)\n        str *= c\n    end\n    return str\nend","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Note that the ::DataType annotation described in the tip above is still effective and recommended. @nospecializeinfer directly affects only arguments that are marked with @nospecialize, and in this case the type-assertion prevents type uncertainty from propagating to the remainder of the function.","category":"page"},{"location":"tutorials/pgdsgui/#Argument-standardization","page":"Profile-guided despecialization","title":"Argument standardization","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"While not immediately relevant to the example above, a very important technique that falls within the domain of reducing specialization is argument standardization: instead of","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"function foo(x, y)\n    # some huge function, slow to compile, and you'd prefer not to compile it many times for different types of x and y\nend","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"consider whether you can safely write this as","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"function foo(x::X, y::Y)   # X and Y are concrete types\n    # some huge function, but the concrete typing ensures you only compile it once\nend\nfoo(x, y) = foo(convert(X, x)::X, convert(Y, y)::Y)   # this allows you to still call it with any argument types","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"The \"standardizing method\" foo(x, y) is short and therefore quick to compile, so it doesn't really matter if you compile many different instances.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"tip: Tip\nIn convert(X, x)::X, the final ::X guards against a broken convert method that fails to return an object of type X. Without it, foo(x, y) might call itself in an infinite loop, ultimately triggering a StackOverflowError. StackOverflowErrors are a particularly nasty form of error, and the typeassert ensures that you get a simple TypeError instead.In other contexts, such typeasserts would also have the effect of fixing inference problems even if the type of x is not well-inferred (this will be discussed in more detail later), but in this case dispatch to foo(x::X, y::Y) would have ensured the same outcome.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"There are of course cases where you can't implement your code in this way: after all, part of the power of Julia is the ability of generic methods to \"do the right thing\" for a wide variety of types. But in cases where you're doing a standard task, e.g., writing some data to a file, there's really no good reason to recompile your save method for a filename encoded as a String and again for a SubString{String} and again for a SubstitutionString and again for an AbstractString and ...: after all, the core of the save method probably isn't sensitive to the precise encoding of the filename.  In such cases, it should be safe to convert all filenames to String, thereby reducing the diversity of input arguments for expensive-to-compile methods.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"If you're using pgdsgui, the cost of inference and the number of specializations may guide you to click on specific dots; collect_for(mref[], tinf) then allows you to detect and diagnose cases where argument standardization might be helpful.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"You can do the same analysis without pgdsgui. The opportunity for argument standardization is often facilitated by looking at, e.g.,","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> tms = accumulate_by_source(flatten(tinf));  # collect all MethodInstances that belong to the same Method\n\njulia> t, m = tms[end-1]        # the ones towards the end take the most time, maybe they are over-specialized?\n(0.4138147, save(filename::AbstractString, data) in SomePkg at /pathto/SomePkg/src/SomePkg.jl:23)\n\njulia> methodinstances(m)       # let's see what specializations we have\n7-element Vector{Core.MethodInstance}:\n MethodInstance for save(::String, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::SubString{String}, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::AbstractString, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::String, ::Vector{SomePkg.SomeDataType{SubString{String}}})\n MethodInstance for save(::SubString{String}, ::Array)\n MethodInstance for save(::String, ::Vector{var\"#s92\"} where var\"#s92\"<:SomePkg.SomeDataType)\n MethodInstance for save(::String, ::Array)","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In this case we have 7 MethodInstances (some of which are clearly due to poor inferrability of the caller) when one might suffice.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Data-collection","page":"Reference","title":"Data collection","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"SnoopCompileCore.@snoop_invalidations\nSnoopCompileCore.@snoop_inference\nSnoopCompileCore.@snoop_llvm","category":"page"},{"location":"reference/#SnoopCompileCore.@snoop_invalidations","page":"Reference","title":"SnoopCompileCore.@snoop_invalidations","text":"invs = @snoop_invalidations expr\n\nCapture method cache invalidations triggered by evaluating expr. invs is a sequence of invalidated Core.MethodInstances together with \"explanations,\" consisting of integers (encoding depth) and strings (documenting the source of an invalidation).\n\nUnless you are working at a low level, you essentially always want to pass invs directly to SnoopCompile.invalidation_trees.\n\nExtended help\n\ninvs is in a format where the \"reason\" comes after the items. Method deletion results in the sequence\n\n[zero or more (mi, \"invalidate_mt_cache\") pairs..., zero or more (depth1 tree, loctag) pairs..., method, loctag] with loctag = \"jl_method_table_disable\"\n\nwhere mi means a MethodInstance. depth1 means a sequence starting at depth=1.\n\nMethod insertion results in the sequence\n\n[zero or more (depth0 tree, sig) pairs..., same info as with delete_method except loctag = \"jl_method_table_insert\"]\n\nThe authoritative reference is Julia's own src/gf.c file.\n\n\n\n\n\n","category":"macro"},{"location":"reference/#SnoopCompileCore.@snoop_inference","page":"Reference","title":"SnoopCompileCore.@snoop_inference","text":"tinf = @snoop_inference commands;\n\nProduce a profile of julia's type inference, recording the amount of time spent inferring every MethodInstance processed while executing commands. Each fresh entrance to type inference (whether executed directly in commands or because a call was made by runtime-dispatch) also collects a backtrace so the caller can be identified.\n\ntinf is a tree, each node containing data on a particular inference \"frame\" (the method, argument-type specializations, parameters, and even any constant-propagated values). Each reports the exclusive/inclusive times, where the exclusive time corresponds to the time spent inferring this frame in and of itself, whereas the inclusive time includes the time needed to infer all the callees of this frame.\n\nThe top-level node in this profile tree is ROOT. Uniquely, its exclusive time corresponds to the time spent not in julia's type inference (codegen, llvm_opt, runtime, etc).\n\nWorking with tinf effectively requires loading SnoopCompile.\n\nwarning: Warning\nNote the semicolon ; at the end of the @snoop_inference macro call. Because SnoopCompileCore is not permitted to invalidate any code, it cannot define the Base.show methods that pretty-print tinf. Defer inspection of tinf until SnoopCompile has been loaded.\n\nExample\n\njulia> tinf = @snoop_inference begin\n           sort(rand(100))  # Evaluate some code and profile julia's type inference\n       end;\n\n\n\n\n\n","category":"macro"},{"location":"reference/#SnoopCompileCore.@snoop_llvm","page":"Reference","title":"SnoopCompileCore.@snoop_llvm","text":"@snoop_llvm \"func_names.csv\" \"llvm_timings.yaml\" begin\n    # Commands to execute, in a new process\nend\n\ncauses the julia compiler to log timing information for LLVM optimization during the provided commands to the files \"funcnames.csv\" and \"llvmtimings.yaml\". These files can be used for the input to SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\").\n\nThe logs contain the amount of time spent optimizing each \"llvm module\", and information about each module, where a module is a collection of functions being optimized together.\n\n\n\n\n\n","category":"macro"},{"location":"reference/#GUIs","page":"Reference","title":"GUIs","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"flamegraph\npgdsgui","category":"page"},{"location":"reference/#SnoopCompile.flamegraph","page":"Reference","title":"SnoopCompile.flamegraph","text":"flamegraph(tinf::InferenceTimingNode; tmin=0.0, excluded_modules=Set([Main]), mode=nothing)\n\nConvert the call tree of inference timings returned from @snoop_inference into a FlameGraph. Returns a FlameGraphs.FlameGraph structure that represents the timing trace recorded for type inference.\n\nFrames that take less than tmin seconds of inclusive time will not be included in the resultant FlameGraph (meaning total time including it and all of its children). This can be helpful if you have a very big profile, to save on processing time.\n\nNon-precompilable frames are marked in reddish colors. excluded_modules can be used to mark methods defined in modules to which you cannot or do not wish to add precompiles.\n\nmode controls how frames are named in tools like ProfileView. nothing uses the default of just the qualified function name, whereas supplying mode=Dict(method => count) counting the number of specializations of each method will cause the number of specializations to be included in the frame name.\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.002148974/0.002767166 on Core.Compiler.Timings.ROOT() with 1 direct children\n\njulia> fg = flamegraph(tinf)\nNode(FlameGraphs.NodeData(ROOT() at typeinfer.jl:75, 0x00, 0:3334431))\n\njulia> ProfileView.view(fg);  # Display the FlameGraph in a package that supports it\n\nYou should be able to reconcile the resulting flamegraph to print_tree(tinf) (see flatten).\n\nThe empty horizontal periods in the flamegraph correspond to times when something other than inference is running. The total width of the flamegraph is set from the ROOT node.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.pgdsgui","page":"Reference","title":"SnoopCompile.pgdsgui","text":"methodref, ax = pgdsgui(tinf::InferenceTimingNode; consts::Bool=true, by=inclusive)\nmethodref     = pgdsgui(ax, tinf::InferenceTimingNode; kwargs...)\n\nCreate a scatter plot comparing:     - (vertical axis) the inference time for all instances of each Method, as captured by tinf;     - (horizontal axis) the run time cost, as estimated by capturing a @profile before calling this function.\n\nEach dot corresponds to a single method. The face color encodes the number of times that method was inferred, and the edge color corresponds to the fraction of the runtime spent on runtime dispatch (black is 0%, bright red is 100%). Clicking on a dot prints the method (or location, if inlined) to the REPL, and sets methodref[] to that method.\n\nax is the pyplot axis of the scatterplot.\n\ncompat: Compat\npgdsgui depends on PyPlot via the Requires.jl package. You must load both SnoopCompile and PyPlot for this function to be defined.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Analysis-of-invalidations","page":"Reference","title":"Analysis of invalidations","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"uinvalidated\ninvalidation_trees\nprecompile_blockers\nfiltermod\nfindcaller\nreport_invalidations","category":"page"},{"location":"reference/#SnoopCompile.uinvalidated","page":"Reference","title":"SnoopCompile.uinvalidated","text":"umis = uinvalidated(invlist)\n\nReturn the unique invalidated MethodInstances. invlist is obtained from SnoopCompileCore.@snoop_invalidations. This is similar to filtering for MethodInstances in invlist, except that it discards any tagged \"invalidate_mt_cache\". These can typically be ignored because they are nearly inconsequential: they do not invalidate any compiled code, they only transiently affect an optimization of runtime dispatch.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.invalidation_trees","page":"Reference","title":"SnoopCompile.invalidation_trees","text":"trees = invalidation_trees(list)\n\nParse list, as captured by SnoopCompileCore.@snoop_invalidations, into a set of invalidation trees, where parents nodes were called by their children.\n\nExample\n\njulia> f(x::Int)  = 1\nf (generic function with 1 method)\n\njulia> f(x::Bool) = 2\nf (generic function with 2 methods)\n\njulia> applyf(container) = f(container[1])\napplyf (generic function with 1 method)\n\njulia> callapplyf(container) = applyf(container)\ncallapplyf (generic function with 1 method)\n\njulia> c = Any[1]\n1-element Array{Any,1}:\n 1\n\njulia> callapplyf(c)\n1\n\njulia> trees = invalidation_trees(@snoop_invalidations f(::AbstractFloat) = 3)\n1-element Array{SnoopCompile.MethodInvalidations,1}:\n inserting f(::AbstractFloat) in Main at REPL[36]:1 invalidated:\n   mt_backedges: 1: signature Tuple{typeof(f),Any} triggered MethodInstance for applyf(::Array{Any,1}) (1 children) more specific\n\nSee the documentation for further details.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.precompile_blockers","page":"Reference","title":"SnoopCompile.precompile_blockers","text":"staletrees = precompile_blockers(invalidations, tinf::InferenceTimingNode)\n\nSelect just those invalidations that contribute to \"stale nodes\" in tinf, and link them together. This can allow one to identify specific blockers of precompilation for particular MethodInstances.\n\nExample\n\nusing SnoopCompileCore\ninvalidations = @snoop_invalidations using PkgA, PkgB;\nusing SnoopCompile\ntrees = invalidation_trees(invalidations)\ntinf = @snoop_inference begin\n    some_workload()\nend\nstaletrees = precompile_blockers(trees, tinf)\n\nIn many cases, this reduces the number of invalidations that require analysis by one or more orders of magnitude.\n\ninfo: Info\nprecompile_blockers is experimental and has not yet been thoroughly vetted by real-world use. Users are encouraged to try it and report any \"misses\" or unnecessary \"hits.\"\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.filtermod","page":"Reference","title":"SnoopCompile.filtermod","text":"modtrigs = filtermod(mod::Module, mtrigs::AbstractVector{MethodTriggers})\n\nSelect just the method-based triggers arising from a particular module.\n\n\n\n\n\nthinned = filtermod(module, trees::AbstractVector{MethodInvalidations}; recursive=false)\n\nSelect just the cases of invalidating a method defined in module.\n\nIf recursive is false, only the roots of trees are examined (i.e., the proximal source of the invalidation must be in module). If recursive is true, then thinned contains all routes to a method in module.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.findcaller","page":"Reference","title":"SnoopCompile.findcaller","text":"methinvs = findcaller(method::Method, trees)\n\nFind a path through trees that reaches method. Returns a single MethodInvalidations object.\n\nExamples\n\nSuppose you know that loading package SomePkg triggers invalidation of f(data). You can find the specific source of invalidation as follows:\n\nf(data)                             # run once to force compilation\nm = @which f(data)\nusing SnoopCompile\ntrees = invalidation_trees(@snoop_invalidations using SomePkg)\nmethinvs = findcaller(m, trees)\n\nIf you don't know which method to look for, but know some operation that has had added latency, you can look for methods using @snoopi. For example, suppose that loading SomePkg makes the next using statement slow. You can find the source of trouble with\n\njulia> using SnoopCompile\n\njulia> trees = invalidation_trees(@snoop_invalidations using SomePkg);\n\njulia> tinf = @snoopi using SomePkg            # this second `using` will need to recompile code invalidated above\n1-element Array{Tuple{Float64,Core.MethodInstance},1}:\n (0.08518409729003906, MethodInstance for require(::Module, ::Symbol))\n\njulia> m = tinf[1][2].def\nrequire(into::Module, mod::Symbol) in Base at loading.jl:887\n\njulia> findcaller(m, trees)\ninserting ==(x, y::SomeType) in SomeOtherPkg at /path/to/code:100 invalidated:\n   backedges: 1: superseding ==(x, y) in Base at operators.jl:83 with MethodInstance for ==(::Symbol, ::Any) (16 children) more specific\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.report_invalidations","page":"Reference","title":"SnoopCompile.report_invalidations","text":"report_invalidations(\n    io::IO = stdout;\n    invalidations,\n    n_rows::Int = 10,\n    process_filename::Function = x -> x,\n)\n\nPrint a tabular summary of invalidations given:\n\ninvalidations the output of SnoopCompileCore.@snoop_invalidations\n\nand (optionally)\n\nio::IO IO stream. Defaults to stdout\nn_rows::Int the number of rows to be displayed in the truncated table. A value of 0 indicates no truncation. A positive value will truncate the table to the specified number of rows.\nprocess_filename(::String)::String a function to post-process each filename, where invalidations are found\n\nExample usage\n\nimport SnoopCompileCore\ninvalidations = SnoopCompileCore.@snoop_invalidations begin\n\n    # load packages & define any additional methods\n\nend;\n\nusing SnoopCompile\nusing PrettyTables # to load report_invalidations\nreport_invalidations(;invalidations)\n\nUsing report_invalidations requires that you first load the PrettyTables.jl package.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Analysis-of-@snoop_inference","page":"Reference","title":"Analysis of @snoop_inference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"flatten\nexclusive\ninclusive\naccumulate_by_source\ncollect_for\nstaleinstances\ninference_triggers\ntrigger_tree\nsuggest\nisignorable\ncallerinstance\ncallingframe\nskiphigherorder\nInferenceTrigger\nruntime_inferencetime\nSnoopCompile.parcel\nSnoopCompile.write\nreport_callee\nreport_callees\nreport_caller","category":"page"},{"location":"reference/#SnoopCompile.flatten","page":"Reference","title":"SnoopCompile.flatten","text":"flatten(tinf; tmin = 0.0, sortby=exclusive)\n\nFlatten the execution graph of InferenceTimingNodes returned from @snoop_inference into a Vector of InferenceTiming frames, each encoding the time needed for inference of a single MethodInstance. By default, results are sorted by exclusive time (the time for inferring the MethodInstance itself, not including any inference of its callees); other options are sortedby=inclusive which includes the time needed for the callees, or nothing to obtain them in the order they were inferred (depth-first order).\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.002148974/0.002767166 on Core.Compiler.Timings.ROOT() with 1 direct children\n\njulia> using AbstractTrees; print_tree(tinf)\nInferenceTimingNode: 0.00242354/0.00303526 on Core.Compiler.Timings.ROOT() with 1 direct children\n└─ InferenceTimingNode: 0.000150891/0.000611721 on SnoopCompile.FlattenDemo.packintype(::Int64) with 2 direct children\n   ├─ InferenceTimingNode: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64) with 0 direct children\n   └─ InferenceTimingNode: 9.43e-5/0.000355512 on SnoopCompile.FlattenDemo.dostuff(::SnoopCompile.FlattenDemo.MyType{Int64}) with 2 direct children\n      ├─ InferenceTimingNode: 6.6458e-5/0.000124716 on SnoopCompile.FlattenDemo.extract(::SnoopCompile.FlattenDemo.MyType{Int64}) with 2 direct children\n      │  ├─ InferenceTimingNode: 3.401e-5/3.401e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, ::Symbol) with 0 direct children\n      │  └─ InferenceTimingNode: 2.4248e-5/2.4248e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, x::Symbol) with 0 direct children\n      └─ InferenceTimingNode: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64) with 0 direct children\n\nNote the printing of getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, x::Symbol): it shows the specific Symbol, here :x, that getproperty was inferred with. This reflects constant-propagation in inference.\n\nThen:\n\njulia> flatten(tinf; sortby=nothing)\n8-element Vector{SnoopCompileCore.InferenceTiming}:\n InferenceTiming: 0.002423543/0.0030352639999999998 on Core.Compiler.Timings.ROOT()\n InferenceTiming: 0.000150891/0.0006117210000000001 on SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 9.43e-5/0.00035551200000000005 on SnoopCompile.FlattenDemo.dostuff(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 6.6458e-5/0.000124716 on SnoopCompile.FlattenDemo.extract(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 3.401e-5/3.401e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, ::Symbol)\n InferenceTiming: 2.4248e-5/2.4248e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, x::Symbol)\n InferenceTiming: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64)\n\njulia> flatten(tinf; tmin=1e-4)                        # sorts by exclusive time (the time before the '/')\n4-element Vector{SnoopCompileCore.InferenceTiming}:\n InferenceTiming: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64)\n InferenceTiming: 0.000150891/0.0006117210000000001 on SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.002423543/0.0030352639999999998 on Core.Compiler.Timings.ROOT()\n\njulia> flatten(tinf; sortby=inclusive, tmin=1e-4)      # sorts by inclusive time (the time after the '/')\n6-element Vector{SnoopCompileCore.InferenceTiming}:\n InferenceTiming: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 6.6458e-5/0.000124716 on SnoopCompile.FlattenDemo.extract(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64)\n InferenceTiming: 9.43e-5/0.00035551200000000005 on SnoopCompile.FlattenDemo.dostuff(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 0.000150891/0.0006117210000000001 on SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.002423543/0.0030352639999999998 on Core.Compiler.Timings.ROOT()\n\nAs you can see, sortby affects not just the order but also the selection of frames; with exclusive times, dostuff did not on its own rise above threshold, but it does when using inclusive times.\n\nSee also: accumulate_by_source.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompileCore.exclusive","page":"Reference","title":"SnoopCompileCore.exclusive","text":"exclusive(frame)\n\nReturn the time spent inferring frame, not including the time needed for any of its callees.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompileCore.inclusive","page":"Reference","title":"SnoopCompileCore.inclusive","text":"inclusive(frame)\n\nReturn the time spent inferring frame and its callees.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.accumulate_by_source","page":"Reference","title":"SnoopCompile.accumulate_by_source","text":"accumulate_by_source(flattened; tmin = 0.0, by=exclusive)\n\nAdd the inference timings for all MethodInstances of a single Method together. flattened is the output of flatten. Returns a list of (t, method) tuples.\n\nWhen the accumulated time for a Method is large, but each instance is small, it indicates that it is being inferred for many specializations (which might include specializations with different constants).\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.004978/0.005447 on Core.Compiler.Timings.ROOT() with 1 direct children\n\njulia> accumulate_by_source(flatten(tinf))\n7-element Vector{Tuple{Float64, Union{Method, Core.MethodInstance}}}:\n (4.6294999999999996e-5, getproperty(x, f::Symbol) @ Base Base.jl:37)\n (5.8965e-5, dostuff(y) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:45)\n (6.4141e-5, extract(y::SnoopCompile.FlattenDemo.MyType) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:36)\n (8.9997e-5, (var\"#ctor-self#\"::Type{SnoopCompile.FlattenDemo.MyType{T}} where T)(x) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:35)\n (9.2256e-5, domath(x) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:41)\n (0.000117514, packintype(x) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:37)\n (0.004977755, ROOT() @ Core.Compiler.Timings compiler/typeinfer.jl:79)\n\nCompared to the output from flatten, the two inferences passes on getproperty have been consolidated into a single aggregate call.\n\n\n\n\n\nmtrigs = accumulate_by_source(Method, itrigs::AbstractVector{InferenceTrigger})\n\nConsolidate inference triggers via their caller method. mtrigs is a vector of Method=>list pairs, where list is a list of InferenceTriggers.\n\n\n\n\n\nloctrigs = accumulate_by_source(itrigs::AbstractVector{InferenceTrigger})\n\nAggregate inference triggers by location (function, file, and line number) of the caller.\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_demo:\n\njulia> itrigs = inference_triggers(SnoopCompile.itrigs_demo())\n2-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:763)\n Inference triggered to call MethodInstance for double(::Float64) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:763)\n\njulia> accumulate_by_source(itrigs)\n1-element Vector{SnoopCompile.LocationTriggers}:\n    calldouble1 at /pathto/SnoopCompile/src/parcel_snoop_inference.jl:762 (2 callees from 1 callers)\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.collect_for","page":"Reference","title":"SnoopCompile.collect_for","text":"list = collect_for(m::Method, tinf::InferenceTimingNode)\nlist = collect_for(m::MethodInstance, tinf::InferenceTimingNode)\n\nCollect all InferenceTimingNodes (descendants of tinf) that match m.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.staleinstances","page":"Reference","title":"SnoopCompile.staleinstances","text":"staleinstances(tinf::InferenceTimingNode)\n\nReturn a list of InferenceTimingNodes corresponding to MethodInstances that have \"stale\" code (specifically, CodeInstances with outdated max_world world ages). These may be a hint that invalidation occurred while running the workload provided to @snoop_inference, and consequently an important origin of (re)inference.\n\nwarning: Warning\nstaleinstances only looks retrospectively for stale code; it does not distinguish whether the code became stale while running @snoop_inference from whether it was already stale before execution commenced.\n\nWhile staleinstances is recommended as a useful \"sanity check\" to run before performing a detailed analysis of inference, any serious examination of invalidation should use @snoop_invalidations.\n\nFor more information about world age, see https://docs.julialang.org/en/v1/manual/methods/#Redefining-Methods.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.inference_triggers","page":"Reference","title":"SnoopCompile.inference_triggers","text":"itrigs = inference_triggers(tinf::InferenceTimingNode; exclude_toplevel=true)\n\nCollect the \"triggers\" of inference, each a fresh entry into inference via a call dispatched at runtime. All the entries in itrigs are previously uninferred, or are freshly-inferred for specific constant inputs.\n\nexclude_toplevel determines whether calls made from the REPL, include, or test suites are excluded.\n\nExample\n\nWe'll use SnoopCompile.itrigs_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.itrigs_demo()\nInferenceTimingNode: 0.004490576/0.004711168 on Core.Compiler.Timings.ROOT() with 2 direct children\n\njulia> itrigs = inference_triggers(tinf)\n2-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/inference_demos.jl:86) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/inference_demos.jl:87)\n Inference triggered to call MethodInstance for double(::Float64) from calldouble1 (/pathto/SnoopCompile/src/inference_demos.jl:86) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/inference_demos.jl:87)\n\njulia> edit(itrigs[1])     # opens an editor at the spot in the caller\n\njulia> using Cthulhu\n\njulia> ascend(itrigs[2])   # use Cthulhu to inspect the stacktrace (caller is the second item in the trace)\nChoose a call for analysis (q to quit):\n >   double(::Float64)\n       calldouble1 at /pathto/SnoopCompile/src/inference_demos.jl:86 => calldouble2(::Vector{Vector{Any}}) at /pathto/SnoopCompile/src/inference_demos.jl:87\n         calleach(::Vector{Vector{Vector{Any}}}) at /pathto/SnoopCompile/src/inference_demos.jl:88\n...\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.trigger_tree","page":"Reference","title":"SnoopCompile.trigger_tree","text":"root = trigger_tree(itrigs)\n\nOrganize inference triggers itrigs in tree format, grouping items via the call tree.\n\nIt is a tree rather than a more general graph due to the fact that caching inference results means that each node gets visited only once.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.suggest","page":"Reference","title":"SnoopCompile.suggest","text":"suggest(itrig::InferenceTrigger)\n\nAnalyze itrig and attempt to suggest an interpretation or remedy. This returns a structure of type Suggested; the easiest thing to do with the result is to show it; however, you can also filter a list of suggestions.\n\nExample\n\njulia> itrigs = inference_triggers(tinf);\n\njulia> sugs = suggest.(itrigs);\n\njulia> sugs_important = filter(!isignorable, sugs)    # discard the ones that probably don't need to be addressed\n\nwarning: Warning\nSuggestions are approximate at best; most often, the proposed fixes should not be taken literally, but instead taken as a hint about the \"outcome\" of a particular runtime dispatch incident. The suggestions target calls made with non-inferrable argumets, but often the best place to fix the problem is at an earlier stage in the code, where the argument was first computed.You can get much deeper insight via ascend (and Cthulhu generally), and even stacktrace is often useful. Suggestions are intended to be a quick and easier-to-comprehend first pass at analyzing an inference trigger.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.isignorable","page":"Reference","title":"SnoopCompile.isignorable","text":"isignorable(s::Suggested)\n\nReturns true if s is unlikely to be an inference problem in need of fixing.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.callerinstance","page":"Reference","title":"SnoopCompile.callerinstance","text":"mi = callerinstance(itrig::InferenceTrigger)\n\nReturn the MethodInstance mi of the caller in the selected stackframe in itrig.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.callingframe","page":"Reference","title":"SnoopCompile.callingframe","text":"itrigcaller = callingframe(itrig::InferenceTrigger)\n\n\"Step out\" one layer of the stacktrace, referencing the caller of the current frame of itrig.\n\nYou can retrieve the proximal trigger of inference with InferenceTrigger(itrigcaller).\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_demo:\n\njulia> itrig = inference_triggers(SnoopCompile.itrigs_demo())[1]\nInference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:763)\n\njulia> itrigcaller = callingframe(itrig)\nInference triggered to call MethodInstance for double(::UInt8) from calleach (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:764) with specialization MethodInstance for calleach(::Vector{Vector{Vector{Any}}})\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.skiphigherorder","page":"Reference","title":"SnoopCompile.skiphigherorder","text":"itrignew = skiphigherorder(itrig; exact::Bool=false)\n\nAttempt to skip over frames of higher-order functions that take the callee as a function-argument. This can be useful if you're analyzing inference triggers for an entire package and would prefer to assign triggers to package-code rather than Base functions like map!, broadcast, etc.\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_higherorder_demo:\n\njulia> itrig = inference_triggers(SnoopCompile.itrigs_higherorder_demo())[1]\nInference triggered to call MethodInstance for double(::Float64) from mymap! (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:706) with specialization MethodInstance for mymap!(::typeof(SnoopCompile.ItrigHigherOrderDemo.double), ::Vector{Any}, ::Vector{Any})\n\njulia> callingframe(itrig)      # step out one (non-inlined) frame\nInference triggered to call MethodInstance for double(::Float64) from mymap (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:710) with specialization MethodInstance for mymap(::typeof(SnoopCompile.ItrigHigherOrderDemo.double), ::Vector{Any})\n\njulia> skiphigherorder(itrig)   # step out to frame that doesn't have `double` as a function-argument\nInference triggered to call MethodInstance for double(::Float64) from callmymap (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:711) with specialization MethodInstance for callmymap(::Vector{Any})\n\nwarn: Warn\nBy default skiphigherorder is conservative, and insists on being sure that it's the callee being passed to the higher-order function. Higher-order functions that do not get specialized (e.g., with ::Function argument types) will not be skipped over. You can pass exact=false to allow ::Function to also be passed over, but keep in mind that this may falsely skip some frames.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.InferenceTrigger","page":"Reference","title":"SnoopCompile.InferenceTrigger","text":"InferenceTrigger(callee::MethodInstance, callerframes::Vector{StackFrame}, btidx::Int, bt)\n\nOrganize information about the \"triggers\" of inference. callee is the MethodInstance requiring inference, callerframes, btidx and bt contain information about the caller. callerframes are the frame(s) of call site that triggered inference; it's a Vector{StackFrame}, rather than a single StackFrame, due to the possibility that the caller was inlined into something else, in which case the first entry is the direct caller and the last entry corresponds to the MethodInstance into which it was ultimately inlined. btidx is the index in bt, the backtrace collected upon entry into inference, corresponding to callerframes.\n\nInferenceTriggers are created by calling inference_triggers. See also: callerinstance and callingframe.\n\n\n\n\n\n","category":"type"},{"location":"reference/#SnoopCompile.runtime_inferencetime","page":"Reference","title":"SnoopCompile.runtime_inferencetime","text":"ridata = runtime_inferencetime(tinf::InferenceTimingNode; consts=true, by=inclusive)\nridata = runtime_inferencetime(tinf::InferenceTimingNode, profiledata; lidict, consts=true, by=inclusive)\n\nCompare runtime and inference-time on a per-method basis. ridata[m::Method] returns (trun, tinfer, nspecializations), measuring the approximate amount of time spent running m, inferring m, and the number of type-specializations, respectively. trun is estimated from profiling data, which the user is responsible for capturing before the call. Typically tinf is collected via @snoop_inference on the first call (in a fresh session) to a workload, and the profiling data collected on a subsequent call. In some cases you may need to repeat the workload several times to collect enough profiling samples.\n\nprofiledata and lidict are obtained from Profile.retrieve().\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.parcel","page":"Reference","title":"SnoopCompile.parcel","text":"ttot, pcs = SnoopCompile.parcel(tinf::InferenceTimingNode)\n\nParcel the \"root-most\" precompilable MethodInstances into separate modules. These can be used to generate precompile directives to cache the results of type-inference, reducing latency on first use.\n\nLoosely speaking, and MethodInstance is precompilable if the module that owns the method also has access to all the types it need to precompile the instance. When the root node of an entrance to inference is not itself precompilable, parcel examines the children (and possibly, children's children...) until it finds the first node on each branch that is precompilable. MethodInstances are then assigned to the module that owns the method.\n\nttot is the total inference time; pcs is a list of module => (tmod, pclist) pairs. For each module, tmod is the amount of inference time affiliated with methods owned by that module; pclist is a list of (t, mi) time/MethodInstance tuples.\n\nSee also: SnoopCompile.write.\n\nExample\n\nWe'll use SnoopCompile.itrigs_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.itrigs_demo()\nInferenceTimingNode: 0.004490576/0.004711168 on Core.Compiler.Timings.ROOT() with 2 direct children\n\njulia> ttot, pcs = SnoopCompile.parcel(tinf);\n\njulia> ttot\n0.000220592\n\njulia> pcs\n1-element Vector{Pair{Module, Tuple{Float64, Vector{Tuple{Float64, Core.MethodInstance}}}}}:\n SnoopCompile.ItrigDemo => (0.000220592, [(9.8986e-5, MethodInstance for double(::Float64)), (0.000121606, MethodInstance for double(::UInt8))])\n\nSince there was only one module, ttot is the same as tmod. The ItrigDemo module had two precomilable MethodInstances, each listed with its corresponding inclusive time.\n\n\n\n\n\nmodtrigs = SnoopCompile.parcel(mtrigs::AbstractVector{MethodTriggers})\n\nSplit method-based triggers into collections organized by the module in which the methods were defined. Returns a module => list vector, with the module having the most MethodTriggers last.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.write","page":"Reference","title":"SnoopCompile.write","text":"write(prefix::AbstractString, pc::Dict; always::Bool = false)\n\nWrite each modules' precompiles to a separate file.  If always is true, the generated function will always run the precompile statements when called, otherwise the statements will only be called during package precompilation.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.report_callee","page":"Reference","title":"SnoopCompile.report_callee","text":"To use report_callee do using JET\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.report_callees","page":"Reference","title":"SnoopCompile.report_callees","text":"To use report_callees do using JET\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.report_caller","page":"Reference","title":"SnoopCompile.report_caller","text":"To use report_caller do using JET\n\n\n\n\n\n","category":"function"},{"location":"reference/#Analysis-of-LLVM","page":"Reference","title":"Analysis of LLVM","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"SnoopCompile.read_snoop_llvm","category":"page"},{"location":"reference/#SnoopCompile.read_snoop_llvm","page":"Reference","title":"SnoopCompile.read_snoop_llvm","text":"times, info = SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\"; tmin_secs=0.0)\n\nReads the log file produced by the compiler and returns the structured representations.\n\nThe results will only contain modules that took longer than tmin_secs to optimize.\n\nReturn value\n\ntimes contains the time spent optimizing each module, as a Pair from the time to an\n\narray of Strings, one for every MethodInstance in that llvm module.\n\ninfo is a Dict containing statistics for each MethodInstance encountered, from before\n\nand after optimization, including number of instructions and number of basicblocks.\n\nExample\n\njulia> @snoop_llvm \"func_names.csv\" \"llvm_timings.yaml\" begin\n           using InteractiveUtils\n           @eval InteractiveUtils.peakflops()\n       end\nLaunching new julia process to run commands...\ndone.\n\njulia> times, info = SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\", tmin_secs = 0.025);\n\njulia> times\n3-element Vector{Pair{Float64, Vector{String}}}:\n 0.028170923 => [\"Tuple{typeof(LinearAlgebra.copy_transpose!), Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}, Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}}\"]\n 0.031356962 => [\"Tuple{typeof(Base.copyto!), Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}, Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}}\"]\n 0.149138788 => [\"Tuple{typeof(LinearAlgebra._generic_matmatmul!), Array{Float64, 2}, Char, Char, Array{Float64, 2}, Array{Float64, 2}, LinearAlgebra.MulAddMul{true, true, Bool, Bool}}\"]\n\njulia> info\nDict{String, NamedTuple{(:before, :after), Tuple{NamedTuple{(:instructions, :basicblocks), Tuple{Int64, Int64}}, NamedTuple{(:instructions, :basicblocks), Tuple{Int64, Int64}}}}} with 3 entries:\n  \"Tuple{typeof(LinearAlgebra.copy_transpose!), Ar… => (before = (instructions = 651, basicblocks = 83), after = (instructions = 348, basicblocks = 40…\n  \"Tuple{typeof(Base.copyto!), Array{Float64, 2}, … => (before = (instructions = 617, basicblocks = 77), after = (instructions = 397, basicblocks = 37…\n  \"Tuple{typeof(LinearAlgebra._generic_matmatmul!)… => (before = (instructions = 4796, basicblocks = 824), after = (instructions = 1421, basicblocks =…\n\n\n\n\n\n","category":"function"},{"location":"reference/#Demos","page":"Reference","title":"Demos","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"SnoopCompile.flatten_demo\nSnoopCompile.itrigs_demo\nSnoopCompile.itrigs_higherorder_demo","category":"page"},{"location":"reference/#SnoopCompile.flatten_demo","page":"Reference","title":"SnoopCompile.flatten_demo","text":"tinf = SnoopCompile.flatten_demo()\n\nA simple demonstration of @snoop_inference. This demo defines a module\n\nmodule FlattenDemo\n    struct MyType{T} x::T end\n    extract(y::MyType) = y.x\n    function packintype(x)\n        y = MyType{Int}(x)\n        return dostuff(y)\n    end\n    function domath(x)\n        y = x + x\n        return y*x + 2*x + 5\n    end\n    dostuff(y) = domath(extract(y))\nend\n\nIt then \"warms up\" (forces inference on) all of Julia's Base methods needed for domath, to ensure that these MethodInstances do not need to be inferred when we collect the data. It then returns the results of\n\n@snoop_inference FlattenDemo.packintypes(1)\n\nSee flatten for an example usage.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.itrigs_demo","page":"Reference","title":"SnoopCompile.itrigs_demo","text":"tinf = SnoopCompile.itrigs_demo()\n\nA simple demonstration of collecting inference triggers. This demo defines a module\n\nmodule ItrigDemo\n@noinline double(x) = 2x\n@inline calldouble1(c) = double(c[1])\ncalldouble2(cc) = calldouble1(cc[1])\ncalleach(ccs) = (calldouble2(ccs[1]), calldouble2(ccs[2]))\nend\n\nIt then \"warms up\" (forces inference on) calldouble2(::Vector{Vector{Any}}), calldouble1(::Vector{Any}), double(::Int):\n\ncc = [Any[1]]\nItrigDemo.calleach([cc,cc])\n\nThen it collects and returns inference data using\n\ncc1, cc2 = [Any[0x01]], [Any[1.0]]\n@snoop_inference ItrigDemo.calleach([cc1, cc2])\n\nThis does not require any new inference for calldouble2 or calldouble1, but it does force inference on double with two new types. See inference_triggers to see what gets collected and returned.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.itrigs_higherorder_demo","page":"Reference","title":"SnoopCompile.itrigs_higherorder_demo","text":"tinf = SnoopCompile.itrigs_higherorder_demo()\n\nA simple demonstration of handling higher-order methods with inference triggers. This demo defines a module\n\nmodule ItrigHigherOrderDemo\ndouble(x) = 2x\n@noinline function mymap!(f, dst, src)\n    for i in eachindex(dst, src)\n        dst[i] = f(src[i])\n    end\n    return dst\nend\n@noinline mymap(f::F, src) where F = mymap!(f, Vector{Any}(undef, length(src)), src)\ncallmymap(src) = mymap(double, src)\nend\n\nThe key feature of this set of definitions is that the function double gets passed as an argument through mymap and mymap! (the latter are higher-order functions).\n\nIt then \"warms up\" (forces inference on) callmymap(::Vector{Any}), mymap(::typeof(double), ::Vector{Any}), mymap!(::typeof(double), ::Vector{Any}, ::Vector{Any}) and double(::Int):\n\nItrigHigherOrderDemo.callmymap(Any[1, 2])\n\nThen it collects and returns inference data using\n\n@snoop_inference ItrigHigherOrderDemo.callmymap(Any[1.0, 2.0])\n\nwhich forces inference for double(::Float64).\n\nSee skiphigherorder for an example using this demo.\n\n\n\n\n\n","category":"function"},{"location":"explanations/gotchas/#Precompilation-\"gotcha\"s","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"","category":"section"},{"location":"explanations/gotchas/#running-during-pc","page":"Precompilation \"gotcha\"s","title":"Running code during module definition","text":"","category":"section"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"Suppose you're working on an astronomy package and your source code has a line","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"const planets = map(makeplanet, [\"Mercury\", ...])","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"Julia will dutifully create planets and store it in the package's precompile cache file. This also runs makeplanet, and if this is the first time it gets run, it will compile makeplanet. Assuming that makeplanet is a method defined in the package, the compiled code for makeplanet will be stored in the cache file.","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"However, two circumstances can lead to puzzling omissions from the cache files:","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"if makeplanet is a method defined in a dependency of your package, it will not be cached in your package. You'd want to add precompilation of makeplanet to the package that creates that method.\nif makeplanet is poorly-infered and uses runtime dispatch, any such callees that are not owned by your package will not be cached. For example, suppose makeplanet ends up calling methods in Base Julia or its standard libraries that are not precompiled into Julia itself: the compiled code for those methods will not be added to the cache file.","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"One option to ensure this dependent code gets cached is to create planets inside PrecompileTools.@compile_workload:","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"@compile_workload begin\n    global planets\n    const planet = map(makeplanet, [\"Mercury\", ...])\nend","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"Note that your package definition can have multiple @compile_workload blocks.","category":"page"},{"location":"explanations/basic/#Understanding-SnoopCompile-and-Julia's-compilation-pipeline","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"","category":"section"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"Julia uses Just-in-time (JIT) compilation to generate the code that runs on your CPU. Broadly speaking, there are two major compilation steps: inference and code generation. Inference is the process of determining the type of each object, which in turn determines which specific methods get called; once type inference is complete, code generation performs optimizations and ultimately generates the assembly language (native code) used on CPUs. Some aspects of this process are documented here.","category":"page"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"Using code that has never been compiled requires that it first be JIT-compiled, and this contributes to the latency of using the package. In some circumstances, you can cache (store) the results of compilation to files to reduce the latency when your package is used. These files are the the *.ji and *.so files that live in the compiled directory of your Julia depot, usually located at ~/.julia/compiled. However, if these files become large, loading them can be another source for latency. Julia needs time both to load and validate the cached compiled code. Minimizing the latency of using a package involves focusing on caching the compilation of code that is both commonly used and takes time to compile.","category":"page"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"Caching code for later use is called precompilation. Julia has had some forms of precompilation almost since the very first packages. However, it was Julia 1.9 that first supported \"complete\" precompilation, including the ability to store native code in shared-library cache files.","category":"page"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"SnoopCompile is designed to try to allow you to analyze the costs of JIT-compilation, identify key bottlenecks that contribute to latency, and set up precompile directives to see whether it produces measurable benefits.","category":"page"},{"location":"explanations/basic/#Package-precompilation","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Package precompilation","text":"","category":"section"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"When a package is precompiled, here's what happens under the hood:","category":"page"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"Julia loads all of the package's dependencies (the ones in the [deps] section of the Project.toml file), typically from precompile cache files\nJulia evaluates the source code (text files) that define the package module(s). Evaluating function foo(args...) ... end creates a new method foo. Note that:\nthe source code might also contain statements that create \"data\" (e.g., consts). In some cases this can lead to some subtle precompilation \"gotchas\"\nthe source code might also contain a precompile workload, which forces compilation and tracking of package methods.\nJulia iterates over the module contents and writes the result to disk. Note that the module contents might include compiled code, and if so it is written along with everything else to the cache file.","category":"page"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"When Julia loads your package, it just loads the \"snapshot\" stored in the cache file: it does not re-evaluate the source-text files that defined your package! It is appropriate to think of the source files of your package as \"build scripts\" that create your module; once the \"build scripts\" are executed, it's the module itself that gets cached, and the job of the build scripts is done.","category":"page"},{"location":"tutorials/invalidations/#Tutorial-on-@snoop_invalidations","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/#What-are-invalidations?","page":"Tutorial on @snoop_invalidations","title":"What are invalidations?","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"In this context, invalidation means discarding previously-compiled code. Invalidations occur because of interactions between independent pieces of code. Invalidations are essential to make Julia fast, interactive, and correct: you need invalidations if you want to be able to define some methods, run (compile) some code, and then in the same session define new methods that might lead to different answers if you were to recompile the code in the presence of the new methods.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Invalidations can happen just from loading packages. Packages are precompiled in isolation, but you can load many packages into a single interactive session. It's impossible for the individual packages to anticipate the full \"world of methods\" in your interactive session, so sometimes Julia has to discard code that was compiled in a smaller world because it's at risk for being incorrect in the larger world.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"The downside of invalidations is that they make latency worse, as code must be recompiled when you first run it. The benefits of precompilation are partially lost, and the work done during precompilation is partially wasted.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"While some invalidations are unavoidable, in practice a good developer can often design packages to minimize the number and/or impact of invalidations. Invalidation-resistant code is often faster, with smaller binary size, than code that is vulnerable to invalidation.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"A good first step is to measure what's being invalidated, and why.","category":"page"},{"location":"tutorials/invalidations/#Learning-to-observe,-diagnose,-and-fix-invalidations","page":"Tutorial on @snoop_invalidations","title":"Learning to observe, diagnose, and fix invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"We'll illustrate invalidations by creating two packages, where loading the second package invalidates some code that was compiled in the first one. We'll then go over approaches for \"fixing\" invalidations (i.e., preventing them from occuring).","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tip: Tip\nSince SnoopCompile's tools are interactive, you are strongly encouraged to try these examples yourself as you read along.","category":"page"},{"location":"tutorials/invalidations/#Add-SnoopCompileCore,-SnoopCompile,-and-helper-packages-to-your-environment","page":"Tutorial on @snoop_invalidations","title":"Add SnoopCompileCore, SnoopCompile, and helper packages to your environment","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Here, we'll add these packages to your default environment. (With the exception of AbstractTrees, these \"developer tool\" packages should not be added to the Project file of any real packages unless you're extending the tool itself.) From your default environment (i.e., in package mode you should see something like (@v1.10) pkg>), do","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"using Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\", \"AbstractTrees\", \"Cthulhu\"]);","category":"page"},{"location":"tutorials/invalidations/#Create-the-demonstration-packages","page":"Tutorial on @snoop_invalidations","title":"Create the demonstration packages","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"We're going to implement a toy version of the card game blackjack, where players take cards with the aim of collecting 21 points. The higher you go the better, unless you go over 21 points, in which case you \"go bust\" (i.e., you lose). Because our real goal is to illustrate invalidations, we'll create a \"blackjack ecosystem\" that involves an interaction between two packages.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"While PkgTemplates is recommended for creating packages, here we'll just use the basic capabilities in Pkg. To create the (empty) packages, the code below executes the following steps:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"navigate to a temporary directory and create both packages\nmake the first package (Blackjack) depend on PrecompileTools (we're interested in reducing latency!)\nmake the second package (BlackjackFacecards) depend on the first one (Blackjack)","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"oldproj = Base.active_project()   # hide\ncd(mktempdir())\nusing Pkg\nPkg.generate(\"Blackjack\");\nPkg.activate(\"Blackjack\")\nPkg.add(\"PrecompileTools\");\nPkg.generate(\"BlackjackFacecards\");\nPkg.activate(\"BlackjackFacecards\")\nPkg.develop(PackageSpec(path=joinpath(pwd(), \"Blackjack\")));","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Now it's time to create the code for Blackjack. Normally, you'd do this with an editor, but to make it reproducible here we'll use code to create these packages. The package code we'll create below defines the following:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"a score function to assign a numeric value to a card\ntallyscore which adds the total score for a hand of cards\nplaygame which uses a simple strategy to decide whether to take another card from the deck and add it to the hand","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"To reduce latency on first use, we then precompile playgame. In a real application, we'd also want a function to manage the deck of cards, but for brevity we'll omit this and do it manually.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"write(joinpath(\"Blackjack\", \"src\", \"Blackjack.jl\"), \"\"\"\n    module Blackjack\n\n    using PrecompileTools\n\n    export playgame\n\n    const deck = []   # the deck of cards that can be dealt\n\n    # Compute the score of one card\n    score(card::Int) = card\n\n    # Add up the score in a hand of cards\n    function tallyscores(cards)\n        s = 0\n        for card in cards\n            s += score(card)\n        end\n        return s\n    end\n\n    # Play the game! We use a simple strategy to decide whether to draw another card.\n    function playgame()\n        myhand = []\n        while tallyscores(myhand) <= 14 && !isempty(deck)\n            push!(myhand, pop!(deck))   # \"Hit me!\"\n        end\n        myscore = tallyscores(myhand)\n        return myscore <= 21 ? myscore : \"Busted\"\n    end\n\n    # Precompile `playgame`:\n    @setup_workload begin\n        push!(deck, 8, 10)    # initialize the deck\n        @compile_workload begin\n            playgame()\n        end\n    end\n\n    end\n    \"\"\")","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Suppose you use Blackjack and like it, but you notice it doesn't support face cards. Perhaps you're nervous about contributing to the Blackjack package (you shouldn't be!), and so you decide to start your own package that extends its functionality. You create BlackjackFacecards to add scoring of the jack, queen, king, and ace (for simplicity we'll make the ace always worth 11):","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"write(joinpath(\"BlackjackFacecards\", \"src\", \"BlackjackFacecards.jl\"), \"\"\"\n    module BlackjackFacecards\n\n    using Blackjack\n\n    # Add a new `score` method:\n    Blackjack.score(card::Char) = card ∈ ('J', 'Q', 'K') ? 10 :\n                                  card == 'A' ? 11 : error(card, \" not known\")\n\n    end\n    \"\"\")","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"warning: Warning\nBecause BlackjackFacecards \"owns\" neither Char nor score, this is piracy and should generally be avoided. Piracy is one way to cause invalidations, but it's not the only one. BlackjackFacecards could avoid committing piracy by defining a struct Facecard ... end and defining score(card::Facecard) instead of score(card::Char). However, this would not fix the invalidations–all the factors described below are unchanged.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Now we're ready!","category":"page"},{"location":"tutorials/invalidations/#Recording-invalidations","page":"Tutorial on @snoop_invalidations","title":"Recording invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Here are the steps executed by the code below","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"load SnoopCompileCore\nload Blackjack and BlackjackFacecards while recording invalidations with the @snoop_invalidations macro.\nload SnoopCompile and AbstractTrees for analysis","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"using SnoopCompileCore\ninvs = @snoop_invalidations using Blackjack, BlackjackFacecards;\nusing SnoopCompile, AbstractTrees","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tip: Tip\nIf you get errors like Package SnoopCompileCore not found in current path, a likely explanation is that you didn't add it to your default environment. In the example above, we're in the BlackjackFacecards environment so we can develop the package, but you also need access to SnoopCompile and SnoopCompileCore. Having these in your default environment lets them be found even if they aren't part of the current environment.","category":"page"},{"location":"tutorials/invalidations/#Analyzing-invalidations","page":"Tutorial on @snoop_invalidations","title":"Analyzing invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Now we're ready to see what, if anything, got invalidated:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"trees = invalidation_trees(invs)","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"This has only one \"tree\" of invalidations. trees is a Vector so we can index it:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tree = trees[1]","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Each tree stems from a single cause described in the top line. For this tree, the cause was adding the new method score(::Char) in BlackjackFacecards.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Each cause is associated with one or more victims of invalidation, a list here named mt_backedges. Let's extract the final (and in this case, only) victim:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"sig, victim = tree.mt_backedges[end];","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"note: Note\nmt_backedges stands for \"MethodTable backedges.\" In other cases you may see a second type of invalidation, just called backedges. With these, there is no sig, and so you'll use just victim = tree.backedges[i].","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"First let's look at the the problematic method signature:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"sig","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"This is a type-tuple, i.e., Tuple{typeof(f), typesof(args)...}. We see that score was called on an object of (inferred) type Any. Calling a function with unknown argument types makes code vulnerable to invalidation, and insertion of the new score method \"exploited\" this vulnerability.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"victim shows which compiled code got invalidated:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"victim","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"But this is not the full extent of what got invalidated:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"print_tree(victim)","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Invalidations propagate throughout entire call trees, here up to playgame(): anything that calls code that may no longer be correct is itself at risk for being incorrect. In general, victims with lots of \"children\" deserve the greatest attention.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"While print_tree can be useful, Cthulhu's ascend is a far more powerful tool for gaining deeper insight:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"julia> using Cthulhu\n\njulia> ascend(victim)\nChoose a call for analysis (q to quit):\n >   tallyscores(::Vector{Any})\n       playgame()","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"This is an interactive REPL-menu, described more completely (via text and video) at ascend.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"There are quite a few other tools for working with invs and trees, see the Reference. If your list of invalidations is dauntingly large, you may be interested in precompile_blockers.","category":"page"},{"location":"tutorials/invalidations/#Why-the-invalidations-occur","page":"Tutorial on @snoop_invalidations","title":"Why the invalidations occur","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tallyscores and playgame were compiled in Blackjack, a \"world\" where the score method defined in BlackjackFacecards does not yet exist. When you load the BlackjackFacecards package, Julia must ask itself: now that this new score method exists, am I certain that I would compile tallyscores the same way? If the answer is \"no,\" Julia invalidates the old compiled code, and compiles a fresh version with full awareness of the new score method in BlackjackFacecards.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Why would the compilation of tallyscores change? Evidently, cards is a Vector{Any}, and this means that tallyscores can't guess what kind of object card might be, and thus it can't guess what kind of objects are passed into score. The crux of the invalidation is thus:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"when Blackjack is compiled, inference does not know which score method will be called. However, at the time of compilation the only score method is for Int. Thus Julia will reason that anything that isn't an Int is going to trigger an error anyway, and so you might as well optimize tallyscore expecting all cards to be Ints. (More information about how tallyscores gets optimized can be found in World-splitting.)\nhowever, when BlackjackFacecards is loaded, suddenly there are two score methods supporting both Int and Char. Now Julia's guess that all cards will probably be Ints doesn't seem so likely to be true, and thus tallyscores should be recompiled.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Thus, invalidations arise from optimization based on what methods and types are \"in the world\" at the time of compilation (sometimes called world-splitting). This form of optimization can have performance benefits, but it also leaves your code vulnerable to invalidation.","category":"page"},{"location":"tutorials/invalidations/#Fixing-invalidations","page":"Tutorial on @snoop_invalidations","title":"Fixing invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"In broad strokes, there are three ways to prevent invalidation.","category":"page"},{"location":"tutorials/invalidations/#Method-1:-defer-compilation-until-the-full-world-is-known","page":"Tutorial on @snoop_invalidations","title":"Method 1: defer compilation until the full world is known","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"The first and simplest technique is to ensure that the full range of possibilties (the entire \"world of code\") is present before any compilation occurs. In this case, probably the best approach would be to merge the BlackjackFacecards package into Blackjack itself. Or, if you are a maintainer of the \"Blackjack ecosystem\" and have reasons for thinking that keeping the packages separate makes sense, you could alternatively move the PrecompileTools workload to BlackjackFacecards. Either approach should prevent the invalidations from occuring.","category":"page"},{"location":"tutorials/invalidations/#Method-2:-improve-inferability","page":"Tutorial on @snoop_invalidations","title":"Method 2: improve inferability","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"The second way to prevent invalidations is to improve the inferability of the victim(s). If Int and Char really are the only possible kinds of cards, then in playgame it would be better to declare","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"myhand = Union{Int,Char}[]","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"and similarly for deck itself. That untyped [] is what makes myhand (and thus cards, when passed to tallyscore) a Vector{Any}, and the possibilities for card are endless. By constraining the possible types, we allow inference to know more clearly what methods might be called. More tips on fixing invalidations through improving inference can be found in Techniques for fixing inference problems.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"In this particular case, just annotating Union{Int,Char}[] isn't sufficient on its own, because the score method for Char doesn't yet exist, so Julia doesn't know what to call. However, in most real-world cases this change alone would be sufficient: usually all the needed methods exist, it's just a question of reassuring Julia that no other options are even possible.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"note: Note\nThis fix leverages union-splitting, which is conceptually related to \"world-splitting.\" However, union-splitting is far more effective at fixing inference problems, as it guarantees that no other possibilities will ever exist, no matter how many other methods get defined.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tip: Tip\nMany vulnerabilities can be fixed by improving inference. In complex code, it's easy to unwittingly write things in ways that defeat Julia's type inference. Tools that help you discover inference problems, like SnoopCompile and JET, help you discover these unwitting \"mistakes.\"","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"While in real life it's usually a bad idea to \"blame the victim,\" it's typically the right attitude for fixing invalidations. Keep in mind, though, that the source of the problem may not be the immediate victim: in this case, it was a poor container choice in playgame that put tallyscore in the bad position of having to operate on a Vector{Any}.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Improving inferability is probably the most broadly-applicable technique, and when applicable it usually gives the best outcomes: not only is your code more resistant to invalidation, but it's likely faster and compiles to smaller binaries. However, of the three approaches it is also the one that requires the deepest understanding of Julia's type system, and thus may be difficult for some coders to use.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"There are cases where there is no good way to make the code inferable, in which case other strategies are needed.","category":"page"},{"location":"tutorials/invalidations/#Method-3:-disable-Julia's-speculative-optimization","page":"Tutorial on @snoop_invalidations","title":"Method 3: disable Julia's speculative optimization","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"The third option is to prevent Julia's speculative optimization: one could replace score(card) with invokelatest(score, card):","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"function tallyscores(cards)\n    s = 0\n    for card in cards\n        s += invokelatest(score, card)\n    end\n    return s\nend","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"This forces Julia to always look up the appropriate method of score while the code is running, and thus prevents the speculative optimizations that leave the code vulnerable to invalidation. However, the cost is that your code may run somewhat more slowly, particularly here where the call is inside a loop.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"If you plan to define at least two score methods, another way to turn off this optimization would be to declare","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Base.Experimental.@max_methods 1 function score end","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"before defining any score methods. You can read the documentation on @max_methods to learn more about how it works.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tip: Tip\nMost of us learn best by doing. Try at least one of these methods of fixing the invalidation, and use SnoopCompile to verify that it works.","category":"page"},{"location":"tutorials/invalidations/#Undoing-the-damage-from-invalidations","page":"Tutorial on @snoop_invalidations","title":"Undoing the damage from invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"If you can't prevent the invalidation, an alternative approach is to recompile the invalidated code. For example, one could repeat the precompile workload from Blackjack in BlackjackFacecards. While this will mean that the whole \"stack\" will be compiled twice and cached twice (which is wasteful), it should be effective in reducing latency for users.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"PrecompileTools also has a @recompile_invalidations. This isn't generally recommended for use in package (you can end up with long compile times for things you don't need), but it can be useful in personal \"Startup packages\" where you want to reduce latency for a particular project you're working on. See the PrecompileTools documentation for details.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Pkg.activate(oldproj)   # hide","category":"page"},{"location":"tutorials/snoop_inference_parcel/#precompilation","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"","category":"section"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"In a few cases, it may be inconvenient or impossible to precompile using a workload. Some examples might be:","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"an application that opens graphical windows\nan application that connects to a database\nan application that creates, deletes, or rewrites files on disk","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"In such cases, one alternative is to create a manual list of precompile directives using Julia's precompile(f, argtypes) function.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"warning: Warning\nManual precompile directives are much more likely to \"go stale\" as the package is developed–-precompile does not throw an error if a method for the given argtypes cannot be found. They are also more likely to be dependent on the Julia version, operating system, or CPU architecture. Whenever possible, it's safer to use a workload.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"precompile directives have to be emitted by the module that owns the method and/or types. SnoopCompile comes with a tool, parcel, that splits out the \"root-most\" precompilable MethodInstances into their constituent modules. This will typically correspond to the bottom row of boxes in the flame graph. In cases where you have some non-precompilable MethodInstances, they will include MethodInstances from higher up in the call tree.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"Let's use SnoopCompile.parcel on OptimizeMeFixed:","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"julia> ttot, pcs = SnoopCompile.parcel(tinf);\n\njulia> ttot\n0.6084431670000001\n\njulia> pcs\n4-element Vector{Pair{Module, Tuple{Float64, Vector{Tuple{Float64, Core.MethodInstance}}}}}:\n                 Core => (0.000135179, [(0.000135179, MethodInstance for (NamedTuple{(:sizehint,), T} where T<:Tuple)(::Tuple{Int64}))])\n                 Base => (0.028383533000000002, [(3.2456e-5, MethodInstance for getproperty(::IOBuffer, ::Symbol)), (4.7474e-5, MethodInstance for ==(::Type, ::Nothing)), (5.7944e-5, MethodInstance for typeinfo_eltype(::Type)), (0.00039092299999999994, MethodInstance for show(::IOContext{IOBuffer}, ::Any)), (0.000433143, MethodInstance for IOContext(::IOBuffer, ::IOContext{Base.TTY})), (0.000484984, MethodInstance for Pair{Symbol, DataType}(::Any, ::Any)), (0.000742383, MethodInstance for print(::IOContext{Base.TTY}, ::String, ::String, ::Vararg{String, N} where N)), (0.001293705, MethodInstance for Pair(::Symbol, ::Type)), (0.0018914350000000003, MethodInstance for show(::IOContext{IOBuffer}, ::UInt16)), (0.010604793000000001, MethodInstance for show(::IOContext{IOBuffer}, ::Tuple{String, Int64})), (0.012404293, MethodInstance for show(::IOContext{IOBuffer}, ::Vector{Int64}))])\n             Base.Ryu => (0.15733664599999997, [(0.05721630600000001, MethodInstance for writeshortest(::Vector{UInt8}, ::Int64, ::Float32, ::Bool, ::Bool, ::Bool, ::Int64, ::UInt8, ::Bool, ::UInt8, ::Bool, ::Bool)), (0.10012033999999997, MethodInstance for show(::IOContext{IOBuffer}, ::Float32))])\n Main.OptimizeMeFixed => (0.4204474180000001, [(0.4204474180000001, MethodInstance for main())])","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"This tells us that a total of ~0.6s were spent on inference. parcel discovered precompilable MethodInstances for four modules, Core, Base, Base.Ryu, and OptimizeMeFixed. These are listed in increasing order of inference time.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"Let's look specifically at OptimizeMeFixed, since that's under our control:","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"julia> pcmod = pcs[end]\nMain.OptimizeMeFixed => (0.4204474180000001, Tuple{Float64, Core.MethodInstance}[(0.4204474180000001, MethodInstance for main())])\n\njulia> tmod, tpcs = pcmod.second;\n\njulia> tmod\n0.4204474180000001\n\njulia> tpcs\n1-element Vector{Tuple{Float64, Core.MethodInstance}}:\n (0.4204474180000001, MethodInstance for main())","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"0.42s of that time is due to OptimizeMeFixed, and parcel discovered a single MethodInstances to precompile, main().","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"We could look at the other modules (packages) similarly.","category":"page"},{"location":"tutorials/snoop_inference_parcel/#SnoopCompile.write","page":"Using @snoop_inference to emit manual precompile directives","title":"SnoopCompile.write","text":"","category":"section"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"You can generate files that contain ready-to-use precompile directives using SnoopCompile.write:","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"julia> SnoopCompile.write(\"/tmp/precompiles_OptimizeMe\", pcs)\nCore: no precompile statements out of 0.000135179\nBase: precompiled 0.026194226 out of 0.028383533000000002\nBase.Ryu: precompiled 0.15733664599999997 out of 0.15733664599999997\nMain.OptimizeMeFixed: precompiled 0.4204474180000001 out of 0.4204474180000001","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"You'll now find a directory /tmp/precompiles_OptimizeMe, and inside you'll find three files, for Base, Base.Ryu, and OptimizeMeFixed, respectively. The contents of the last of these should be recognizable:","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"function _precompile_()\n    ccall(:jl_generating_output, Cint, ()) == 1 || return nothing\n    Base.precompile(Tuple{typeof(main)})   # time: 0.4204474\nend","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"The first ccall line ensures we only pay the cost of running these precompile directives if we're building the package; this is relevant mostly if you're running Julia with --compiled-modules=no, which can be a convenient way to disable precompilation and examine packages in their \"native state.\" (It would also matter if you've set __precompile__(false) at the top of your module, but if so why are you reading this?)","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"This file is ready to be moved into the OptimizeMe repository and included into your module definition. Since we added warmup manually, you could consider moving precompile(warmup, ()) into this function.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"You might also consider submitting some of the other files (or their precompile directives) to the packages you depend on. In some cases, the specific argument type combinations may be too \"niche\" to be worth specializing; one such case is found here, a show method for Tuple{String, Int64} for Base. But in other cases, these may be very worthy additions to the package.","category":"page"},{"location":"tutorials/snoop_inference_parcel/#Final-results","page":"Using @snoop_inference to emit manual precompile directives","title":"Final results","text":"","category":"section"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"Let's check out the final results of adding these precompile directives to OptimizeMeFixed. First, let's build both modules as precompiled packages:","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"ulia> push!(LOAD_PATH, \".\")\n4-element Vector{String}:\n \"@\"\n \"@v#.#\"\n \"@stdlib\"\n \".\"\n\njulia> using OptimizeMe\n[ Info: Precompiling OptimizeMe [top-level]\n\njulia> using OptimizeMeFixed\n[ Info: Precompiling OptimizeMeFixed [top-level]","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"Now in fresh sessions,","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"julia> @time (using OptimizeMe; OptimizeMe.main())\n3.14 is great\n2.718 is jealous\n⋮\nObject x: 7\n  3.159908 seconds (10.63 M allocations: 582.091 MiB, 5.19% gc time, 99.67% compilation time)","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"versus","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"julia> @time (using OptimizeMeFixed; OptimizeMeFixed.main())\n3.14 is great\n2.718 is jealous\n⋮\n Object x: 7\n  1.840034 seconds (5.38 M allocations: 289.402 MiB, 5.03% gc time, 96.70% compilation time)","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"We've cut down on the latency by nearly a factor of two. Moreover, if Julia someday caches generated code, we're well-prepared to capitalize on the benefits, because the same improvements in \"code ownership\" are almost certain to pay dividends there too.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"If you inspect the results, you may sometimes suffer a few disappointments: some methods that we expected to precompile don't \"take.\" At the moment there appears to be a small subset of methods that fail to precompile, and the reasons are not yet widely understood. At present, the best advice seems to be to comment-out any precompile directives that don't \"take,\" since otherwise they increase the build time for the package without material benefit. These failures may be addressed in future versions of Julia. It's also worth appreciating how much we have succeeded in reducing latency, with the awareness that we may be able to get even greater benefit in the future.","category":"page"},{"location":"tutorials/snoop_inference_parcel/#Summary","page":"Using @snoop_inference to emit manual precompile directives","title":"Summary","text":"","category":"section"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"@snoop_inference collects enough data to learn which methods are triggering inference, how heavily methods are being specialized, and so on. Examining your code from the standpoint of inference and specialization may be unfamiliar at first, but like other aspects of package development (testing, documentation, and release compatibility management) it can lead to significant improvements in the quality-of-life for you and your users.","category":"page"},{"location":"#SnoopCompile.jl","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"Julia is fast, but its execution speed depends on optimizing code through compilation. Code must be compiled before you can use it, and unfortunately compilation is slow. This can cause latency the first time you use code: this latency is often called time-to-first-plot (TTFP) or more generally time-to-first-execution (TTFX). If something feels slow the first time you use it, and fast thereafter, you're probably experiencing the latency of compilation. Note that TTFX is distinct from time-to-load (TTL, which refers to the time you spend waiting for using MyPkg to finish), even though both contribute to latency.","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"Modern versions of Julia can store compiled code to disk (precompilation) to reduce or eliminate latency. Users and developers who are interested in reducing TTFX should first head to PrecompileTools, read its documentation thoroughly, and try using it to solve latency problems.","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"This package, SnoopCompile, should be considered when:","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"precompilation doesn't reduce TTFX as much as you wish\nprecompilation \"works,\" but only in isolation: as soon as you load (certain) additional packages, TTFX is bad again\nyou're wondering if you can reduce the amount of time needed to precompile your package and/or the size of the precompilation cache files","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"In other words, SnoopCompile is a diagonostic package that helps reveal the causes of latency. Historically, it proceeded PrecompileTools, and indeed PrecompileTools was split out from SnoopCompile. Today, SnoopCompile is generally needed only when PrecompileTools fails to deliver the desired benefits.","category":"page"},{"location":"#SnoopCompile-analysis-modes","page":"SnoopCompile.jl","title":"SnoopCompile analysis modes","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"SnoopCompile \"snoops\" on the Julia compiler, collecting information that may be useful to developers. Here are some of the things you can do with SnoopCompile:","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"diagnose invalidations, cases where Julia must throw away previously-compiled code (see Tutorial on @snoop_invalidations)\ntrace inference, to learn what code is being newly (or freshly) analyzed in an early stage of the compilation pipeline (Tutorial on @snoop_inference)\ntrace code generation by LLVM, a late stage in the compilation pipeline (Tutorial on @snoop_llvm)\nreveal methods with excessive numbers of compiler-generated specializations, a.k.a.profile-guided despecialization (Tutorial on PGDS)\nintegrate with tools like JET to further reduce the risk that your lovingly-precompiled code will be invalidated by loading other packages (Tutorial on JET integration)","category":"page"},{"location":"#Background-information","page":"SnoopCompile.jl","title":"Background information","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"If nothing else, you should know this:","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"invalidations occur when you load code (e.g., using MyPkg) or otherwise define new methods\ninference and other stages of compilation occur the first time you run code for a particular combination of input types","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"The individual tutorials briefly explain core concepts. More detail can be found in Understanding SnoopCompile and Julia's compilation pipeline.","category":"page"},{"location":"#Who-should-use-this-package","page":"SnoopCompile.jl","title":"Who should use this package","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"SnoopCompile is intended primarily for package developers who want to improve the experience for their users. It is also recommended for users who are willing to \"dig deep\" and understand why packages they depend on have high latency. Your experience with latency may be personal, as it can depend on the specific combination of packages you load. If latency troubles you, don't make the assumption that it must be unfixable: you might be the first person affected by that specific cause of latency.","category":"page"},{"location":"tutorials/snoop_llvm/#Tutorial-on-@snoop_llvm","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"","category":"section"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"Julia uses the LLVM compiler to generate machine code. Typically, the two main contributors to the overall compile time are inference and LLVM, and thus together @snoop_inference and @snoop_llvm collect fairly comprehensive data on the compiler.","category":"page"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"@snoop_llvm has a somewhat different design than @snoop_inference: while @snoop_inference runs in the same session that you'll be using for analysis (and thus requires that you remember to do the data gathering in a fresh session), @snoop_llvm spawns a fresh process to collect the data. The downside is that you get less interactivity, as the data have to be written out in intermediate forms as a text file.","category":"page"},{"location":"tutorials/snoop_llvm/#Add-SnoopCompileCore-and-SnoopCompile-to-your-environment","page":"Tutorial on @snoop_llvm","title":"Add SnoopCompileCore and SnoopCompile to your environment","text":"","category":"section"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"Here, we'll add these packages to your default environment.","category":"page"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"using Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\"]);","category":"page"},{"location":"tutorials/snoop_llvm/#Collecting-the-data","page":"Tutorial on @snoop_llvm","title":"Collecting the data","text":"","category":"section"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"Here's a simple demonstration of usage:","category":"page"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"using SnoopCompileCore\n@snoop_llvm \"func_names.csv\" \"llvm_timings.yaml\" begin\n    using InteractiveUtils\n    @eval InteractiveUtils.peakflops()\nend\n\nusing SnoopCompile\ntimes, info = SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\", tmin_secs = 0.025);","category":"page"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"This will write two files, \"func_names.csv\" and \"llvm_timings.yaml\", in your current working directory. Let's look at what was read from these files:","category":"page"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"times\ninfo","category":"page"}]
}
