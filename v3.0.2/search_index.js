var documenterSearchIndex = {"docs":
[{"location":"tutorials/jet/#Tutorial-on-JET-integration","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"","category":"section"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"JET is a powerful tool for analyzing your code. As described elsewhere, some of its functionality overlaps SnoopCompile, but its mechanism of action is very different. The combination JET and SnoopCompile provides capabilities that neither package has on its own. Specifically, one can use SnoopCompile to collect data on the full callgraph and JET to perform the exhaustive analysis of individual nodes.","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"The integration between the two packages is bundled into SnoopCompile, specifically report_callee, report_callees, and report_caller. These take InferenceTrigger (see the page on inference failures) and use them to generate JET reports. These tools focus on error-analysis rather than optimization, as SnoopCompile can already identify runtime dispatch.","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"We can demonstrate both the need and use of these tools with a simple extended example.","category":"page"},{"location":"tutorials/jet/#JET-usage","page":"Tutorial on JET integration","title":"JET usage","text":"","category":"section"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"As a basic introduction to JET, let's analyze the following call from JET's own documentation:","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"julia> using JET\n\njulia> list = Any[1,2,3];\n\njulia> sum(list)\n6\n\njulia> @report_call sum(list)\n═════ 1 possible error found ═════\n┌ sum(a::Vector{Any}) @ Base ./reducedim.jl:1010\n│┌ sum(a::Vector{Any}; dims::Colon, kw::@Kwargs{}) @ Base ./reducedim.jl:1010\n││┌ _sum(a::Vector{Any}, ::Colon) @ Base ./reducedim.jl:1014\n│││┌ _sum(a::Vector{Any}, ::Colon; kw::@Kwargs{}) @ Base ./reducedim.jl:1014\n││││┌ _sum(f::typeof(identity), a::Vector{Any}, ::Colon) @ Base ./reducedim.jl:1015\n│││││┌ _sum(f::typeof(identity), a::Vector{Any}, ::Colon; kw::@Kwargs{}) @ Base ./reducedim.jl:1015\n││││││┌ mapreduce(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{Any}) @ Base ./reducedim.jl:357\n│││││││┌ mapreduce(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{Any}; dims::Colon, init::Base._InitialValue) @ Base ./reducedim.jl:357\n││││││││┌ _mapreduce_dim(f::typeof(identity), op::typeof(Base.add_sum), ::Base._InitialValue, A::Vector{Any}, ::Colon) @ Base ./reducedim.jl:365\n│││││││││┌ _mapreduce(f::typeof(identity), op::typeof(Base.add_sum), ::IndexLinear, A::Vector{Any}) @ Base ./reduce.jl:432\n││││││││││┌ mapreduce_empty_iter(f::typeof(identity), op::typeof(Base.add_sum), itr::Vector{Any}, ItrEltype::Base.HasEltype) @ Base ./reduce.jl:380\n│││││││││││┌ reduce_empty_iter(op::Base.MappingRF{typeof(identity), typeof(Base.add_sum)}, itr::Vector{Any}, ::Base.HasEltype) @ Base ./reduce.jl:384\n││││││││││││┌ reduce_empty(op::Base.MappingRF{typeof(identity), typeof(Base.add_sum)}, ::Type{Any}) @ Base ./reduce.jl:361\n│││││││││││││┌ mapreduce_empty(::typeof(identity), op::typeof(Base.add_sum), T::Type{Any}) @ Base ./reduce.jl:372\n││││││││││││││┌ reduce_empty(::typeof(Base.add_sum), ::Type{Any}) @ Base ./reduce.jl:352\n│││││││││││││││┌ reduce_empty(::typeof(+), ::Type{Any}) @ Base ./reduce.jl:343\n││││││││││││││││┌ zero(::Type{Any}) @ Base ./missing.jl:106\n│││││││││││││││││ MethodError: no method matching zero(::Type{Any}): Base.throw(Base.MethodError(zero, tuple(Base.Any)::Tuple{DataType})::MethodError)\n││││││││││││││││└────────────────────","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"The final line reveals that while sum happened to work for the specific list we provided, it nevertheless has a \"gotcha\" for the types we supplied: if list happens to be empty, sum depends on the ability to generate zero(T) for the element-type T of list, but because we constructed list to have an element-type of Any, there is no such method and sum(Any[]) throws an error:","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"julia> sum(Int[])\n0\n\njulia> sum(Any[])\nERROR: MethodError: no method matching zero(::Type{Any})\n[...]","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"(This can be circumvented with sum(Any[]; init=0).)","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"This is the kind of bug that can lurk undetected for a long time, and JET excels at exposing them.","category":"page"},{"location":"tutorials/jet/#JET-limitations","page":"Tutorial on JET integration","title":"JET limitations","text":"","category":"section"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"JET is a static analyzer, meaning that it works from the argument types provided, and that has an important consequence: if a particular callee can't be inferred, JET can't analyze it. We can illustrate that quite easily:","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"julia> callsum(listcontainer) = sum(listcontainer[1])\ncallsum (generic function with 1 method)\n\njulia> lc = Any[list];   # \"hide\" `list` inside a Vector{Any}\n\njulia> callsum(lc)\n6\n\njulia> @report_call callsum(lc)\nNo errors detected","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"Because we \"hid\" the type of list from inference, JET couldn't tell what specific instance of sum was going to be called, so it was unable to detect any errors.","category":"page"},{"location":"tutorials/jet/#JET/SnoopCompile-integration","page":"Tutorial on JET integration","title":"JET/SnoopCompile integration","text":"","category":"section"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"A resolution to this problem is to use SnoopCompile to do the \"data collection\" and JET to do the analysis. The key reason is that SnoopCompile is a dynamic analyzer, and is capable of bridging across runtime dispatch. As always, you need to do the data collection in a fresh session where the calls have not previously been inferred. After restarting Julia, we can do this:","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"julia> using SnoopCompileCore\n\njulia> list = Any[1,2,3];\n\njulia> lc = Any[list];   # \"hide\" `list` inside a Vector{Any}\n\njulia> callsum(listcontainer) = sum(listcontainer[1]);\n\njulia> tinf = @snoop_inference callsum(lc);\n\njulia> using SnoopCompile, JET, Cthulhu\n\njulia> tinf.children\n2-element Vector{SnoopCompileCore.InferenceTimingNode}:\n InferenceTimingNode: 0.000869/0.000869 on callsum(::Vector{Any}) with 0 direct children\n InferenceTimingNode: 0.000196/0.006685 on sum(::Vector{Any}) with 1 direct children\n\njulia> report_callees(inference_triggers(tinf))\n1-element Vector{Pair{InferenceTrigger, JET.JETCallResult{JET.JETAnalyzer{JET.BasicPass}, Base.Pairs{Symbol, Union{}, Tuple{}, @NamedTuple{}}}}}:\n Inference triggered to call sum(::Vector{Any}) from callsum (./REPL[5]:1) with specialization callsum(::Vector{Any}) => ═════ 1 possible error found ═════\n┌ sum(a::Vector{Any}) @ Base ./reducedim.jl:1010\n│┌ sum(a::Vector{Any}; dims::Colon, kw::@Kwargs{}) @ Base ./reducedim.jl:1010\n││┌ _sum(a::Vector{Any}, ::Colon) @ Base ./reducedim.jl:1014\n│││┌ _sum(a::Vector{Any}, ::Colon; kw::@Kwargs{}) @ Base ./reducedim.jl:1014\n││││┌ _sum(f::typeof(identity), a::Vector{Any}, ::Colon) @ Base ./reducedim.jl:1015\n│││││┌ _sum(f::typeof(identity), a::Vector{Any}, ::Colon; kw::@Kwargs{}) @ Base ./reducedim.jl:1015\n││││││┌ mapreduce(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{Any}) @ Base ./reducedim.jl:357\n│││││││┌ mapreduce(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{Any}; dims::Colon, init::Base._InitialValue) @ Base ./reducedim.jl:357\n││││││││┌ _mapreduce_dim(f::typeof(identity), op::typeof(Base.add_sum), ::Base._InitialValue, A::Vector{Any}, ::Colon) @ Base ./reducedim.jl:365\n│││││││││┌ _mapreduce(f::typeof(identity), op::typeof(Base.add_sum), ::IndexLinear, A::Vector{Any}) @ Base ./reduce.jl:432\n││││││││││┌ mapreduce_empty_iter(f::typeof(identity), op::typeof(Base.add_sum), itr::Vector{Any}, ItrEltype::Base.HasEltype) @ Base ./reduce.jl:380\n│││││││││││┌ reduce_empty_iter(op::Base.MappingRF{typeof(identity), typeof(Base.add_sum)}, itr::Vector{Any}, ::Base.HasEltype) @ Base ./reduce.jl:384\n││││││││││││┌ reduce_empty(op::Base.MappingRF{typeof(identity), typeof(Base.add_sum)}, ::Type{Any}) @ Base ./reduce.jl:361\n│││││││││││││┌ mapreduce_empty(::typeof(identity), op::typeof(Base.add_sum), T::Type{Any}) @ Base ./reduce.jl:372\n││││││││││││││┌ reduce_empty(::typeof(Base.add_sum), ::Type{Any}) @ Base ./reduce.jl:352\n│││││││││││││││┌ reduce_empty(::typeof(+), ::Type{Any}) @ Base ./reduce.jl:343\n││││││││││││││││┌ zero(::Type{Any}) @ Base ./missing.jl:106\n│││││││││││││││││ MethodError: no method matching zero(::Type{Any}): Base.throw(Base.MethodError(zero, tuple(Base.Any)::Tuple{DataType})::MethodError)\n││││││││││││││││└────────────────────","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"Because SnoopCompileCore collected the runtime-dispatched sum call, we can pass it to JET. report_callees filters those calls which generate JET reports, allowing you to focus on potential errors.","category":"page"},{"location":"tutorials/jet/","page":"Tutorial on JET integration","title":"Tutorial on JET integration","text":"note: Note\nJET integration is enabled only if JET.jl and Cthulhu.jl have been loaded into your main session. This is why there's the using JET, Cthulhu statement included in the example given.","category":"page"},{"location":"tutorials/snoop_inference/#Tutorial-on-@snoop_inference","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Inference may occur when you run code. Inference is the first step of type-specialized compilation. @snoop_inference collects data on what inference is doing, giving you greater insight into what is being inferred and how long it takes.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Compilation is needed only for \"fresh\" code; running the demos below on code you've already used will yield misleading results. When analyzing inference, you're advised to always start from a fresh session. See also the comparison between SnoopCompile and JET.","category":"page"},{"location":"tutorials/snoop_inference/#Add-SnoopCompileCore,-SnoopCompile,-and-helper-packages-to-your-environment","page":"Tutorial on @snoop_inference","title":"Add SnoopCompileCore, SnoopCompile, and helper packages to your environment","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Here, we'll add these packages to your default environment. (With the exception of AbstractTrees, these \"developer tool\" packages should not be added to the Project file of any real packages unless you're extending the tool itself.)","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"using Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\", \"AbstractTrees\", \"ProfileView\"]);","category":"page"},{"location":"tutorials/snoop_inference/#Setting-up-the-demo","page":"Tutorial on @snoop_inference","title":"Setting up the demo","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"To see @snoop_inference in action, we'll use the following demo:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"module FlattenDemo\n    struct MyType{T} x::T end\n\n    extract(y::MyType) = y.x\n\n    function domath(x)\n        y = x + x\n        return y*x + 2*x + 5\n    end\n\n    dostuff(y) = domath(extract(y))\n\n    function packintype(x)\n        y = MyType{Int}(x)\n        return dostuff(y)\n    end\nend\n\n# output\n\nFlattenDemo","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"The main call, packintype, stores the input in a struct, and then calls functions that extract the field value and performs arithmetic on the result.","category":"page"},{"location":"tutorials/snoop_inference/#sccshow","page":"Tutorial on @snoop_inference","title":"Collecting the data","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"To profile inference on this call, do the following:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> using SnoopCompileCore\n\njulia> tinf = @snoop_inference FlattenDemo.packintype(1);\n\njulia> using SnoopCompile\n\njulia> tinf\nInferenceTimingNode: 0.002712/0.003278 on Core.Compiler.Timings.ROOT() with 1 direct children","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"tip: Tip\nDon't omit the semicolon on the tinf = @snoop_inference ... line, or you may get an enormous amount of output. The compact display on the final line is possible only because SnoopCompile defines nice Base.show methods for the data returned by @snoop_inference. These methods cannot be defined in SnoopCompileCore because it has a fundamental design constraint: loading SnoopCompileCore is not allowed to invalidate any code. Moving those Base.show methods to SnoopCompileCore would violate that guarantee.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"This may not look like much, but there's a wealth of information hidden inside tinf.","category":"page"},{"location":"tutorials/snoop_inference/#A-quick-check-for-potential-invalidations","page":"Tutorial on @snoop_inference","title":"A quick check for potential invalidations","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"After running @snoop_inference, it's generally recommended to check the output of staleinstances:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> staleinstances(tinf)\nSnoopCompileCore.InferenceTiming[]","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"If you see this, all's well. A non-empty list might indicate method invalidations, which can be checked (in a fresh session) using the tools described in Tutorial on @snoop_invalidations.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"If you do have a lot of invalidations, precompile_blockers may be an effective way to reveal those invalidations that affect your particular package and workload.","category":"page"},{"location":"tutorials/snoop_inference/#flamegraph","page":"Tutorial on @snoop_inference","title":"Viewing the results","text":"","category":"section"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Let's start unpacking the output of @snoop_inference and see how to get more insight. First, notice that the output is an InferenceTimingNode: it's the root element of a tree of such nodes, all connected by caller-callee relationships. Indeed, this particular node is for Core.Compiler.Timings.ROOT(), a \"dummy\" node that is the root of all such trees.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"You may have noticed that this ROOT node prints with two numbers. It will be easier to understand their meaning if we first display the whole tree. We can do that with the AbstractTrees package:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> using AbstractTrees\n\njulia> print_tree(tinf, maxdepth=100)\nInferenceTimingNode: 0.002712/0.003278 on Core.Compiler.Timings.ROOT() with 1 direct children\n└─ InferenceTimingNode: 0.000133/0.000566 on FlattenDemo.packintype(::Int64) with 2 direct children\n   ├─ InferenceTimingNode: 0.000094/0.000094 on FlattenDemo.MyType{Int64}(::Int64) with 0 direct children\n   └─ InferenceTimingNode: 0.000089/0.000339 on FlattenDemo.dostuff(::FlattenDemo.MyType{Int64}) with 2 direct children\n      ├─ InferenceTimingNode: 0.000064/0.000122 on FlattenDemo.extract(::FlattenDemo.MyType{Int64}) with 2 direct children\n      │  ├─ InferenceTimingNode: 0.000034/0.000034 on getproperty(::FlattenDemo.MyType{Int64}, ::Symbol) with 0 direct children\n      │  └─ InferenceTimingNode: 0.000024/0.000024 on getproperty(::FlattenDemo.MyType{Int64}, x::Symbol) with 0 direct children\n      └─ InferenceTimingNode: 0.000127/0.000127 on FlattenDemo.domath(::Int64) with 0 direct children","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"This tree structure reveals the caller-callee relationships, showing the specific types that were used for each MethodInstance. Indeed, as the calls to getproperty reveal, it goes beyond the types and even shows the results of constant propagation; the getproperty(::MyType{Int64}, x::Symbol) corresponds to y.x in the definition of extract.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"note: Note\nGenerally we speak of call graphs rather than call trees. But because inference results are cached (a.k.a., we only \"visit\" each node once), we obtain a tree as a depth-first-search of the full call graph.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"You can extract the MethodInstance with","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> Core.MethodInstance(tinf)\nMethodInstance for Core.Compiler.Timings.ROOT()\n\njulia> Core.MethodInstance(tinf.children[1])\nMethodInstance for FlattenDemo.packintype(::Int64)","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Each node in this tree is accompanied by a pair of numbers. The first number is the exclusive inference time (in seconds), meaning the time spent inferring the particular MethodInstance, not including the time spent inferring its callees. The second number is the inclusive time, which is the exclusive time plus the time spent on the callees. Therefore, the inclusive time is always at least as large as the exclusive time.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"The ROOT node is a bit different: its exclusive time measures the time spent on all operations except inference. In this case, we see that the entire call took approximately 3.3ms, of which 2.7ms was spent on activities besides inference. Almost all of that was code-generation, but it also includes the time needed to run the code. Just 0.55ms was needed to run type-inference on this entire series of calls. As you will quickly discover, inference takes much more time on more complicated code.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"We can also display this tree as a flame graph, using the ProfileView.jl package:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> fg = flamegraph(tinf)\nNode(FlameGraphs.NodeData(ROOT() at typeinfer.jl:75, 0x00, 0:10080857))","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"julia> using ProfileView\n\njulia> ProfileView.view(fg)","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"You should see something like this:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"(Image: flamegraph)","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Users are encouraged to read the ProfileView documentation to understand how to interpret this, but briefly:","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"the horizontal axis is time (wide boxes take longer than narrow ones), the vertical axis is call depth\nhovering over a box displays the method that was inferred\nleft-clicking on a box causes the full MethodInstance to be printed in your REPL session\nright-clicking on a box opens the corresponding method in your editor\nctrl-click can be used to zoom in\nempty horizontal spaces correspond to activities other than type-inference\nany boxes colored red (there are none in this particular example, but you'll see some later) correspond to naively non-precompilable MethodInstances, in which the method is owned by one module but the types are from another unrelated module. Such MethodInstances are omitted from the precompile cache file unless they've been \"marked\" by PrecompileTools.@compile_workload or an explicit precompile directive.\nany boxes colored orange-yellow (there is one in this demo) correspond to methods inferred for specific constants (constant propagation).","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"You can explore this flamegraph and compare it to the output from print_tree.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"note: Note\nOrange-yellow boxes that appear at the base of a flame are worth special attention, and may represent something that you thought you had precompiled. For example, suppose your workload \"exercises\" myfun(args...; warn=true), so you might think you have myfun covered for the corresponding argument types. But constant-propagation (as indicated by the orange-yellow coloration) results in (re)compilation for specific values: if Julia has decided that myfun merits constant-propagation, a call myfun(args...; warn=false) might need to be compiled separately.When you want to prevent constant-propagation from hurting your TTFX, you have two options:precompile for all relevant argument values as well as types. The most common argument types to trigger Julia's constprop heuristics are numbers (Bool/Int/etc) and Symbol.\nDisable constant-propagation for this method by adding Base.@constprop :none in front of your definition of myfun. Constant-propagation can be a big performance boost when it changes how performance-sensitive code is optimized for specific input values, but when this doesn't apply you can safely disable it.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"Finally, flatten, on its own or together with accumulate_by_source, allows you to get an sense for the cost of individual MethodInstances or Methods.","category":"page"},{"location":"tutorials/snoop_inference/","page":"Tutorial on @snoop_inference","title":"Tutorial on @snoop_inference","text":"The tools here allow you to get an overview of where inference is spending its time. This gives you insight into the major contributors to latency.","category":"page"},{"location":"explanations/tools/#Package-roles-and-alternatives","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"","category":"section"},{"location":"explanations/tools/#SnoopCompileCore","page":"Package roles and alternatives","title":"SnoopCompileCore","text":"","category":"section"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"SnoopCompileCore is a tiny package with no dependencies; it's used for collecting data, and it has been designed in such a way that it cannot cause any invalidations of its own. Collecting data on invalidations and inference with SnoopCompileCore is the only way you can be sure you are observing the \"native state\" of your code.","category":"page"},{"location":"explanations/tools/#SnoopCompile","page":"Package roles and alternatives","title":"SnoopCompile","text":"","category":"section"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"SnoopCompile is a much larger package that performs analysis on the data collected by SnoopCompileCore; loading SnoopCompile can (and does) trigger invalidations. Consequently, you're urged to always collect data with just SnoopCompileCore loaded, and wait to load SnoopCompile until after you've finished collecting the data.","category":"page"},{"location":"explanations/tools/#Cthulhu","page":"Package roles and alternatives","title":"Cthulhu","text":"","category":"section"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"Cthulhu is a companion package that gives deep insights into the origin of invalidations or inference failures.","category":"page"},{"location":"explanations/tools/#AbstractTrees","page":"Package roles and alternatives","title":"AbstractTrees","text":"","category":"section"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"AbstractTrees is the one package in this list that can be both a \"workhorse\" and a developer tool. SnoopCompile uses it mostly for pretty-printing.","category":"page"},{"location":"explanations/tools/#JET","page":"Package roles and alternatives","title":"JET","text":"","category":"section"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"JET is a powerful developer tool that in some ways is an alternative to SnoopCompile. While the two have different goals, the packages have some overlap in what they can tell you about your code. However, their mechanisms of action are fundamentally different:","category":"page"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"JET is a \"static analyzer,\" which means that it analyzes the code itself. JET can tell you about inference failures (runtime dispatch) much like SnoopCompile, with a major advantage: SnoopCompileCore omits information about any callees that are already compiled, but JET's @report_opt provides exhaustive information about the entire inferable callgraph (i.e., the part of the callgraph that inference can predict from the initial call) regardless of whether it has been previously compiled. With JET, you don't have to remember to run each analysis in a fresh session.\nSnoopCompileCore collects data by watching normal inference at work. On code that hasn't been compiled previously, this can yield results similar to JET's, with a different major advantage: JET can't \"see through\" runtime dispatch, but SnoopCompileCore can. With SnoopCompile, you can immediately get a wholistic view of your entire callgraph.","category":"page"},{"location":"explanations/tools/","page":"Package roles and alternatives","title":"Package roles and alternatives","text":"Combining JET and SnoopCompile can provide insights that are difficult to obtain with either package in isolation. See the Tutorial on JET integration.","category":"page"},{"location":"explanations/fixing_inference/#Techniques-for-fixing-inference-problems","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Here we assume you've dug into your code with a tool like Cthulhu, and want to know how to fix some of the problems that you discover. Below is a collection of specific cases and some tricks for handling them.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Note that there is also a tutorial on fixing inference that delves into advanced topics.","category":"page"},{"location":"explanations/fixing_inference/#Adding-type-annotations","page":"Techniques for fixing inference problems","title":"Adding type annotations","text":"","category":"section"},{"location":"explanations/fixing_inference/#Using-concrete-types","page":"Techniques for fixing inference problems","title":"Using concrete types","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Defining variables like list = [] can be convenient, but it creates a list of type Vector{Any}. This prevents inference from knowing the type of items extracted from list. Using list = String[] for a container of strings, etc., is an excellent fix. When in doubt, check the type with isconcretetype: a common mistake is to think that list_of_lists = Array{Int}[] gives you a vector-of-vectors, but","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"julia> isconcretetype(Array{Int})\nfalse","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"reminds you that Array requires a second parameter indicating the dimensionality of the array. (Or use list_of_lists = Vector{Int}[] instead, as Vector{Int} === Array{Int, 1}.)","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Many valuable tips can be found among Julia's performance tips, and readers are encouraged to consult that page.","category":"page"},{"location":"explanations/fixing_inference/#Working-with-non-concrete-types","page":"Techniques for fixing inference problems","title":"Working with non-concrete types","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"In cases where invalidations occur, but you can't use concrete types (there are indeed many valid uses of Vector{Any}), you can often prevent the invalidation using some additional knowledge. One common example is extracting information from an IOContext structure, which is roughly defined as","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"struct IOContext{IO_t <: IO} <: AbstractPipe\n    io::IO_t\n    dict::ImmutableDict{Symbol, Any}\nend","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"There are good reasons that dict uses a value-type of Any, but that makes it impossible for the compiler to infer the type of any object looked up in an IOContext. Fortunately, you can help! For example, the documentation specifies that the :color setting should be a Bool, and since it appears in documentation it's something we can safely enforce. Changing","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"iscolor = get(io, :color, false)","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"to","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"iscolor = get(io, :color, false)::Bool     # assert that the rhs is Bool-valued","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"will throw an error if it isn't a Bool, and this allows the compiler to take advantage of the type being known in subsequent operations.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"If the return type is one of a small number of possibilities (generally three or fewer), you can annotate the return type with Union{...}. This is generally advantageous only when the intersection of what inference already knows about the types of a variable and the types in the Union results in an concrete type.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"As a more detailed example, suppose you're writing code that parses Julia's Expr type:","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"julia> ex = :(Array{Float32,3})\n:(Array{Float32, 3})\n\njulia> dump(ex)\nExpr\n  head: Symbol curly\n  args: Vector{Any(3,))\n    1: Symbol Array\n    2: Symbol Float32\n    3: Int64 3","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"ex.args is a Vector{Any}. However, for a :curly expression only certain types will be found among the arguments; you could write key portions of your code as","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"a = ex.args[2]\nif a isa Symbol\n    # inside this block, Julia knows `a` is a Symbol, and so methods called on `a` will be resistant to invalidation\n    foo(a)\nelseif a isa Expr && length((a::Expr).args) > 2\n    a::Expr         # sometimes you have to help inference by adding a type-assert\n    x = bar(a)      # `bar` is now resistant to invalidation\nelseif a isa Integer\n    # even though you've not made this fully-inferrable, you've at least reduced the scope for invalidations\n    # by limiting the subset of `foobar` methods that might be called\n    y = foobar(a)\nend","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Other tricks include replacing broadcasting on v::Vector{Any} with Base.mapany(f, v)–mapany avoids trying to narrow the type of f(v[i]) and just assumes it will be Any, thereby avoiding invalidations of many convert methods.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Adding type-assertions and fixing inference problems are the most common approaches for fixing invalidations. You can discover these manually, but using Cthulhu is highly recommended.","category":"page"},{"location":"explanations/fixing_inference/#Inferrable-field-access-for-abstract-types","page":"Techniques for fixing inference problems","title":"Inferrable field access for abstract types","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"When invalidations happen for methods that manipulate fields of abstract types, often there is a simple solution: create an \"interface\" for the abstract type specifying that certain fields must have certain types. Here's an example:","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"abstract type AbstractDisplay end\n\nstruct Monitor <: AbstractDisplay\n    height::Int\n    width::Int\n    maker::String\nend\n\nstruct Phone <: AbstractDisplay\n    height::Int\n    width::Int\n    maker::Symbol\nend\n\nfunction Base.show(@nospecialize(d::AbstractDisplay), x)\n    str = string(x)\n    w = d.width\n    if length(str) > w  # do we have to truncate to fit the display width?\n        ...","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"In this show method, we've deliberately chosen to prevent specialization on the specific type of AbstractDisplay (to reduce the total number of times we have to compile this method). As a consequence, Julia's inference may not realize that d.width returns an Int.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Fortunately, you can help by defining an interface for generic AbstractDisplay objects:","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"function Base.getproperty(d::AbstractDisplay, name::Symbol)\n    if name === :height\n        return getfield(d, :height)::Int\n    elseif name === :width\n        return getfield(d, :width)::Int\n    elseif name === :maker\n        return getfield(d, :maker)::Union{String,Symbol}\n    end\n    return getfield(d, name)\nend","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Julia's constant propagation will ensure that most accesses of those fields will be determined at compile-time, so this simple change robustly fixes many inference problems.","category":"page"},{"location":"explanations/fixing_inference/#Fixing-Core.Box","page":"Techniques for fixing inference problems","title":"Fixing Core.Box","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Julia issue 15276 is one of the more surprising forms of inference failure; it is the most common cause of a Core.Box annotation. If other variables depend on the Boxed variable, then a single Core.Box can lead to widespread inference problems. For this reason, these are also among the first inference problems you should tackle.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Read this explanation of why this happens and what you can do to fix it. If you are directed to find Core.Box inference triggers via suggest, you may need to explore around the call site a bit– the inference trigger may be in the closure itself, but the fix needs to go in the method that creates the closure.","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"Use of ascend is highly recommended for fixing Core.Box inference failures.","category":"page"},{"location":"explanations/fixing_inference/#Handling-edge-cases","page":"Techniques for fixing inference problems","title":"Handling edge cases","text":"","category":"section"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"You can sometimes get invalidations from failing to handle \"formal\" possibilities. For example, operations with regular expressions might return a Union{Nothing, RegexMatch}. You can sometimes get poor type inference by writing code that fails to take account of the possibility that nothing might be returned. For example, a comprehension","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"ms = [m.match for m in match.((rex,), my_strings)]","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"might be replaced with","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"ms = [m.match for m in match.((rex,), my_strings) if m !== nothing]","category":"page"},{"location":"explanations/fixing_inference/","page":"Techniques for fixing inference problems","title":"Techniques for fixing inference problems","text":"and return a better-typed result.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#inferrability","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Throughout this page, we'll use the OptimizeMe demo, which ships with SnoopCompile.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"note: Note\nTo understand what follows, it's essential to refer to OptimizeMe source code as you follow along.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"using SnoopCompileCore, SnoopCompile # here we need the SnoopCompile path for the next line (normally you should wait until after data collection is complete)\ninclude(joinpath(pkgdir(SnoopCompile), \"examples\", \"OptimizeMe.jl\"))\ntinf = @snoop_inference OptimizeMe.main();\nfg = flamegraph(tinf)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"If you visualize fg with ProfileView, you may see something like this:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"(Image: flamegraph-OptimizeMe)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"From the standpoint of precompilation, this has some obvious problems:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"even though we called a single method, OptimizeMe.main(), there are many distinct flames separated by blank spaces. This indicates that many calls are being made by runtime dispatch:  each separate flame is a fresh entrance into inference.\nseveral of the flames are marked in red, indicating that they are not naively precompilable (see the Tutorial on @snoop_inference). While @compile_workload can handle these flames, an even more robust solution is to eliminate them altogether.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Our goal will be to improve the design of OptimizeMe to make it more readily precompilable.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#Analyzing-inference-triggers","page":"Using @snoop_inference results to improve inferrability","title":"Analyzing inference triggers","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We'll first extract the \"triggers\" of inference, which is just a repackaging of part of the information contained within tinf. Specifically an InferenceTrigger captures callee/caller relationships that straddle a fresh entrance to type-inference, allowing you to identify which calls were made by runtime dispatch and what MethodInstance they called.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"itrigs = inference_triggers(tinf)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"The number of elements in this Vector{InferenceTrigger} tells you how many calls were (1) made by runtime dispatch and (2) the callee had not previously been inferred.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"tip: Tip\nIn the REPL, SnoopCompile displays InferenceTriggers with yellow coloration for the callee, red for the caller method, and blue for the caller specialization. This makes it easier to quickly identify the most important information.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"In some cases, this might indicate that you'll need to fix each case separately; fortunately, in many cases fixing one problem addresses many other.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#methtrigs","page":"Using @snoop_inference results to improve inferrability","title":"Method triggers","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Most often, it's most convenient to organize them by the method triggering the need for inference:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"mtrigs = accumulate_by_source(Method, itrigs)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"The methods triggering the largest number of inference runs are shown at the bottom. You can also select methods from a particular module:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"modtrigs = filtermod(OptimizeMe, mtrigs)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Rather than filter by a single module, you can alternatively call SnoopCompile.parcel(mtrigs) to split them out by module. In this case, most of the triggers came from Base, not OptimizeMe. However, many of the failures in Base were nevertheless indirectly due to OptimizeMe: our methods in OptimizeMe call Base methods with arguments that trigger internal inference failures. Fortunately, we'll see that using more careful design in OptimizeMe can avoid many of those problems.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"tip: Tip\nIf you have a longer list of inference triggers than you feel comfortable tackling, filtering by your package's module or using precompile_blockers can be a good way to start. Fixing issues in the package itself can end up resolving many of the \"indirect\" triggers too. Also be sure to note the ability to filter out likely \"noise\" from test suites.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"You can get an overview of each Method trigger with summary:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"mtrig = modtrigs[1]\nsummary(mtrig)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"You can also say edit(mtrig) and be taken directly to the method you're analyzing in your editor. You can still \"dig deep\" into individual triggers:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"itrig = mtrig.itrigs[1]","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"This is useful if you want to analyze with Cthulhu.ascend. Method-based triggers, which may aggregate many different individual triggers, can be useful because tools like Cthulhu.jl show you the inference results for the entire MethodInstance, allowing you to fix many different inference problems at once.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#Trigger-trees","page":"Using @snoop_inference results to improve inferrability","title":"Trigger trees","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"While method triggers are probably the most useful way of organizing these inference triggers, for learning purposes here we'll use a more detailed scheme, which organizes inference triggers in a tree:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"itree = trigger_tree(itrigs)\nusing AbstractTrees\nprint_tree(itree)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"This gives you a big-picture overview of how the inference failures arose. The parent-child relationships are based on the backtraces at the entrance to inference, and the nodes are organized in the order in which inference occurred. Inspection of these trees can be informative; for example, here we notice a lot of method specializations for Container{T} for different T.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We're going to march through these systematically.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#suggest-and-fixing-Core.Box","page":"Using @snoop_inference results to improve inferrability","title":"suggest and fixing Core.Box","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"You may have noticed above that summary(mtrig) generated a red has Core.Box message. Assuming that itrig is still the first (and it turns out, only) trigger from this method, let's look at this again, explicitly using suggest, the tool that generated this hint:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"suggest(itrig)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"You can see that SnoopCompile recommends tackling this first; depending on how much additional code is affected, fixing a  Core.Box allows inference to work better and may resolve other triggers.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"This message also directs readers to a section of this documentation that links to a page of the Julia manual describing the underlying problem. The Julia documentation suggests a couple of fixes, of which the best (in this case) is to use the let statement to rebind the variable and end any \"conflict\" with the closure:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"function abmult(r::Int, ys)\n    if r < 0\n        r = -r\n    end\n    let r = r    # Julia #15276\n        return map(x -> howbig(r * x), ys)\n    end\nend","category":"page"},{"location":"tutorials/snoop_inference_analysis/#suggest-and-a-fix-involving-manual-eltype-specification","page":"Using @snoop_inference results to improve inferrability","title":"suggest and a fix involving manual eltype specification","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Let's look at the other Method-trigger rooted in OptimizeMe:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"mtrig = modtrigs[2]\nsummary(mtrig)\nitrig = mtrig.itrigs[1]","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"If you use Cthulhu's ascend(itrig) you might see something like this:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"(Image: ascend-lotsa)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"The first thing to note here is that cs is inferred as an AbstractVector–fixing this to make it a concrete type should be our next goal. There's a second, more subtle hint: in the call menu at the bottom, the selected call is marked < semi-concrete eval >. This is a hint that a method is being called with a non-concrete type.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"What might that non-concrete type be?","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"isconcretetype(OptimizeMe.Container)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"The statement Container.(list) is thus creating an AbstractVector with a non-concrete element type. You can seem in greater detail what happens, inference-wise, in this snippet from print_tree(itree):","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"   ├─ similar(::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Tuple{Base.OneTo{Int64}}, Type{Main.OptimizeMe.Container}, Tuple{Base.Broadcast.Extruded{Vector{Any}, Tuple{Bool}, Tuple{Int64}}}}, ::Type{Main.OptimizeMe.Container{Int64}})\n   ├─ setindex!(::Vector{Main.OptimizeMe.Container{Int64}}, ::Main.OptimizeMe.Container{Int64}, ::Int64)\n   ├─ Base.Broadcast.copyto_nonleaf!(::Vector{Main.OptimizeMe.Container{Int64}}, ::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Tuple{Base.OneTo{Int64}}, Type{Main.OptimizeMe.Container}, Tuple{Base.Broadcast.Extruded{Vector{Any}, Tuple{Bool}, Tuple{Int64}}}}, ::Base.OneTo{Int64}, ::Int64, ::Int64)\n   │  ├─ similar(::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Tuple{Base.OneTo{Int64}}, Type{Main.OptimizeMe.Container}, Tuple{Base.Broadcast.Extruded{Vector{Any}, Tuple{Bool}, Tuple{Int64}}}}, ::Type{Main.OptimizeMe.Container})\n   │  └─ Base.Broadcast.restart_copyto_nonleaf!(::Vector{Main.OptimizeMe.Container}, ::Vector{Main.OptimizeMe.Container{Int64}}, ::Base.Broadcast.Broadcasted","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"In rough terms, what this means is the following:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"since the first item in list is an Int, the output initially gets created as a Vector{Container{Int}}\nhowever, copyto_nonleaf! runs into trouble when it goes to copy the second item, which is a Container{UInt8}\nhence, copyto_nonleaf! re-allocates the output array to be a generic Vector{Container} and then calls restart_copyto_nonleaf!.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We can prevent all this hassle with one simple change: rewrite that line as","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"cs = Container{Any}.(list)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We use Container{Any} here because there is no more specific element type–other than an unreasonably-large Union–that can hold all the items in list.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"If you make these edits manually, you'll see that we've gone from dozens of itrigs (38 on Julia 1.10, you may get a different number on other Julia versions) down to about a dozen (13 on Julia 1.10). Real progress!","category":"page"},{"location":"tutorials/snoop_inference_analysis/#Replacing-hard-to-infer-calls-with-lower-level-APIs","page":"Using @snoop_inference results to improve inferrability","title":"Replacing hard-to-infer calls with lower-level APIs","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"We note that many of the remaining triggers are somehow related to show, for example:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Inference triggered to call show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMe.Container{Any}}) from #55 (/cache/build/builder-amdci4-0/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:273) with specialization (::REPL.var\"#55#56\"{REPL.REPLDisplay{REPL.LineEditREPL}, MIME{Symbol(\"text/plain\")}, Base.RefValue{Any}})(::Any)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"In this case we see that the calling method is #55.  This is a gensym, or generated symbol, indicating that the method was generated during Julia's lowering pass, and might indicate a macro, a do block or other anonymous function, the generator for a @generated function, etc.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"edit(itrig) (or equivalently, edit(node) where node is a child of itree) takes us to this method in Base, for which key lines are","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"function display(d::REPLDisplay, mime::MIME\"text/plain\", x)\n    x = Ref{Any}(x)\n    with_repl_linfo(d.repl) do io\n        ⋮\n        show(io, mime, x[])\n        ⋮\nend","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"The generated method corresponds to the do block here. The call to show comes from show(io, mime, x[]). This implementation uses a clever trick, wrapping x in a Ref{Any}(x), to prevent specialization of the method defined by the do block on the specific type of x. This trick is designed to limit the number of MethodInstances inferred for this display method.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"A great option is to replace the call to display with an explicit","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"show(stdout, MIME(\"text/plain\"), cs)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"There's one extra detail: the type of stdout is not fixed (and therefore not known), because one can use a terminal, a file, devnull, etc., as stdout. If you want to prevent all runtime dispatch from this call, you'd need to supply an io::IO object of known type as the first argument. It could, for example, be passed in to lotsa_containers from main:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"function lotsa_containers(io::IO)\n    ⋮\n    println(io, \"lotsa containers:\")\n    show(io, MIME(\"text/plain\"), cs)\nend","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"However, if you want it to go to stdout–and to allow users to redirect stdout to a target of their choosing–then an io argument may have to be of unknown type when called from main.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#When-you-need-to-rely-on-@compile_workload","page":"Using @snoop_inference results to improve inferrability","title":"When you need to rely on @compile_workload","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"Most of the remaining triggers are difficult to fix because they occur in deliberately-@nospecialized portions of Julia's internal code for displaying arrays. In such cases, adding a PrecompileTools.@compile_workload is your best option. Here we use an interesting trick:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"@compile_workload begin\n    lotsa_containers(devnull)  # use `devnull` to suppress output\n    abmult(rand(-5:5), rand(3))\nend\nprecompile(lotsa_containers, (Base.TTY,))","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"During the workload, we pass devnull as the io object to lotsa_containers: this suppresses the output so you don't see anything during precompilation. However, devnull is not a Base.TTY, the standard type of stdout. Nevertheless, this is effective because we can see that many of the callees in the remaining inference-triggers do not depend on the io object.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"To really ice the cake, we also add a manual precompile directive. (precompile doesn't execute the method, it just compiles it.) This doesn't \"step through\" runtime dispatch, but at least it precompiles the entry point. Thus, at least lotsa_containers will be precompiled for the most likely IO type encountered in practice.","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"With these changes, we've fixed nearly all the latency problems in OptimizeMe, and made it much less vulnerable to invalidation as well. You can see the final code in the OptimizeMeFixed source code. Note that this would have to be turned into a real package for the @compile_workload to have any effect.","category":"page"},{"location":"tutorials/snoop_inference_analysis/#test-suites","page":"Using @snoop_inference results to improve inferrability","title":"A note on analyzing test suites","text":"","category":"section"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"If you're doing a package analysis, it's convenient to use the package's runtests.jl script as a way to cover much of the package's functionality. SnoopCompile has a couple of enhancements designed to make it easier to ignore inference triggers that come from the test suite itself. First, suggest.(itrigs) may show something like this:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":" ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"This indicates a broadcasting operation in the @testset itself. Second, while it's a little dangerous (because suggest cannot entirely be trusted), you can filter these out:","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"julia> itrigsel = [itrig for itrig in itrigs if !isignorable(suggest(itrig))];\n\njulia> length(itrigs)\n222\n\njulia> length(itrigsel)\n71","category":"page"},{"location":"tutorials/snoop_inference_analysis/","page":"Using @snoop_inference results to improve inferrability","title":"Using @snoop_inference results to improve inferrability","text":"While there is some risk of discarding triggers that provide clues about the origin of other triggers (e.g., they would have shown up in the same branch of the trigger_tree), the shorter list may help direct your attention to the \"real\" issues.","category":"page"},{"location":"tutorials/pgdsgui/#pgds","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Julia's multiple dispatch allows developers to create methods for specific argument types. On top of this, the Julia compiler performs automatic specialization:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"function countnonzeros(A::AbstractArray)\n    ...\nend","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"will be compiled separately for Vector{Int}, Matrix{Float64}, SubArray{...}, and so on, if it gets called for each of these types. Each specialization (each MethodInstance with different argument types) costs extra inference and code-generation time, so while specialization often improves runtime performance, that has to be weighed against the cost in latency. There are also cases in which overspecialization can hurt both run-time and compile-time performance. Consequently, an analysis of specialization can be a powerful tool for improving package quality.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"SnoopCompile ships with an interactive tool, pgdsgui, short for \"Profile-guided despecialization.\" The name is a reference to a related technique, profile-guided optimization (PGO). Both PGO and PGDS use runtime profiling to help guide decisions about code optimization. PGO is often used in languages whose default mode is to avoid specialization, whereas PGDS seems more appropriate for a language like Julia which specializes by default. While PGO is sometimes an automatic part of the compiler that optimizes code midstream during execution, SnoopCompile's PGDS is a tool for making static changes (edits) to code. Again, this seems appropriate for a language where specialization typically happens prior to the first execution of the code.","category":"page"},{"location":"tutorials/pgdsgui/#Add-SnoopCompileCore,-SnoopCompile,-and-helper-packages-to-your-environment","page":"Profile-guided despecialization","title":"Add SnoopCompileCore, SnoopCompile, and helper packages to your environment","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"We'll add these packages to your default environment so you can use them while in the package environment:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"using Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\", \"PyPlot\"]);","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"PyPLot is used for the PGDS interface in part to reduce interference with native-Julia plotting packages like Makie–it's a little awkward to depend on a package that you might be simultaneously modifying!","category":"page"},{"location":"tutorials/pgdsgui/#Using-the-PGDS-graphical-user-interface","page":"Profile-guided despecialization","title":"Using the PGDS graphical user interface","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"To illustrate the use of PGDS, we'll examine an example in which some methods get specialized for hundreds of types. To keep this example short, we'll create functions that operate on types themselves.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"note: Note\nAs background to this example, for a DataType T, T.name returns a Core.TypeName, and T.name.name returns the name as a Symbol. Base.unwrap_unionall(T) preserves DataTypes as-is, but converts a UnionAll type into a DataType.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"\"\"\"\n    spelltype(T::Type)\n\nSpell out a type's name, one character at a time.\n\"\"\"\nfunction spelltype(::Type{T}) where T\n    name = Base.unwrap_unionall(T).name.name\n    str = \"\"\n    for c in string(name)\n        str *= c\n    end\n    return str\nend\n\n\"\"\"\n    mappushes!(f, dest, src)\n\nLike `map!` except it grows `dest` by one for each element in `src`.\n\"\"\"\nfunction mappushes!(f, dest, src)\n    for item in src\n        push!(dest, f(item))\n    end\n    return dest\nend\n\nmappushes(f, src) = mappushes!(f, [], src)","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"There are two stages to PGDS: first (and preferrably starting in a fresh Julia session), we profile type-inference:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> using SnoopCompileCore\n\njulia> Ts = subtypes(Any);  # get a long list of different types\n\njulia> tinf = @snoop_inference mappushes(spelltype, Ts);","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Then, in the same session, profile the runtime:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> using Profile\n\njulia> @profile mappushes(spelltype, Ts);","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Typically, it's best if the workload here is reflective of a \"real\" workload (test suites often are not), so that you get a realistic view of where your code spends its time during actual use.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Now let's launch the PDGS GUI:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> using SnoopCompile\n\njulia> import PyPlot        # the GUI is dependent on PyPlot, must load it before the next line\n\njulia> mref, ax = pgdsgui(tinf);","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"You should see something like this:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"(Image: pgdsgui)","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In this graph, each dot corresponds to a single method; for this method, we plot inference time (vertical axis) against the run time (horizontal axis). The coloration of each dot encodes the number of specializations (the number of distinct MethodInstances) for that method; by default it even includes the number of times the method was inferred for specific constants (constant propagation), although you can exclude those cases using the consts=false keyword. Finally, the edge of each dot encodes the fraction of time spent on runtime dispatch (aka, type-instability), with black indicating 0% and bright red indicating 100%.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In this plot, we can see that no method runs for more than 0.01 seconds, whereas some methods have an aggregate inference time of up to 1s. Overall, inference-time dominates this plot. Moreover, for the most expensive cases, the number of specializations is in the hundreds or thousands.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"To learn more about what is being specialized, just click on one of the dots; if you choose the upper-left dot (the one with highest inference time), you should see something like this in your REPL:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"spelltype(::Type{T}) where T in Main at REPL[1]:6 (586 specializations)","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"This tells you the method corresponding to this dot. Moreover, mref (one of the outputs of pgdsgui) holds this method:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> mref[]\nspelltype(::Type{T}) where T in Main at REPL[1]:6","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"What are the specializations, and how costly was each?","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> collect_for(mref[], tinf)\n586-element Vector{SnoopCompileCore.InferenceTimingNode}:\n InferenceTimingNode: 0.003486/0.020872 on InferenceFrameInfo for spelltype(::Type{T}) where T with 7 direct children\n InferenceTimingNode: 0.003281/0.003892 on InferenceFrameInfo for spelltype(::Type{AbstractArray}) with 2 direct children\n InferenceTimingNode: 0.003349/0.004023 on InferenceFrameInfo for spelltype(::Type{AbstractChannel}) with 2 direct children\n InferenceTimingNode: 0.000827/0.001154 on InferenceFrameInfo for spelltype(::Type{AbstractChar}) with 5 direct children\n InferenceTimingNode: 0.003326/0.004070 on InferenceFrameInfo for spelltype(::Type{AbstractDict}) with 2 direct children\n InferenceTimingNode: 0.000833/0.001159 on InferenceFrameInfo for spelltype(::Type{AbstractDisplay}) with 5 direct children\n⋮\n InferenceTimingNode: 0.000848/0.001160 on InferenceFrameInfo for spelltype(::Type{YAML.Span}) with 5 direct children\n InferenceTimingNode: 0.000838/0.001148 on InferenceFrameInfo for spelltype(::Type{YAML.Token}) with 5 direct children\n InferenceTimingNode: 0.000833/0.001150 on InferenceFrameInfo for spelltype(::Type{YAML.TokenStream}) with 5 direct children\n InferenceTimingNode: 0.000809/0.001126 on InferenceFrameInfo for spelltype(::Type{YAML.YAMLDocIterator}) with 5 direct children","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"So we can see that one MethodInstance for each type in Ts was generated.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"If you see a list of MethodInstances, and the first is extremely costly in terms of inclusive time, but all the rest are not, then you might not need to worry much about over-specialization: your inference time will be dominated by that one costly method (often, the first time the method was called), and the fact that lots of additional specializations were generated may not be anything to worry about. However, in this case, the distribution of time is fairly flat, each contributing a small portion to the overall time. In such cases, over-specialization may be a problem.","category":"page"},{"location":"tutorials/pgdsgui/#Reducing-specialization-with-@nospecialize","page":"Profile-guided despecialization","title":"Reducing specialization with @nospecialize","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"How might we change this? To reduce the number of specializations of spelltype, we use @nospecialize in its definition:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"function spelltype(@nospecialize(T::Type))\n    name = Base.unwrap_unionall(T).name.name\n    str = \"\"\n    for c in string(name)\n        str *= c\n    end\n    return str\nend","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"warning: Warning\nwhere type-parameters force specialization: in spelltype(@nospecialize(::Type{T})) where T, the @nospecialize has no impact and you'll get full specialization on T. Instead, use @nospecialize(T::Type) (without the where statement) as shown.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"If we now rerun that demo, you should see a plot of the same kind as shown above, but with different costs for each dot. The differences are best appreciated comparing them side-by-side (pgdsgui allows you to specify a particular axis into which to plot):","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"(Image: pgdsgui-compare)","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"The results with @nospecialize are shown on the right. You can see that:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Now, the most expensive-to-infer method is <0.01s (formerly it was ~1s)\nNo method has more than 2 specializations","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Moreover, our runtimes (post-compilation) really aren't very different, both in the ballpark of a few millseconds (you can check with @btime from BenchmarkTools to be sure).","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In total, we've reduced compilation time approximately 50× without appreciably hurting runtime performance. Reducing specialization, when appropriate, can often yield your biggest reductions in latency.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"tip: Tip\nWhen you add @nospecialize, sometimes it's beneficial to compensate for the loss of inferrability by adding some type assertions. This topic will be discussed in greater detail in the next section, but for the example above we can improve runtime performance by annotating the return type of Base.unwrap_unionall(T): name = (Base.unwrap_unionall(T)::DataType).name.name. Then, later lines in spelltype know that name is a Symbol.With this change, the unspecialized variant outperforms the specialized variant in both compile-time and run-time. The reason is that the specialized variant of spell needs to be called by runtime dispatch, whereas for the unspecialized variant there's only one MethodInstance, so its dispatch is handled at compile time.","category":"page"},{"location":"tutorials/pgdsgui/#Blocking-inference:-Base.@nospecializeinfer","page":"Profile-guided despecialization","title":"Blocking inference: Base.@nospecializeinfer","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Perhaps surprisingly, @nospecialize doesn't prevent Julia's type-inference from inspecting a method. The reason is that it's sometimes useful if the caller knows what type will be returned, even if the callee doesn't exploit this information. In our mappushes example, this isn't an issue, because Ts is a Vector{Any} and this already defeats inference. But in other cases, the caller may be inferable but (to save inference time) you'd prefer to block inference from inspecting the method.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Beginning with Julia 1.10, you can prevent even inference from \"looking at\" @nospecialized arguments with Base.@nospecializeinfer:","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Base.@nospecializeinfer function spelltype(@nospecialize(T::Type))\n    name = (Base.unwrap_unionall(T)::DataType).name.name\n    str = \"\"\n    for c in string(name)\n        str *= c\n    end\n    return str\nend","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Note that the ::DataType annotation described in the tip above is still effective and recommended. @nospecializeinfer directly affects only arguments that are marked with @nospecialize, and in this case the type-assertion prevents type uncertainty from propagating to the remainder of the function.","category":"page"},{"location":"tutorials/pgdsgui/#Argument-standardization","page":"Profile-guided despecialization","title":"Argument standardization","text":"","category":"section"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"While not immediately relevant to the example above, a very important technique that falls within the domain of reducing specialization is argument standardization: instead of","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"function foo(x, y)\n    # some huge function, slow to compile, and you'd prefer not to compile it many times for different types of x and y\nend","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"consider whether you can safely write this as","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"function foo(x::X, y::Y)   # X and Y are concrete types\n    # some huge function, but the concrete typing ensures you only compile it once\nend\nfoo(x, y) = foo(convert(X, x)::X, convert(Y, y)::Y)   # this allows you to still call it with any argument types","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"The \"standardizing method\" foo(x, y) is short and therefore quick to compile, so it doesn't really matter if you compile many different instances.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"tip: Tip\nIn convert(X, x)::X, the final ::X guards against a broken convert method that fails to return an object of type X. Without it, foo(x, y) might call itself in an infinite loop, ultimately triggering a StackOverflowError. StackOverflowErrors are a particularly nasty form of error, and the typeassert ensures that you get a simple TypeError instead.In other contexts, such typeasserts would also have the effect of fixing inference problems even if the type of x is not well-inferred, but in this case dispatch to foo(x::X, y::Y) would have ensured the same outcome.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"There are of course cases where you can't implement your code in this way: after all, part of the power of Julia is the ability of generic methods to \"do the right thing\" for a wide variety of types. But in cases where you're doing a standard task, e.g., writing some data to a file, there's really no good reason to recompile your save method for a filename encoded as a String and again for a SubString{String} and again for a SubstitutionString and again for an AbstractString and ...: after all, the core of the save method probably isn't sensitive to the precise encoding of the filename.  In such cases, it should be safe to convert all filenames to String, thereby reducing the diversity of input arguments for expensive-to-compile methods.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"If you're using pgdsgui, the cost of inference and the number of specializations may guide you to click on specific dots; collect_for(mref[], tinf) then allows you to detect and diagnose cases where argument standardization might be helpful.","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"You can do the same analysis without pgdsgui. The opportunity for argument standardization is often facilitated by looking at, e.g.,","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> tms = accumulate_by_source(flatten(tinf));  # collect all MethodInstances that belong to the same Method\n\njulia> t, m = tms[end-1]        # the ones towards the end take the most time, maybe they are over-specialized?\n(0.4138147, save(filename::AbstractString, data) in SomePkg at /pathto/SomePkg/src/SomePkg.jl:23)\n\njulia> methodinstances(m)       # let's see what specializations we have\n7-element Vector{Core.MethodInstance}:\n MethodInstance for save(::String, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::SubString{String}, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::AbstractString, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::String, ::Vector{SomePkg.SomeDataType{SubString{String}}})\n MethodInstance for save(::SubString{String}, ::Array)\n MethodInstance for save(::String, ::Vector{var\"#s92\"} where var\"#s92\"<:SomePkg.SomeDataType)\n MethodInstance for save(::String, ::Array)","category":"page"},{"location":"tutorials/pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In this case we have 7 MethodInstances (some of which are clearly due to poor inferrability of the caller) when one might suffice.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Data-collection","page":"Reference","title":"Data collection","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"SnoopCompileCore.@snoop_invalidations\nSnoopCompileCore.@snoop_inference\nSnoopCompileCore.@snoop_llvm","category":"page"},{"location":"reference/#SnoopCompileCore.@snoop_invalidations","page":"Reference","title":"SnoopCompileCore.@snoop_invalidations","text":"invs = @snoop_invalidations expr\n\nCapture method cache invalidations triggered by evaluating expr. invs is a sequence of invalidated Core.MethodInstances together with \"explanations,\" consisting of integers (encoding depth) and strings (documenting the source of an invalidation).\n\nUnless you are working at a low level, you essentially always want to pass invs directly to SnoopCompile.invalidation_trees.\n\nExtended help\n\ninvs is in a format where the \"reason\" comes after the items. Method deletion results in the sequence\n\n[zero or more (mi, \"invalidate_mt_cache\") pairs..., zero or more (depth1 tree, loctag) pairs..., method, loctag] with loctag = \"jl_method_table_disable\"\n\nwhere mi means a MethodInstance. depth1 means a sequence starting at depth=1.\n\nMethod insertion results in the sequence\n\n[zero or more (depth0 tree, sig) pairs..., same info as with delete_method except loctag = \"jl_method_table_insert\"]\n\nThe authoritative reference is Julia's own src/gf.c file.\n\n\n\n\n\n","category":"macro"},{"location":"reference/#SnoopCompileCore.@snoop_inference","page":"Reference","title":"SnoopCompileCore.@snoop_inference","text":"tinf = @snoop_inference commands;\n\nProduce a profile of julia's type inference, recording the amount of time spent inferring every MethodInstance processed while executing commands. Each fresh entrance to type inference (whether executed directly in commands or because a call was made by runtime-dispatch) also collects a backtrace so the caller can be identified.\n\ntinf is a tree, each node containing data on a particular inference \"frame\" (the method, argument-type specializations, parameters, and even any constant-propagated values). Each reports the exclusive/inclusive times, where the exclusive time corresponds to the time spent inferring this frame in and of itself, whereas the inclusive time includes the time needed to infer all the callees of this frame.\n\nThe top-level node in this profile tree is ROOT. Uniquely, its exclusive time corresponds to the time spent not in julia's type inference (codegen, llvm_opt, runtime, etc).\n\nWorking with tinf effectively requires loading SnoopCompile.\n\nwarning: Warning\nNote the semicolon ; at the end of the @snoop_inference macro call. Because SnoopCompileCore is not permitted to invalidate any code, it cannot define the Base.show methods that pretty-print tinf. Defer inspection of tinf until SnoopCompile has been loaded.\n\nExample\n\njulia> tinf = @snoop_inference begin\n           sort(rand(100))  # Evaluate some code and profile julia's type inference\n       end;\n\n\n\n\n\n","category":"macro"},{"location":"reference/#SnoopCompileCore.@snoop_llvm","page":"Reference","title":"SnoopCompileCore.@snoop_llvm","text":"@snoop_llvm \"func_names.csv\" \"llvm_timings.yaml\" begin\n    # Commands to execute, in a new process\nend\n\ncauses the julia compiler to log timing information for LLVM optimization during the provided commands to the files \"funcnames.csv\" and \"llvmtimings.yaml\". These files can be used for the input to SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\").\n\nThe logs contain the amount of time spent optimizing each \"llvm module\", and information about each module, where a module is a collection of functions being optimized together.\n\n\n\n\n\n","category":"macro"},{"location":"reference/#GUIs","page":"Reference","title":"GUIs","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"flamegraph\npgdsgui","category":"page"},{"location":"reference/#SnoopCompile.flamegraph","page":"Reference","title":"SnoopCompile.flamegraph","text":"flamegraph(tinf::InferenceTimingNode; tmin=0.0, excluded_modules=Set([Main]), mode=nothing)\n\nConvert the call tree of inference timings returned from @snoop_inference into a FlameGraph. Returns a FlameGraphs.FlameGraph structure that represents the timing trace recorded for type inference.\n\nFrames that take less than tmin seconds of inclusive time will not be included in the resultant FlameGraph (meaning total time including it and all of its children). This can be helpful if you have a very big profile, to save on processing time.\n\nNon-precompilable frames are marked in reddish colors. excluded_modules can be used to mark methods defined in modules to which you cannot or do not wish to add precompiles.\n\nmode controls how frames are named in tools like ProfileView. nothing uses the default of just the qualified function name, whereas supplying mode=Dict(method => count) counting the number of specializations of each method will cause the number of specializations to be included in the frame name.\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.002148974/0.002767166 on Core.Compiler.Timings.ROOT() with 1 direct children\n\njulia> fg = flamegraph(tinf)\nNode(FlameGraphs.NodeData(ROOT() at typeinfer.jl:75, 0x00, 0:3334431))\n\njulia> ProfileView.view(fg);  # Display the FlameGraph in a package that supports it\n\nYou should be able to reconcile the resulting flamegraph to print_tree(tinf) (see flatten).\n\nThe empty horizontal periods in the flamegraph correspond to times when something other than inference is running. The total width of the flamegraph is set from the ROOT node.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.pgdsgui","page":"Reference","title":"SnoopCompile.pgdsgui","text":"methodref, ax = pgdsgui(tinf::InferenceTimingNode; consts::Bool=true, by=inclusive)\nmethodref     = pgdsgui(ax, tinf::InferenceTimingNode; kwargs...)\n\nCreate a scatter plot comparing:     - (vertical axis) the inference time for all instances of each Method, as captured by tinf;     - (horizontal axis) the run time cost, as estimated by capturing a @profile before calling this function.\n\nEach dot corresponds to a single method. The face color encodes the number of times that method was inferred, and the edge color corresponds to the fraction of the runtime spent on runtime dispatch (black is 0%, bright red is 100%). Clicking on a dot prints the method (or location, if inlined) to the REPL, and sets methodref[] to that method.\n\nax is the pyplot axis of the scatterplot.\n\ncompat: Compat\npgdsgui depends on PyPlot via Julia extensions. You must load both SnoopCompile and PyPlot for this function to be defined.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Analysis-of-invalidations","page":"Reference","title":"Analysis of invalidations","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"uinvalidated\ninvalidation_trees\nprecompile_blockers\nfiltermod\nfindcaller\nreport_invalidations","category":"page"},{"location":"reference/#SnoopCompile.uinvalidated","page":"Reference","title":"SnoopCompile.uinvalidated","text":"umis = uinvalidated(invlist)\n\nReturn the unique invalidated MethodInstances. invlist is obtained from SnoopCompileCore.@snoop_invalidations. This is similar to filtering for MethodInstances in invlist, except that it discards any tagged \"invalidate_mt_cache\". These can typically be ignored because they are nearly inconsequential: they do not invalidate any compiled code, they only transiently affect an optimization of runtime dispatch.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.invalidation_trees","page":"Reference","title":"SnoopCompile.invalidation_trees","text":"trees = invalidation_trees(list)\n\nParse list, as captured by SnoopCompileCore.@snoop_invalidations, into a set of invalidation trees, where parents nodes were called by their children.\n\nExample\n\njulia> f(x::Int)  = 1\nf (generic function with 1 method)\n\njulia> f(x::Bool) = 2\nf (generic function with 2 methods)\n\njulia> applyf(container) = f(container[1])\napplyf (generic function with 1 method)\n\njulia> callapplyf(container) = applyf(container)\ncallapplyf (generic function with 1 method)\n\njulia> c = Any[1]\n1-element Array{Any,1}:\n 1\n\njulia> callapplyf(c)\n1\n\njulia> trees = invalidation_trees(@snoop_invalidations f(::AbstractFloat) = 3)\n1-element Array{SnoopCompile.MethodInvalidations,1}:\n inserting f(::AbstractFloat) in Main at REPL[36]:1 invalidated:\n   mt_backedges: 1: signature Tuple{typeof(f),Any} triggered MethodInstance for applyf(::Array{Any,1}) (1 children) more specific\n\nSee the documentation for further details.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.precompile_blockers","page":"Reference","title":"SnoopCompile.precompile_blockers","text":"staletrees = precompile_blockers(invalidations, tinf::InferenceTimingNode)\n\nSelect just those invalidations that contribute to \"stale nodes\" in tinf, and link them together. This can allow one to identify specific blockers of precompilation for particular MethodInstances.\n\nExample\n\nusing SnoopCompileCore\ninvalidations = @snoop_invalidations using PkgA, PkgB;\nusing SnoopCompile\ntrees = invalidation_trees(invalidations)\ntinf = @snoop_inference begin\n    some_workload()\nend\nstaletrees = precompile_blockers(trees, tinf)\n\nIn many cases, this reduces the number of invalidations that require analysis by one or more orders of magnitude.\n\ninfo: Info\nprecompile_blockers is experimental and has not yet been thoroughly vetted by real-world use. Users are encouraged to try it and report any \"misses\" or unnecessary \"hits.\"\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.filtermod","page":"Reference","title":"SnoopCompile.filtermod","text":"modtrigs = filtermod(mod::Module, mtrigs::AbstractVector{MethodTriggers})\n\nSelect just the method-based triggers arising from a particular module.\n\n\n\n\n\nthinned = filtermod(module, trees::AbstractVector{MethodInvalidations}; recursive=false)\n\nSelect just the cases of invalidating a method defined in module.\n\nIf recursive is false, only the roots of trees are examined (i.e., the proximal source of the invalidation must be in module). If recursive is true, then thinned contains all routes to a method in module.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.findcaller","page":"Reference","title":"SnoopCompile.findcaller","text":"methinvs = findcaller(method::Method, trees)\n\nFind a path through trees that reaches method. Returns a single MethodInvalidations object.\n\nExamples\n\nSuppose you know that loading package SomePkg triggers invalidation of f(data). You can find the specific source of invalidation as follows:\n\nf(data)                             # run once to force compilation\nm = @which f(data)\nusing SnoopCompile\ntrees = invalidation_trees(@snoop_invalidations using SomePkg)\nmethinvs = findcaller(m, trees)\n\nIf you don't know which method to look for, but know some operation that has had added latency, you can look for methods using @snoopi. For example, suppose that loading SomePkg makes the next using statement slow. You can find the source of trouble with\n\njulia> using SnoopCompile\n\njulia> trees = invalidation_trees(@snoop_invalidations using SomePkg);\n\njulia> tinf = @snoopi using SomePkg            # this second `using` will need to recompile code invalidated above\n1-element Array{Tuple{Float64,Core.MethodInstance},1}:\n (0.08518409729003906, MethodInstance for require(::Module, ::Symbol))\n\njulia> m = tinf[1][2].def\nrequire(into::Module, mod::Symbol) in Base at loading.jl:887\n\njulia> findcaller(m, trees)\ninserting ==(x, y::SomeType) in SomeOtherPkg at /path/to/code:100 invalidated:\n   backedges: 1: superseding ==(x, y) in Base at operators.jl:83 with MethodInstance for ==(::Symbol, ::Any) (16 children) more specific\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.report_invalidations","page":"Reference","title":"SnoopCompile.report_invalidations","text":"report_invalidations(\n    io::IO = stdout;\n    invalidations,\n    n_rows::Int = 10,\n    process_filename::Function = x -> x,\n)\n\nPrint a tabular summary of invalidations given:\n\ninvalidations the output of SnoopCompileCore.@snoop_invalidations\n\nand (optionally)\n\nio::IO IO stream. Defaults to stdout\nn_rows::Int the number of rows to be displayed in the truncated table. A value of 0 indicates no truncation. A positive value will truncate the table to the specified number of rows.\nprocess_filename(::String)::String a function to post-process each filename, where invalidations are found\n\nExample usage\n\nimport SnoopCompileCore\ninvalidations = SnoopCompileCore.@snoop_invalidations begin\n\n    # load packages & define any additional methods\n\nend;\n\nusing SnoopCompile\nusing PrettyTables # to load report_invalidations\nreport_invalidations(;invalidations)\n\nUsing report_invalidations requires that you first load the PrettyTables.jl package.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Analysis-of-@snoop_inference","page":"Reference","title":"Analysis of @snoop_inference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"flatten\nexclusive\ninclusive\naccumulate_by_source\ncollect_for\nstaleinstances\ninference_triggers\ntrigger_tree\nsuggest\nisignorable\ncallerinstance\ncallingframe\nskiphigherorder\nInferenceTrigger\nruntime_inferencetime\nSnoopCompile.parcel\nSnoopCompile.write\nreport_callee\nreport_callees\nreport_caller","category":"page"},{"location":"reference/#SnoopCompile.flatten","page":"Reference","title":"SnoopCompile.flatten","text":"flatten(tinf; tmin = 0.0, sortby=exclusive)\n\nFlatten the execution graph of InferenceTimingNodes returned from @snoop_inference into a Vector of InferenceTiming frames, each encoding the time needed for inference of a single MethodInstance. By default, results are sorted by exclusive time (the time for inferring the MethodInstance itself, not including any inference of its callees); other options are sortedby=inclusive which includes the time needed for the callees, or nothing to obtain them in the order they were inferred (depth-first order).\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.002148974/0.002767166 on Core.Compiler.Timings.ROOT() with 1 direct children\n\njulia> using AbstractTrees; print_tree(tinf)\nInferenceTimingNode: 0.00242354/0.00303526 on Core.Compiler.Timings.ROOT() with 1 direct children\n└─ InferenceTimingNode: 0.000150891/0.000611721 on SnoopCompile.FlattenDemo.packintype(::Int64) with 2 direct children\n   ├─ InferenceTimingNode: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64) with 0 direct children\n   └─ InferenceTimingNode: 9.43e-5/0.000355512 on SnoopCompile.FlattenDemo.dostuff(::SnoopCompile.FlattenDemo.MyType{Int64}) with 2 direct children\n      ├─ InferenceTimingNode: 6.6458e-5/0.000124716 on SnoopCompile.FlattenDemo.extract(::SnoopCompile.FlattenDemo.MyType{Int64}) with 2 direct children\n      │  ├─ InferenceTimingNode: 3.401e-5/3.401e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, ::Symbol) with 0 direct children\n      │  └─ InferenceTimingNode: 2.4248e-5/2.4248e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, x::Symbol) with 0 direct children\n      └─ InferenceTimingNode: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64) with 0 direct children\n\nNote the printing of getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, x::Symbol): it shows the specific Symbol, here :x, that getproperty was inferred with. This reflects constant-propagation in inference.\n\nThen:\n\njulia> flatten(tinf; sortby=nothing)\n8-element Vector{SnoopCompileCore.InferenceTiming}:\n InferenceTiming: 0.002423543/0.0030352639999999998 on Core.Compiler.Timings.ROOT()\n InferenceTiming: 0.000150891/0.0006117210000000001 on SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 9.43e-5/0.00035551200000000005 on SnoopCompile.FlattenDemo.dostuff(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 6.6458e-5/0.000124716 on SnoopCompile.FlattenDemo.extract(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 3.401e-5/3.401e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, ::Symbol)\n InferenceTiming: 2.4248e-5/2.4248e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, x::Symbol)\n InferenceTiming: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64)\n\njulia> flatten(tinf; tmin=1e-4)                        # sorts by exclusive time (the time before the '/')\n4-element Vector{SnoopCompileCore.InferenceTiming}:\n InferenceTiming: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64)\n InferenceTiming: 0.000150891/0.0006117210000000001 on SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.002423543/0.0030352639999999998 on Core.Compiler.Timings.ROOT()\n\njulia> flatten(tinf; sortby=inclusive, tmin=1e-4)      # sorts by inclusive time (the time after the '/')\n6-element Vector{SnoopCompileCore.InferenceTiming}:\n InferenceTiming: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 6.6458e-5/0.000124716 on SnoopCompile.FlattenDemo.extract(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64)\n InferenceTiming: 9.43e-5/0.00035551200000000005 on SnoopCompile.FlattenDemo.dostuff(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 0.000150891/0.0006117210000000001 on SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.002423543/0.0030352639999999998 on Core.Compiler.Timings.ROOT()\n\nAs you can see, sortby affects not just the order but also the selection of frames; with exclusive times, dostuff did not on its own rise above threshold, but it does when using inclusive times.\n\nSee also: accumulate_by_source.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompileCore.exclusive","page":"Reference","title":"SnoopCompileCore.exclusive","text":"exclusive(frame)\n\nReturn the time spent inferring frame, not including the time needed for any of its callees.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompileCore.inclusive","page":"Reference","title":"SnoopCompileCore.inclusive","text":"inclusive(frame)\n\nReturn the time spent inferring frame and its callees.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.accumulate_by_source","page":"Reference","title":"SnoopCompile.accumulate_by_source","text":"accumulate_by_source(flattened; tmin = 0.0, by=exclusive)\n\nAdd the inference timings for all MethodInstances of a single Method together. flattened is the output of flatten. Returns a list of (t, method) tuples.\n\nWhen the accumulated time for a Method is large, but each instance is small, it indicates that it is being inferred for many specializations (which might include specializations with different constants).\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.004978/0.005447 on Core.Compiler.Timings.ROOT() with 1 direct children\n\njulia> accumulate_by_source(flatten(tinf))\n7-element Vector{Tuple{Float64, Union{Method, Core.MethodInstance}}}:\n (4.6294999999999996e-5, getproperty(x, f::Symbol) @ Base Base.jl:37)\n (5.8965e-5, dostuff(y) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:45)\n (6.4141e-5, extract(y::SnoopCompile.FlattenDemo.MyType) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:36)\n (8.9997e-5, (var\"#ctor-self#\"::Type{SnoopCompile.FlattenDemo.MyType{T}} where T)(x) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:35)\n (9.2256e-5, domath(x) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:41)\n (0.000117514, packintype(x) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:37)\n (0.004977755, ROOT() @ Core.Compiler.Timings compiler/typeinfer.jl:79)\n\nCompared to the output from flatten, the two inferences passes on getproperty have been consolidated into a single aggregate call.\n\n\n\n\n\nmtrigs = accumulate_by_source(Method, itrigs::AbstractVector{InferenceTrigger})\n\nConsolidate inference triggers via their caller method. mtrigs is a vector of Method=>list pairs, where list is a list of InferenceTriggers.\n\n\n\n\n\nloctrigs = accumulate_by_source(itrigs::AbstractVector{InferenceTrigger})\n\nAggregate inference triggers by location (function, file, and line number) of the caller.\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_demo:\n\njulia> itrigs = inference_triggers(SnoopCompile.itrigs_demo())\n2-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:763)\n Inference triggered to call MethodInstance for double(::Float64) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:763)\n\njulia> accumulate_by_source(itrigs)\n1-element Vector{SnoopCompile.LocationTriggers}:\n    calldouble1 at /pathto/SnoopCompile/src/parcel_snoop_inference.jl:762 (2 callees from 1 callers)\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.collect_for","page":"Reference","title":"SnoopCompile.collect_for","text":"list = collect_for(m::Method, tinf::InferenceTimingNode)\nlist = collect_for(m::MethodInstance, tinf::InferenceTimingNode)\n\nCollect all InferenceTimingNodes (descendants of tinf) that match m.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.staleinstances","page":"Reference","title":"SnoopCompile.staleinstances","text":"staleinstances(tinf::InferenceTimingNode)\n\nReturn a list of InferenceTimingNodes corresponding to MethodInstances that have \"stale\" code (specifically, CodeInstances with outdated max_world world ages). These may be a hint that invalidation occurred while running the workload provided to @snoop_inference, and consequently an important origin of (re)inference.\n\nwarning: Warning\nstaleinstances only looks retrospectively for stale code; it does not distinguish whether the code became stale while running @snoop_inference from whether it was already stale before execution commenced.\n\nWhile staleinstances is recommended as a useful \"sanity check\" to run before performing a detailed analysis of inference, any serious examination of invalidation should use @snoop_invalidations.\n\nFor more information about world age, see https://docs.julialang.org/en/v1/manual/methods/#Redefining-Methods.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.inference_triggers","page":"Reference","title":"SnoopCompile.inference_triggers","text":"itrigs = inference_triggers(tinf::InferenceTimingNode; exclude_toplevel=true)\n\nCollect the \"triggers\" of inference, each a fresh entry into inference via a call dispatched at runtime. All the entries in itrigs are previously uninferred, or are freshly-inferred for specific constant inputs.\n\nexclude_toplevel determines whether calls made from the REPL, include, or test suites are excluded.\n\nExample\n\nWe'll use SnoopCompile.itrigs_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.itrigs_demo()\nInferenceTimingNode: 0.004490576/0.004711168 on Core.Compiler.Timings.ROOT() with 2 direct children\n\njulia> itrigs = inference_triggers(tinf)\n2-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/inference_demos.jl:86) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/inference_demos.jl:87)\n Inference triggered to call MethodInstance for double(::Float64) from calldouble1 (/pathto/SnoopCompile/src/inference_demos.jl:86) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/inference_demos.jl:87)\n\njulia> edit(itrigs[1])     # opens an editor at the spot in the caller\n\njulia> using Cthulhu\n\njulia> ascend(itrigs[2])   # use Cthulhu to inspect the stacktrace (caller is the second item in the trace)\nChoose a call for analysis (q to quit):\n >   double(::Float64)\n       calldouble1 at /pathto/SnoopCompile/src/inference_demos.jl:86 => calldouble2(::Vector{Vector{Any}}) at /pathto/SnoopCompile/src/inference_demos.jl:87\n         calleach(::Vector{Vector{Vector{Any}}}) at /pathto/SnoopCompile/src/inference_demos.jl:88\n...\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.trigger_tree","page":"Reference","title":"SnoopCompile.trigger_tree","text":"root = trigger_tree(itrigs)\n\nOrganize inference triggers itrigs in tree format, grouping items via the call tree.\n\nIt is a tree rather than a more general graph due to the fact that caching inference results means that each node gets visited only once.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.suggest","page":"Reference","title":"SnoopCompile.suggest","text":"suggest(itrig::InferenceTrigger)\n\nAnalyze itrig and attempt to suggest an interpretation or remedy. This returns a structure of type Suggested; the easiest thing to do with the result is to show it; however, you can also filter a list of suggestions.\n\nExample\n\njulia> itrigs = inference_triggers(tinf);\n\njulia> sugs = suggest.(itrigs);\n\njulia> sugs_important = filter(!isignorable, sugs)    # discard the ones that probably don't need to be addressed\n\nwarning: Warning\nSuggestions are approximate at best; most often, the proposed fixes should not be taken literally, but instead taken as a hint about the \"outcome\" of a particular runtime dispatch incident. The suggestions target calls made with non-inferrable argumets, but often the best place to fix the problem is at an earlier stage in the code, where the argument was first computed.You can get much deeper insight via ascend (and Cthulhu generally), and even stacktrace is often useful. Suggestions are intended to be a quick and easier-to-comprehend first pass at analyzing an inference trigger.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.isignorable","page":"Reference","title":"SnoopCompile.isignorable","text":"isignorable(s::Suggested)\n\nReturns true if s is unlikely to be an inference problem in need of fixing.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.callerinstance","page":"Reference","title":"SnoopCompile.callerinstance","text":"mi = callerinstance(itrig::InferenceTrigger)\n\nReturn the MethodInstance mi of the caller in the selected stackframe in itrig.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.callingframe","page":"Reference","title":"SnoopCompile.callingframe","text":"itrigcaller = callingframe(itrig::InferenceTrigger)\n\n\"Step out\" one layer of the stacktrace, referencing the caller of the current frame of itrig.\n\nYou can retrieve the proximal trigger of inference with InferenceTrigger(itrigcaller).\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_demo:\n\njulia> itrig = inference_triggers(SnoopCompile.itrigs_demo())[1]\nInference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:763)\n\njulia> itrigcaller = callingframe(itrig)\nInference triggered to call MethodInstance for double(::UInt8) from calleach (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:764) with specialization MethodInstance for calleach(::Vector{Vector{Vector{Any}}})\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.skiphigherorder","page":"Reference","title":"SnoopCompile.skiphigherorder","text":"itrignew = skiphigherorder(itrig; exact::Bool=false)\n\nAttempt to skip over frames of higher-order functions that take the callee as a function-argument. This can be useful if you're analyzing inference triggers for an entire package and would prefer to assign triggers to package-code rather than Base functions like map!, broadcast, etc.\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_higherorder_demo:\n\njulia> itrig = inference_triggers(SnoopCompile.itrigs_higherorder_demo())[1]\nInference triggered to call MethodInstance for double(::Float64) from mymap! (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:706) with specialization MethodInstance for mymap!(::typeof(SnoopCompile.ItrigHigherOrderDemo.double), ::Vector{Any}, ::Vector{Any})\n\njulia> callingframe(itrig)      # step out one (non-inlined) frame\nInference triggered to call MethodInstance for double(::Float64) from mymap (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:710) with specialization MethodInstance for mymap(::typeof(SnoopCompile.ItrigHigherOrderDemo.double), ::Vector{Any})\n\njulia> skiphigherorder(itrig)   # step out to frame that doesn't have `double` as a function-argument\nInference triggered to call MethodInstance for double(::Float64) from callmymap (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:711) with specialization MethodInstance for callmymap(::Vector{Any})\n\nwarn: Warn\nBy default skiphigherorder is conservative, and insists on being sure that it's the callee being passed to the higher-order function. Higher-order functions that do not get specialized (e.g., with ::Function argument types) will not be skipped over. You can pass exact=false to allow ::Function to also be passed over, but keep in mind that this may falsely skip some frames.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.InferenceTrigger","page":"Reference","title":"SnoopCompile.InferenceTrigger","text":"InferenceTrigger(callee::MethodInstance, callerframes::Vector{StackFrame}, btidx::Int, bt)\n\nOrganize information about the \"triggers\" of inference. callee is the MethodInstance requiring inference, callerframes, btidx and bt contain information about the caller. callerframes are the frame(s) of call site that triggered inference; it's a Vector{StackFrame}, rather than a single StackFrame, due to the possibility that the caller was inlined into something else, in which case the first entry is the direct caller and the last entry corresponds to the MethodInstance into which it was ultimately inlined. btidx is the index in bt, the backtrace collected upon entry into inference, corresponding to callerframes.\n\nInferenceTriggers are created by calling inference_triggers. See also: callerinstance and callingframe.\n\n\n\n\n\n","category":"type"},{"location":"reference/#SnoopCompile.runtime_inferencetime","page":"Reference","title":"SnoopCompile.runtime_inferencetime","text":"ridata = runtime_inferencetime(tinf::InferenceTimingNode; consts=true, by=inclusive)\nridata = runtime_inferencetime(tinf::InferenceTimingNode, profiledata; lidict, consts=true, by=inclusive)\n\nCompare runtime and inference-time on a per-method basis. ridata[m::Method] returns (trun, tinfer, nspecializations), measuring the approximate amount of time spent running m, inferring m, and the number of type-specializations, respectively. trun is estimated from profiling data, which the user is responsible for capturing before the call. Typically tinf is collected via @snoop_inference on the first call (in a fresh session) to a workload, and the profiling data collected on a subsequent call. In some cases you may need to repeat the workload several times to collect enough profiling samples.\n\nprofiledata and lidict are obtained from Profile.retrieve().\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.parcel","page":"Reference","title":"SnoopCompile.parcel","text":"ttot, pcs = SnoopCompile.parcel(tinf::InferenceTimingNode)\n\nParcel the \"root-most\" precompilable MethodInstances into separate modules. These can be used to generate precompile directives to cache the results of type-inference, reducing latency on first use.\n\nLoosely speaking, and MethodInstance is precompilable if the module that owns the method also has access to all the types it need to precompile the instance. When the root node of an entrance to inference is not itself precompilable, parcel examines the children (and possibly, children's children...) until it finds the first node on each branch that is precompilable. MethodInstances are then assigned to the module that owns the method.\n\nttot is the total inference time; pcs is a list of module => (tmod, pclist) pairs. For each module, tmod is the amount of inference time affiliated with methods owned by that module; pclist is a list of (t, mi) time/MethodInstance tuples.\n\nSee also: SnoopCompile.write.\n\nExample\n\nWe'll use SnoopCompile.itrigs_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.itrigs_demo()\nInferenceTimingNode: 0.004490576/0.004711168 on Core.Compiler.Timings.ROOT() with 2 direct children\n\njulia> ttot, pcs = SnoopCompile.parcel(tinf);\n\njulia> ttot\n0.000220592\n\njulia> pcs\n1-element Vector{Pair{Module, Tuple{Float64, Vector{Tuple{Float64, Core.MethodInstance}}}}}:\n SnoopCompile.ItrigDemo => (0.000220592, [(9.8986e-5, MethodInstance for double(::Float64)), (0.000121606, MethodInstance for double(::UInt8))])\n\nSince there was only one module, ttot is the same as tmod. The ItrigDemo module had two precomilable MethodInstances, each listed with its corresponding inclusive time.\n\n\n\n\n\nmodtrigs = SnoopCompile.parcel(mtrigs::AbstractVector{MethodTriggers})\n\nSplit method-based triggers into collections organized by the module in which the methods were defined. Returns a module => list vector, with the module having the most MethodTriggers last.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.write","page":"Reference","title":"SnoopCompile.write","text":"write(prefix::AbstractString, pc::Dict; always::Bool = false)\n\nWrite each modules' precompiles to a separate file.  If always is true, the generated function will always run the precompile statements when called, otherwise the statements will only be called during package precompilation.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.report_callee","page":"Reference","title":"SnoopCompile.report_callee","text":"To use report_callee do using JET\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.report_callees","page":"Reference","title":"SnoopCompile.report_callees","text":"To use report_callees do using JET\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.report_caller","page":"Reference","title":"SnoopCompile.report_caller","text":"To use report_caller do using JET\n\n\n\n\n\n","category":"function"},{"location":"reference/#Analysis-of-LLVM","page":"Reference","title":"Analysis of LLVM","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"SnoopCompile.read_snoop_llvm","category":"page"},{"location":"reference/#SnoopCompile.read_snoop_llvm","page":"Reference","title":"SnoopCompile.read_snoop_llvm","text":"times, info = SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\"; tmin_secs=0.0)\n\nReads the log file produced by the compiler and returns the structured representations.\n\nThe results will only contain modules that took longer than tmin_secs to optimize.\n\nReturn value\n\ntimes contains the time spent optimizing each module, as a Pair from the time to an\n\narray of Strings, one for every MethodInstance in that llvm module.\n\ninfo is a Dict containing statistics for each MethodInstance encountered, from before\n\nand after optimization, including number of instructions and number of basicblocks.\n\nExample\n\njulia> @snoop_llvm \"func_names.csv\" \"llvm_timings.yaml\" begin\n           using InteractiveUtils\n           @eval InteractiveUtils.peakflops()\n       end\nLaunching new julia process to run commands...\ndone.\n\njulia> times, info = SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\", tmin_secs = 0.025);\n\njulia> times\n3-element Vector{Pair{Float64, Vector{String}}}:\n 0.028170923 => [\"Tuple{typeof(LinearAlgebra.copy_transpose!), Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}, Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}}\"]\n 0.031356962 => [\"Tuple{typeof(Base.copyto!), Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}, Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}}\"]\n 0.149138788 => [\"Tuple{typeof(LinearAlgebra._generic_matmatmul!), Array{Float64, 2}, Char, Char, Array{Float64, 2}, Array{Float64, 2}, LinearAlgebra.MulAddMul{true, true, Bool, Bool}}\"]\n\njulia> info\nDict{String, NamedTuple{(:before, :after), Tuple{NamedTuple{(:instructions, :basicblocks), Tuple{Int64, Int64}}, NamedTuple{(:instructions, :basicblocks), Tuple{Int64, Int64}}}}} with 3 entries:\n  \"Tuple{typeof(LinearAlgebra.copy_transpose!), Ar… => (before = (instructions = 651, basicblocks = 83), after = (instructions = 348, basicblocks = 40…\n  \"Tuple{typeof(Base.copyto!), Array{Float64, 2}, … => (before = (instructions = 617, basicblocks = 77), after = (instructions = 397, basicblocks = 37…\n  \"Tuple{typeof(LinearAlgebra._generic_matmatmul!)… => (before = (instructions = 4796, basicblocks = 824), after = (instructions = 1421, basicblocks =…\n\n\n\n\n\n","category":"function"},{"location":"reference/#Demos","page":"Reference","title":"Demos","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"SnoopCompile.flatten_demo\nSnoopCompile.itrigs_demo\nSnoopCompile.itrigs_higherorder_demo","category":"page"},{"location":"reference/#SnoopCompile.flatten_demo","page":"Reference","title":"SnoopCompile.flatten_demo","text":"tinf = SnoopCompile.flatten_demo()\n\nA simple demonstration of @snoop_inference. This demo defines a module\n\nmodule FlattenDemo\n    struct MyType{T} x::T end\n    extract(y::MyType) = y.x\n    function packintype(x)\n        y = MyType{Int}(x)\n        return dostuff(y)\n    end\n    function domath(x)\n        y = x + x\n        return y*x + 2*x + 5\n    end\n    dostuff(y) = domath(extract(y))\nend\n\nIt then \"warms up\" (forces inference on) all of Julia's Base methods needed for domath, to ensure that these MethodInstances do not need to be inferred when we collect the data. It then returns the results of\n\n@snoop_inference FlattenDemo.packintypes(1)\n\nSee flatten for an example usage.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.itrigs_demo","page":"Reference","title":"SnoopCompile.itrigs_demo","text":"tinf = SnoopCompile.itrigs_demo()\n\nA simple demonstration of collecting inference triggers. This demo defines a module\n\nmodule ItrigDemo\n@noinline double(x) = 2x\n@inline calldouble1(c) = double(c[1])\ncalldouble2(cc) = calldouble1(cc[1])\ncalleach(ccs) = (calldouble2(ccs[1]), calldouble2(ccs[2]))\nend\n\nIt then \"warms up\" (forces inference on) calldouble2(::Vector{Vector{Any}}), calldouble1(::Vector{Any}), double(::Int):\n\ncc = [Any[1]]\nItrigDemo.calleach([cc,cc])\n\nThen it collects and returns inference data using\n\ncc1, cc2 = [Any[0x01]], [Any[1.0]]\n@snoop_inference ItrigDemo.calleach([cc1, cc2])\n\nThis does not require any new inference for calldouble2 or calldouble1, but it does force inference on double with two new types. See inference_triggers to see what gets collected and returned.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.itrigs_higherorder_demo","page":"Reference","title":"SnoopCompile.itrigs_higherorder_demo","text":"tinf = SnoopCompile.itrigs_higherorder_demo()\n\nA simple demonstration of handling higher-order methods with inference triggers. This demo defines a module\n\nmodule ItrigHigherOrderDemo\ndouble(x) = 2x\n@noinline function mymap!(f, dst, src)\n    for i in eachindex(dst, src)\n        dst[i] = f(src[i])\n    end\n    return dst\nend\n@noinline mymap(f::F, src) where F = mymap!(f, Vector{Any}(undef, length(src)), src)\ncallmymap(src) = mymap(double, src)\nend\n\nThe key feature of this set of definitions is that the function double gets passed as an argument through mymap and mymap! (the latter are higher-order functions).\n\nIt then \"warms up\" (forces inference on) callmymap(::Vector{Any}), mymap(::typeof(double), ::Vector{Any}), mymap!(::typeof(double), ::Vector{Any}, ::Vector{Any}) and double(::Int):\n\nItrigHigherOrderDemo.callmymap(Any[1, 2])\n\nThen it collects and returns inference data using\n\n@snoop_inference ItrigHigherOrderDemo.callmymap(Any[1.0, 2.0])\n\nwhich forces inference for double(::Float64).\n\nSee skiphigherorder for an example using this demo.\n\n\n\n\n\n","category":"function"},{"location":"explanations/gotchas/#Precompilation-\"gotcha\"s","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"","category":"section"},{"location":"explanations/gotchas/#running-during-pc","page":"Precompilation \"gotcha\"s","title":"Running code during module definition","text":"","category":"section"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"Suppose you're working on an astronomy package and your source code has a line","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"const planets = map(makeplanet, [\"Mercury\", ...])","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"Julia will dutifully create planets and store it in the package's precompile cache file. This also runs makeplanet, and if this is the first time it gets run, it will compile makeplanet. Assuming that makeplanet is a method defined in the package, the compiled code for makeplanet will be stored in the cache file.","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"However, two circumstances can lead to puzzling omissions from the cache files:","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"if makeplanet is a method defined in a dependency of your package, it will not be cached in your package. You'd want to add precompilation of makeplanet to the package that creates that method.\nif makeplanet is poorly-infered and uses runtime dispatch, any such callees that are not owned by your package will not be cached. For example, suppose makeplanet ends up calling methods in Base Julia or its standard libraries that are not precompiled into Julia itself: the compiled code for those methods will not be added to the cache file.","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"One option to ensure this dependent code gets cached is to create planets inside PrecompileTools.@compile_workload:","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"@compile_workload begin\n    global planets\n    const planet = map(makeplanet, [\"Mercury\", ...])\nend","category":"page"},{"location":"explanations/gotchas/","page":"Precompilation \"gotcha\"s","title":"Precompilation \"gotcha\"s","text":"Note that your package definition can have multiple @compile_workload blocks.","category":"page"},{"location":"explanations/basic/#Understanding-SnoopCompile-and-Julia's-compilation-pipeline","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"","category":"section"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"Julia uses Just-in-time (JIT) compilation to generate the code that runs on your CPU. Broadly speaking, there are two major compilation steps: inference and code generation. Inference is the process of determining the type of each object, which in turn determines which specific methods get called; once type inference is complete, code generation performs optimizations and ultimately generates the assembly language (native code) used on CPUs. Some aspects of this process are documented here.","category":"page"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"Using code that has never been compiled requires that it first be JIT-compiled, and this contributes to the latency of using the package. In some circumstances, you can cache (store) the results of compilation to files to reduce the latency when your package is used. These files are the the *.ji and *.so files that live in the compiled directory of your Julia depot, usually located at ~/.julia/compiled. However, if these files become large, loading them can be another source for latency. Julia needs time both to load and validate the cached compiled code. Minimizing the latency of using a package involves focusing on caching the compilation of code that is both commonly used and takes time to compile.","category":"page"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"Caching code for later use is called precompilation. Julia has had some forms of precompilation almost since the very first packages. However, it was Julia 1.9 that first supported \"complete\" precompilation, including the ability to store native code in shared-library cache files.","category":"page"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"SnoopCompile is designed to try to allow you to analyze the costs of JIT-compilation, identify key bottlenecks that contribute to latency, and set up precompile directives to see whether it produces measurable benefits.","category":"page"},{"location":"explanations/basic/#Package-precompilation","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Package precompilation","text":"","category":"section"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"When a package is precompiled, here's what happens under the hood:","category":"page"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"Julia loads all of the package's dependencies (the ones in the [deps] section of the Project.toml file), typically from precompile cache files\nJulia evaluates the source code (text files) that define the package module(s). Evaluating function foo(args...) ... end creates a new method foo. Note that:\nthe source code might also contain statements that create \"data\" (e.g., consts). In some cases this can lead to some subtle precompilation \"gotchas\"\nthe source code might also contain a precompile workload, which forces compilation and tracking of package methods.\nJulia iterates over the module contents and writes the result to disk. Note that the module contents might include compiled code, and if so it is written along with everything else to the cache file.","category":"page"},{"location":"explanations/basic/","page":"Understanding SnoopCompile and Julia's compilation pipeline","title":"Understanding SnoopCompile and Julia's compilation pipeline","text":"When Julia loads your package, it just loads the \"snapshot\" stored in the cache file: it does not re-evaluate the source-text files that defined your package! It is appropriate to think of the source files of your package as \"build scripts\" that create your module; once the \"build scripts\" are executed, it's the module itself that gets cached, and the job of the build scripts is done.","category":"page"},{"location":"tutorials/invalidations/#Tutorial-on-@snoop_invalidations","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/#What-are-invalidations?","page":"Tutorial on @snoop_invalidations","title":"What are invalidations?","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"In this context, invalidation means discarding previously-compiled code. Invalidations occur because of interactions between independent pieces of code. Invalidations are essential to make Julia fast, interactive, and correct: you need invalidations if you want to be able to define some methods, run (compile) some code, and then in the same session define new methods that might lead to different answers if you were to recompile the code in the presence of the new methods.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Invalidations can happen just from loading packages. Packages are precompiled in isolation, but you can load many packages into a single interactive session. It's impossible for the individual packages to anticipate the full \"world of methods\" in your interactive session, so sometimes Julia has to discard code that was compiled in a smaller world because it's at risk for being incorrect in the larger world.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"The downside of invalidations is that they make latency worse, as code must be recompiled when you first run it. The benefits of precompilation are partially lost, and the work done during precompilation is partially wasted.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"While some invalidations are unavoidable, in practice a good developer can often design packages to minimize the number and/or impact of invalidations. Invalidation-resistant code is often faster, with smaller binary size, than code that is vulnerable to invalidation.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"A good first step is to measure what's being invalidated, and why.","category":"page"},{"location":"tutorials/invalidations/#Learning-to-observe,-diagnose,-and-fix-invalidations","page":"Tutorial on @snoop_invalidations","title":"Learning to observe, diagnose, and fix invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"We'll illustrate invalidations by creating two packages, where loading the second package invalidates some code that was compiled in the first one. We'll then go over approaches for \"fixing\" invalidations (i.e., preventing them from occuring).","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tip: Tip\nSince SnoopCompile's tools are interactive, you are strongly encouraged to try these examples yourself as you read along.","category":"page"},{"location":"tutorials/invalidations/#Add-SnoopCompileCore,-SnoopCompile,-and-helper-packages-to-your-environment","page":"Tutorial on @snoop_invalidations","title":"Add SnoopCompileCore, SnoopCompile, and helper packages to your environment","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Here, we'll add these packages to your default environment. (With the exception of AbstractTrees, these \"developer tool\" packages should not be added to the Project file of any real packages unless you're extending the tool itself.) From your default environment (i.e., in package mode you should see something like (@v1.10) pkg>), do","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"using Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\", \"AbstractTrees\", \"Cthulhu\"]);","category":"page"},{"location":"tutorials/invalidations/#Create-the-demonstration-packages","page":"Tutorial on @snoop_invalidations","title":"Create the demonstration packages","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"We're going to implement a toy version of the card game blackjack, where players take cards with the aim of collecting 21 points. The higher you go the better, unless you go over 21 points, in which case you \"go bust\" (i.e., you lose). Because our real goal is to illustrate invalidations, we'll create a \"blackjack ecosystem\" that involves an interaction between two packages.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"While PkgTemplates is recommended for creating packages, here we'll just use the basic capabilities in Pkg. To create the (empty) packages, the code below executes the following steps:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"navigate to a temporary directory and create both packages\nmake the first package (Blackjack) depend on PrecompileTools (we're interested in reducing latency!)\nmake the second package (BlackjackFacecards) depend on the first one (Blackjack)","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"oldproj = Base.active_project()   # hide\ncd(mktempdir())\nusing Pkg\nPkg.generate(\"Blackjack\");\nPkg.activate(\"Blackjack\")\nPkg.add(\"PrecompileTools\");\nPkg.generate(\"BlackjackFacecards\");\nPkg.activate(\"BlackjackFacecards\")\nPkg.develop(PackageSpec(path=joinpath(pwd(), \"Blackjack\")));","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Now it's time to create the code for Blackjack. Normally, you'd do this with an editor, but to make it reproducible here we'll use code to create these packages. The package code we'll create below defines the following:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"a score function to assign a numeric value to a card\ntallyscore which adds the total score for a hand of cards\nplaygame which uses a simple strategy to decide whether to take another card from the deck and add it to the hand","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"To reduce latency on first use, we then precompile playgame. In a real application, we'd also want a function to manage the deck of cards, but for brevity we'll omit this and do it manually.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"write(joinpath(\"Blackjack\", \"src\", \"Blackjack.jl\"), \"\"\"\n    module Blackjack\n\n    using PrecompileTools\n\n    export playgame\n\n    const deck = []   # the deck of cards that can be dealt\n\n    # Compute the score of one card\n    score(card::Int) = card\n\n    # Add up the score in a hand of cards\n    function tallyscores(cards)\n        s = 0\n        for card in cards\n            s += score(card)\n        end\n        return s\n    end\n\n    # Play the game! We use a simple strategy to decide whether to draw another card.\n    function playgame()\n        myhand = []\n        while tallyscores(myhand) <= 14 && !isempty(deck)\n            push!(myhand, pop!(deck))   # \"Hit me!\"\n        end\n        myscore = tallyscores(myhand)\n        return myscore <= 21 ? myscore : \"Busted\"\n    end\n\n    # Precompile `playgame`:\n    @setup_workload begin\n        push!(deck, 8, 10)    # initialize the deck\n        @compile_workload begin\n            playgame()\n        end\n    end\n\n    end\n    \"\"\")","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Suppose you use Blackjack and like it, but you notice it doesn't support face cards. Perhaps you're nervous about contributing to the Blackjack package (you shouldn't be!), and so you decide to start your own package that extends its functionality. You create BlackjackFacecards to add scoring of the jack, queen, king, and ace (for simplicity we'll make the ace always worth 11):","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"write(joinpath(\"BlackjackFacecards\", \"src\", \"BlackjackFacecards.jl\"), \"\"\"\n    module BlackjackFacecards\n\n    using Blackjack\n\n    # Add a new `score` method:\n    Blackjack.score(card::Char) = card ∈ ('J', 'Q', 'K') ? 10 :\n                                  card == 'A' ? 11 : error(card, \" not known\")\n\n    end\n    \"\"\")","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"warning: Warning\nBecause BlackjackFacecards \"owns\" neither Char nor score, this is piracy and should generally be avoided. Piracy is one way to cause invalidations, but it's not the only one. BlackjackFacecards could avoid committing piracy by defining a struct Facecard ... end and defining score(card::Facecard) instead of score(card::Char). However, this would not fix the invalidations–all the factors described below are unchanged.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Now we're ready!","category":"page"},{"location":"tutorials/invalidations/#Recording-invalidations","page":"Tutorial on @snoop_invalidations","title":"Recording invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Here are the steps executed by the code below","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"load SnoopCompileCore\nload Blackjack and BlackjackFacecards while recording invalidations with the @snoop_invalidations macro.\nload SnoopCompile and AbstractTrees for analysis","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"using SnoopCompileCore\ninvs = @snoop_invalidations using Blackjack, BlackjackFacecards;\nusing SnoopCompile, AbstractTrees","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tip: Tip\nIf you get errors like Package SnoopCompileCore not found in current path, a likely explanation is that you didn't add it to your default environment. In the example above, we're in the BlackjackFacecards environment so we can develop the package, but you also need access to SnoopCompile and SnoopCompileCore. Having these in your default environment lets them be found even if they aren't part of the current environment.","category":"page"},{"location":"tutorials/invalidations/#Analyzing-invalidations","page":"Tutorial on @snoop_invalidations","title":"Analyzing invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Now we're ready to see what, if anything, got invalidated:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"trees = invalidation_trees(invs)","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"This has only one \"tree\" of invalidations. trees is a Vector so we can index it:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tree = trees[1]","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Each tree stems from a single cause described in the top line. For this tree, the cause was adding the new method score(::Char) in BlackjackFacecards.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Each cause is associated with one or more victims of invalidation, a list here named mt_backedges. Let's extract the final (and in this case, only) victim:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"sig, victim = tree.mt_backedges[end];","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"note: Note\nmt_backedges stands for \"MethodTable backedges.\" In other cases you may see a second type of invalidation, just called backedges. With these, there is no sig, and so you'll use just victim = tree.backedges[i].","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"First let's look at the the problematic method signature:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"sig","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"This is a type-tuple, i.e., Tuple{typeof(f), typesof(args)...}. We see that score was called on an object of (inferred) type Any. Calling a function with unknown argument types makes code vulnerable to invalidation, and insertion of the new score method \"exploited\" this vulnerability.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"victim shows which compiled code got invalidated:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"victim","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"But this is not the full extent of what got invalidated:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"print_tree(victim)","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Invalidations propagate throughout entire call trees, here up to playgame(): anything that calls code that may no longer be correct is itself at risk for being incorrect. In general, victims with lots of \"children\" deserve the greatest attention.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"While print_tree can be useful, Cthulhu's ascend is a far more powerful tool for gaining deeper insight:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"julia> using Cthulhu\n\njulia> ascend(victim)\nChoose a call for analysis (q to quit):\n >   tallyscores(::Vector{Any})\n       playgame()","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"This is an interactive REPL-menu, described more completely (via text and video) at ascend.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"There are quite a few other tools for working with invs and trees, see the Reference. If your list of invalidations is dauntingly large, you may be interested in precompile_blockers.","category":"page"},{"location":"tutorials/invalidations/#Why-the-invalidations-occur","page":"Tutorial on @snoop_invalidations","title":"Why the invalidations occur","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tallyscores and playgame were compiled in Blackjack, a \"world\" where the score method defined in BlackjackFacecards does not yet exist. When you load the BlackjackFacecards package, Julia must ask itself: now that this new score method exists, am I certain that I would compile tallyscores the same way? If the answer is \"no,\" Julia invalidates the old compiled code, and compiles a fresh version with full awareness of the new score method in BlackjackFacecards.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Why would the compilation of tallyscores change? Evidently, cards is a Vector{Any}, and this means that tallyscores can't guess what kind of object card might be, and thus it can't guess what kind of objects are passed into score. The crux of the invalidation is thus:","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"when Blackjack is compiled, inference does not know which score method will be called. However, at the time of compilation the only score method is for Int. Thus Julia will reason that anything that isn't an Int is going to trigger an error anyway, and so you might as well optimize tallyscore expecting all cards to be Ints.\nhowever, when BlackjackFacecards is loaded, suddenly there are two score methods supporting both Int and Char. Now Julia's guess that all cards will probably be Ints doesn't seem so likely to be true, and thus tallyscores should be recompiled.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Thus, invalidations arise from optimization based on what methods and types are \"in the world\" at the time of compilation (sometimes called world-splitting). This form of optimization can have performance benefits, but it also leaves your code vulnerable to invalidation.","category":"page"},{"location":"tutorials/invalidations/#Fixing-invalidations","page":"Tutorial on @snoop_invalidations","title":"Fixing invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"In broad strokes, there are three ways to prevent invalidation.","category":"page"},{"location":"tutorials/invalidations/#Method-1:-defer-compilation-until-the-full-world-is-known","page":"Tutorial on @snoop_invalidations","title":"Method 1: defer compilation until the full world is known","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"The first and simplest technique is to ensure that the full range of possibilties (the entire \"world of code\") is present before any compilation occurs. In this case, probably the best approach would be to merge the BlackjackFacecards package into Blackjack itself. Or, if you are a maintainer of the \"Blackjack ecosystem\" and have reasons for thinking that keeping the packages separate makes sense, you could alternatively move the PrecompileTools workload to BlackjackFacecards. Either approach should prevent the invalidations from occuring.","category":"page"},{"location":"tutorials/invalidations/#Method-2:-improve-inferability","page":"Tutorial on @snoop_invalidations","title":"Method 2: improve inferability","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"The second way to prevent invalidations is to improve the inferability of the victim(s). If Int and Char really are the only possible kinds of cards, then in playgame it would be better to declare","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"myhand = Union{Int,Char}[]","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"and similarly for deck itself. That untyped [] is what makes myhand (and thus cards, when passed to tallyscore) a Vector{Any}, and the possibilities for card are endless. By constraining the possible types, we allow inference to know more clearly what methods might be called. More tips on fixing invalidations through improving inference can be found in Techniques for fixing inference problems.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"In this particular case, just annotating Union{Int,Char}[] isn't sufficient on its own, because the score method for Char doesn't yet exist, so Julia doesn't know what to call. However, in most real-world cases this change alone would be sufficient: usually all the needed methods exist, it's just a question of reassuring Julia that no other options are even possible.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"note: Note\nThis fix leverages union-splitting, which is conceptually related to \"world-splitting.\" However, union-splitting is far more effective at fixing inference problems, as it guarantees that no other possibilities will ever exist, no matter how many other methods get defined.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tip: Tip\nMany vulnerabilities can be fixed by improving inference. In complex code, it's easy to unwittingly write things in ways that defeat Julia's type inference. Tools that help you discover inference problems, like SnoopCompile and JET, help you discover these unwitting \"mistakes.\"","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"While in real life it's usually a bad idea to \"blame the victim,\" it's typically the right attitude for fixing invalidations. Keep in mind, though, that the source of the problem may not be the immediate victim: in this case, it was a poor container choice in playgame that put tallyscore in the bad position of having to operate on a Vector{Any}.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Improving inferability is probably the most broadly-applicable technique, and when applicable it usually gives the best outcomes: not only is your code more resistant to invalidation, but it's likely faster and compiles to smaller binaries. However, of the three approaches it is also the one that requires the deepest understanding of Julia's type system, and thus may be difficult for some coders to use.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"There are cases where there is no good way to make the code inferable, in which case other strategies are needed.","category":"page"},{"location":"tutorials/invalidations/#Method-3:-disable-Julia's-speculative-optimization","page":"Tutorial on @snoop_invalidations","title":"Method 3: disable Julia's speculative optimization","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"The third option is to prevent Julia's speculative optimization: one could replace score(card) with invokelatest(score, card):","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"function tallyscores(cards)\n    s = 0\n    for card in cards\n        s += invokelatest(score, card)\n    end\n    return s\nend","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"This forces Julia to always look up the appropriate method of score while the code is running, and thus prevents the speculative optimizations that leave the code vulnerable to invalidation. However, the cost is that your code may run somewhat more slowly, particularly here where the call is inside a loop.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"If you plan to define at least two score methods, another way to turn off this optimization would be to declare","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Base.Experimental.@max_methods 1 function score end","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"before defining any score methods. You can read the documentation on @max_methods to learn more about how it works.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"tip: Tip\nMost of us learn best by doing. Try at least one of these methods of fixing the invalidation, and use SnoopCompile to verify that it works.","category":"page"},{"location":"tutorials/invalidations/#Undoing-the-damage-from-invalidations","page":"Tutorial on @snoop_invalidations","title":"Undoing the damage from invalidations","text":"","category":"section"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"If you can't prevent the invalidation, an alternative approach is to recompile the invalidated code. For example, one could repeat the precompile workload from Blackjack in BlackjackFacecards. While this will mean that the whole \"stack\" will be compiled twice and cached twice (which is wasteful), it should be effective in reducing latency for users.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"PrecompileTools also has a @recompile_invalidations. This isn't generally recommended for use in package (you can end up with long compile times for things you don't need), but it can be useful in personal \"Startup packages\" where you want to reduce latency for a particular project you're working on. See the PrecompileTools documentation for details.","category":"page"},{"location":"tutorials/invalidations/","page":"Tutorial on @snoop_invalidations","title":"Tutorial on @snoop_invalidations","text":"Pkg.activate(oldproj)   # hide","category":"page"},{"location":"tutorials/snoop_inference_parcel/#precompilation","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"","category":"section"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"In a few cases, it may be inconvenient or impossible to precompile using a workload. Some examples might be:","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"an application that opens graphical windows\nan application that connects to a database\nan application that creates, deletes, or rewrites files on disk","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"In such cases, one alternative is to create a manual list of precompile directives using Julia's precompile(f, argtypes) function.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"warning: Warning\nManual precompile directives are much more likely to \"go stale\" as the package is developed–-precompile does not throw an error if a method for the given argtypes cannot be found. They are also more likely to be dependent on the Julia version, operating system, or CPU architecture. Whenever possible, it's safer to use a workload.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"precompile directives have to be emitted by the module that owns the method and/or types. SnoopCompile comes with a tool, parcel, that splits out the \"root-most\" precompilable MethodInstances into their constituent modules. This will typically correspond to the bottom row of boxes in the flame graph. In cases where you have some that are not naively precompilable, they will include MethodInstances from higher up in the call tree.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"Let's use SnoopCompile.parcel on our OptimizeMe demo:","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"using SnoopCompileCore, SnoopCompile # here we need the SnoopCompile path for the next line (normally you should wait until after data collection is complete)\ninclude(joinpath(pkgdir(SnoopCompile), \"examples\", \"OptimizeMe.jl\"))\ntinf = @snoop_inference OptimizeMe.main();\nttot, pcs = SnoopCompile.parcel(tinf);\nttot\npcs","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"ttot shows the total amount of time spent on type-inference. parcel discovered precompilable MethodInstances for four modules, Core, Base.Multimedia, Base, and OptimizeMe that might benefit from precompile directives. These are listed in increasing order of inference time.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"Let's look specifically at OptimizeMeFixed, since that's under our control:","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"pcmod = pcs[end]\ntmod, tpcs = pcmod.second;\ntmod\ntpcs","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"This indicates the amount of time spent specifically on OptimizeMe, plus the list of calls that could be precompiled in that module.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"We could look at the other modules (packages) similarly.","category":"page"},{"location":"tutorials/snoop_inference_parcel/#SnoopCompile.write","page":"Using @snoop_inference to emit manual precompile directives","title":"SnoopCompile.write","text":"","category":"section"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"You can generate files that contain ready-to-use precompile directives using SnoopCompile.write:","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"SnoopCompile.write(\"/tmp/precompiles_OptimizeMe\", pcs)","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"You'll now find a directory /tmp/precompiles_OptimizeMe, and inside you'll find files for modules that could have precompile directives added manually. The contents of the last of these should be recognizable:","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"function _precompile_()\n    ccall(:jl_generating_output, Cint, ()) == 1 || return nothing\n    Base.precompile(Tuple{typeof(main)})   # time: 0.4204474\nend","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"The first ccall line ensures we only pay the cost of running these precompile directives if we're building the package; this is relevant mostly if you're running Julia with --compiled-modules=no, which can be a convenient way to disable precompilation and examine packages in their \"native state.\" (It would also matter if you've set __precompile__(false) at the top of your module, but if so why are you reading this?)","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"This file is ready to be moved into the OptimizeMe repository and included into your module definition.","category":"page"},{"location":"tutorials/snoop_inference_parcel/","page":"Using @snoop_inference to emit manual precompile directives","title":"Using @snoop_inference to emit manual precompile directives","text":"You might also consider submitting some of the other files (or their precompile directives) to the packages you depend on.","category":"page"},{"location":"#SnoopCompile.jl","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"Julia is fast, but its execution speed depends on optimizing code through compilation. Code must be compiled before you can use it, and unfortunately compilation is slow. This can cause latency the first time you use code: this latency is often called time-to-first-plot (TTFP) or more generally time-to-first-execution (TTFX). If something feels slow the first time you use it, and fast thereafter, you're probably experiencing the latency of compilation. Note that TTFX is distinct from time-to-load (TTL, which refers to the time you spend waiting for using MyPkg to finish), even though both contribute to latency.","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"Modern versions of Julia can store compiled code to disk (precompilation) to reduce or eliminate latency. Users and developers who are interested in reducing TTFX should first head to PrecompileTools, read its documentation thoroughly, and try using it to solve latency problems.","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"This package, SnoopCompile, should be considered when:","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"precompilation doesn't reduce TTFX as much as you wish\nprecompilation \"works,\" but only in isolation: as soon as you load (certain) additional packages, TTFX is bad again\nyou're wondering if you can reduce the amount of time needed to precompile your package and/or the size of the precompilation cache files","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"In other words, SnoopCompile is a diagonostic package that helps reveal the causes of latency. Historically, it proceeded PrecompileTools, and indeed PrecompileTools was split out from SnoopCompile. Today, SnoopCompile is generally needed only when PrecompileTools fails to deliver the desired benefits.","category":"page"},{"location":"#SnoopCompile-analysis-modes","page":"SnoopCompile.jl","title":"SnoopCompile analysis modes","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"SnoopCompile \"snoops\" on the Julia compiler, collecting information that may be useful to developers. Here are some of the things you can do with SnoopCompile:","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"diagnose invalidations, cases where Julia must throw away previously-compiled code (see Tutorial on @snoop_invalidations)\ntrace inference, to learn what code is being newly (or freshly) analyzed in an early stage of the compilation pipeline (Tutorial on @snoop_inference)\ntrace code generation by LLVM, a late stage in the compilation pipeline (Tutorial on @snoop_llvm)\nreveal methods with excessive numbers of compiler-generated specializations, a.k.a.profile-guided despecialization (Tutorial on PGDS)\nintegrate with tools like JET to further reduce the risk that your lovingly-precompiled code will be invalidated by loading other packages (Tutorial on JET integration)","category":"page"},{"location":"#Background-information","page":"SnoopCompile.jl","title":"Background information","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"If nothing else, you should know this:","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"invalidations occur when you load code (e.g., using MyPkg) or otherwise define new methods\ninference and other stages of compilation occur the first time you run code for a particular combination of input types","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"The individual tutorials briefly explain core concepts. More detail can be found in Understanding SnoopCompile and Julia's compilation pipeline.","category":"page"},{"location":"#Who-should-use-this-package","page":"SnoopCompile.jl","title":"Who should use this package","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"SnoopCompile is intended primarily for package developers who want to improve the experience for their users. It is also recommended for users who are willing to \"dig deep\" and understand why packages they depend on have high latency. Your experience with latency may be personal, as it can depend on the specific combination of packages you load. If latency troubles you, don't make the assumption that it must be unfixable: you might be the first person affected by that specific cause of latency.","category":"page"},{"location":"tutorials/snoop_llvm/#Tutorial-on-@snoop_llvm","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"","category":"section"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"Julia uses the LLVM compiler to generate machine code. Typically, the two main contributors to the overall compile time are inference and LLVM, and thus together @snoop_inference and @snoop_llvm collect fairly comprehensive data on the compiler.","category":"page"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"@snoop_llvm has a somewhat different design than @snoop_inference: while @snoop_inference runs in the same session that you'll be using for analysis (and thus requires that you remember to do the data gathering in a fresh session), @snoop_llvm spawns a fresh process to collect the data. The downside is that you get less interactivity, as the data have to be written out in intermediate forms as a text file.","category":"page"},{"location":"tutorials/snoop_llvm/#Add-SnoopCompileCore-and-SnoopCompile-to-your-environment","page":"Tutorial on @snoop_llvm","title":"Add SnoopCompileCore and SnoopCompile to your environment","text":"","category":"section"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"Here, we'll add these packages to your default environment.","category":"page"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"using Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\"]);","category":"page"},{"location":"tutorials/snoop_llvm/#Collecting-the-data","page":"Tutorial on @snoop_llvm","title":"Collecting the data","text":"","category":"section"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"Here's a simple demonstration of usage:","category":"page"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"using SnoopCompileCore\n@snoop_llvm \"func_names.csv\" \"llvm_timings.yaml\" begin\n    using InteractiveUtils\n    @eval InteractiveUtils.peakflops()\nend\n\nusing SnoopCompile\ntimes, info = SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\", tmin_secs = 0.025);","category":"page"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"This will write two files, \"func_names.csv\" and \"llvm_timings.yaml\", in your current working directory. Let's look at what was read from these files:","category":"page"},{"location":"tutorials/snoop_llvm/","page":"Tutorial on @snoop_llvm","title":"Tutorial on @snoop_llvm","text":"times\ninfo","category":"page"}]
}
