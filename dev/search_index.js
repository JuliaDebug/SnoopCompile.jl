var documenterSearchIndex = {"docs":
[{"title":"Package roles and alternatives","page":"Package roles and alternatives","location":"explanations/tools/#Package-roles-and-alternatives","category":"section","text":""},{"title":"SnoopCompileCore","page":"Package roles and alternatives","location":"explanations/tools/#SnoopCompileCore","category":"section","text":"SnoopCompileCore is a tiny package with no dependencies; it's used for collecting data, and it has been designed in such a way that it cannot cause any invalidations of its own. Collecting data on invalidations and inference with SnoopCompileCore is the only way you can be sure you are observing the \"native state\" of your code."},{"title":"SnoopCompile","page":"Package roles and alternatives","location":"explanations/tools/#SnoopCompile","category":"section","text":"SnoopCompile is a much larger package that performs analysis on the data collected by SnoopCompileCore; loading SnoopCompile can (and does) trigger invalidations. Consequently, you're urged to always collect data with just SnoopCompileCore loaded, and wait to load SnoopCompile until after you've finished collecting the data."},{"title":"Cthulhu","page":"Package roles and alternatives","location":"explanations/tools/#Cthulhu","category":"section","text":"Cthulhu is a companion package that gives deep insights into the origin of invalidations or inference failures."},{"title":"AbstractTrees","page":"Package roles and alternatives","location":"explanations/tools/#AbstractTrees","category":"section","text":"AbstractTrees is the one package in this list that can be both a \"workhorse\" and a developer tool. SnoopCompile uses it mostly for pretty-printing."},{"title":"JET","page":"Package roles and alternatives","location":"explanations/tools/#JET","category":"section","text":"JET is a powerful developer tool that in some ways is an alternative to SnoopCompile. While the two have different goals, the packages have some overlap in what they can tell you about your code. However, their mechanisms of action are fundamentally different:\n\nJET is a \"static analyzer,\" which means that it analyzes the code itself. JET can tell you about inference failures (runtime dispatch) much like SnoopCompile, with a major advantage: SnoopCompileCore omits information about any callees that are already compiled, but JET's @report_opt provides exhaustive information about the entire inferable callgraph (i.e., the part of the callgraph that inference can predict from the initial call) regardless of whether it has been previously compiled. With JET, you don't have to remember to run each analysis in a fresh session.\nSnoopCompileCore collects data by watching normal inference at work. On code that hasn't been compiled previously, this can yield results similar to JET's, with a different major advantage: JET can't \"see through\" runtime dispatch, but SnoopCompileCore can. With SnoopCompile, you can immediately get a wholistic view of your entire callgraph.\n\nCombining JET and SnoopCompile can provide insights that are difficult to obtain with either package in isolation. See the Tutorial on JET integration."},{"title":"Tutorial on @snoop_invalidations","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Tutorial-on-@snoop_invalidations","category":"section","text":""},{"title":"What are invalidations?","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#What-are-invalidations?","category":"section","text":"In this context, invalidation means discarding previously-compiled code. Invalidations occur because of interactions between independent pieces of code. Invalidations are essential to make Julia fast, interactive, and correct: you need invalidations if you want to be able to define some methods, run (compile) some code, and then in the same session define new methods that might lead to different answers if you were to recompile the code in the presence of the new methods.\n\nInvalidations can happen just from loading packages. Packages are precompiled in isolation, but you can load many packages into a single interactive session. It's impossible for the individual packages to anticipate the full \"world of methods\" in your interactive session, so sometimes Julia has to discard code that was compiled in a smaller world because it's at risk for being incorrect in the larger world.\n\nThe downside of invalidations is that they make latency worse, as code must be recompiled when you first run it. The benefits of precompilation are partially lost, and the work done during precompilation is partially wasted.\n\nWhile some invalidations are unavoidable, in practice a good developer can often design packages to minimize the number and/or impact of invalidations. Invalidation-resistant code is often faster, with smaller binary size, than code that is vulnerable to invalidation.\n\nA good first step is to measure what's being invalidated, and why."},{"title":"Learning to observe, diagnose, and fix invalidations","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Learning-to-observe,-diagnose,-and-fix-invalidations","category":"section","text":"We'll illustrate invalidations by creating two packages, where loading the second package invalidates some code that was compiled in the first one. We'll then go over approaches for \"fixing\" invalidations (i.e., preventing them from occuring).\n\ntip: Tip\nSince SnoopCompile's tools are interactive, you are strongly encouraged to try these examples yourself as you read along."},{"title":"Add SnoopCompileCore, SnoopCompile, and helper packages to your environment","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Add-SnoopCompileCore,-SnoopCompile,-and-helper-packages-to-your-environment","category":"section","text":"Here, we'll add these packages to your default environment. (With the exception of AbstractTrees, these \"developer tool\" packages should not be added to the Project file of any real packages unless you're extending the tool itself.) From your default environment (i.e., in package mode you should see something like (@v1.10) pkg>), do\n\nusing Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\", \"AbstractTrees\", \"Cthulhu\"]);"},{"title":"Create the demonstration packages","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Create-the-demonstration-packages","category":"section","text":"We're going to implement a toy version of the card game blackjack, where players take cards with the aim of collecting 21 points. The higher you go the better, unless you go over 21 points, in which case you \"go bust\" (i.e., you lose). Because our real goal is to illustrate invalidations, we'll create a \"blackjack ecosystem\" that involves an interaction between two packages.\n\nWhile PkgTemplates is recommended for creating packages, here we'll just use the basic capabilities in Pkg. To create the (empty) packages, the code below executes the following steps:\n\nnavigate to a temporary directory and create both packages\nmake the first package (Blackjack) depend on PrecompileTools (we're interested in reducing latency!)\nmake the second package (BlackjackFacecards) depend on the first one (Blackjack)\n\noldproj = Base.active_project()   # hide\ncd(mktempdir())\nusing Pkg\nPkg.generate(\"Blackjack\");\nPkg.activate(\"Blackjack\")\nPkg.add(\"PrecompileTools\");\nPkg.generate(\"BlackjackFacecards\");\nPkg.activate(\"BlackjackFacecards\")\nPkg.develop(PackageSpec(path=joinpath(pwd(), \"Blackjack\")));\n\nNow it's time to create the code for Blackjack. Normally, you'd do this with an editor, but to make it reproducible here we'll use code to create these packages. The package code we'll create below defines the following:\n\na score function to assign a numeric value to a card\ntallyscore which adds the total score for a hand of cards\nplaygame which uses a simple strategy to decide whether to take another card from the deck and add it to the hand\n\nTo reduce latency on first use, we then precompile playgame. In a real application, we'd also want a function to manage the deck of cards, but for brevity we'll omit this and do it manually.\n\nwrite(joinpath(\"Blackjack\", \"src\", \"Blackjack.jl\"), \"\"\"\n    module Blackjack\n\n    using PrecompileTools\n\n    export playgame\n\n    const deck = []   # the deck of cards that can be dealt\n\n    # Compute the score of one card\n    score(card::Int) = card\n\n    # Add up the score in a hand of cards\n    function tallyscores(cards)\n        s = 0\n        for card in cards\n            s += score(card)\n        end\n        return s\n    end\n\n    # Play the game! We use a simple strategy to decide whether to draw another card.\n    function playgame()\n        myhand = []\n        while tallyscores(myhand) <= 14 && !isempty(deck)\n            push!(myhand, pop!(deck))   # \"Hit me!\"\n        end\n        myscore = tallyscores(myhand)\n        return myscore <= 21 ? myscore : \"Busted\"\n    end\n\n    # Precompile `playgame`:\n    @setup_workload begin\n        push!(deck, 8, 10)    # initialize the deck\n        @compile_workload begin\n            playgame()\n        end\n    end\n\n    end\n    \"\"\")\n\nSuppose you use Blackjack and like it, but you notice it doesn't support face cards. Perhaps you're nervous about contributing to the Blackjack package (you shouldn't be!), and so you decide to start your own package that extends its functionality. You create BlackjackFacecards to add scoring of the jack, queen, king, and ace (for simplicity we'll make the ace always worth 11):\n\nwrite(joinpath(\"BlackjackFacecards\", \"src\", \"BlackjackFacecards.jl\"), \"\"\"\n    module BlackjackFacecards\n\n    using Blackjack\n\n    # Add a new `score` method:\n    Blackjack.score(card::Char) = card ∈ ('J', 'Q', 'K') ? 10 :\n                                  card == 'A' ? 11 : error(card, \" not known\")\n\n    end\n    \"\"\")\n\nwarning: Warning\nBecause BlackjackFacecards \"owns\" neither Char nor score, this is piracy and should generally be avoided. Piracy is one way to cause invalidations, but it's not the only one. BlackjackFacecards could avoid committing piracy by defining a struct Facecard ... end and defining score(card::Facecard) instead of score(card::Char). However, this would not fix the invalidations–all the factors described below are unchanged.\n\nNow we're ready!"},{"title":"Recording invalidations","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Recording-invalidations","category":"section","text":"Here are the steps executed by the code below\n\nload SnoopCompileCore\nload Blackjack and BlackjackFacecards while recording invalidations with the @snoop_invalidations macro.\nload SnoopCompile and AbstractTrees for analysis\n\nusing SnoopCompileCore\ninvs = @snoop_invalidations using Blackjack, BlackjackFacecards;\nusing SnoopCompile, AbstractTrees\n\ntip: Tip\nIf you get errors like Package SnoopCompileCore not found in current path, a likely explanation is that you didn't add it to your default environment. In the example above, we're in the BlackjackFacecards environment so we can develop the package, but you also need access to SnoopCompile and SnoopCompileCore. Having these in your default environment lets them be found even if they aren't part of the current environment."},{"title":"Analyzing invalidations","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Analyzing-invalidations","category":"section","text":"Now we're ready to see what, if anything, got invalidated:\n\ntrees = invalidation_trees(invs)\n\nThis has only one \"tree\" of invalidations. trees is a Vector so we can index it:\n\ntree = trees[1]\n\nEach tree stems from a single cause described in the top line. For this tree, the cause was adding the new method score(::Char) in BlackjackFacecards.\n\nnote: Note\nA tree with no cause indicates that the cause occurred before you turned on snooping.\n\nEach cause is associated with one or more victims of invalidation, a list here named mt_backedges. Let's extract the final (and in this case, only) victim:\n\nsig, victim = tree.mt_backedges[end];\n\nnote: Note\nmt_backedges stands for \"MethodTable backedges.\" In other cases you may see a second kind of invalidation, just called backedges. With these, there is no sig, and so you'll use just victim = tree.backedges[i]. For those curious about the reasons for these two kinds of invalidation, see Invalidation classes.\n\nFirst let's look at the the problematic method signature:\n\nsig\n\nThis is a type-tuple, i.e., Tuple{typeof(f), typesof(args)...}. We see that score was called on an object of (inferred) type Any. Calling a function with unknown argument types makes code vulnerable to invalidation, and insertion of the new score method \"exploited\" this vulnerability.\n\nvictim shows which compiled code got invalidated:\n\nvictim\n\nBut this is not the full extent of what got invalidated:\n\nprint_tree(victim)\n\nInvalidations propagate throughout entire call trees, here up to playgame(): anything that calls code that may no longer be correct is itself at risk for being incorrect. In general, victims with lots of \"children\" deserve the greatest attention.\n\nWhile print_tree can be useful, Cthulhu's ascend is a far more powerful tool for gaining deeper insight:\n\njulia> using Cthulhu\n\njulia> ascend(victim)\nChoose a call for analysis (q to quit):\n >   tallyscores(::Vector{Any})\n       playgame()\n\nThis is an interactive REPL-menu, described more completely (via text and video) at ascend.\n\nThere are quite a few other tools for working with invs and trees, see the Reference. If your list of invalidations is dauntingly large, you may be interested in precompile_blockers."},{"title":"Why the invalidations occur","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Why-the-invalidations-occur","category":"section","text":"tallyscores and playgame were compiled in Blackjack, a \"world\" where the score method defined in BlackjackFacecards does not yet exist. When you load the BlackjackFacecards package, Julia must ask itself: now that this new score method exists, am I certain that I would compile tallyscores the same way? If the answer is \"no,\" Julia invalidates the old compiled code, and compiles a fresh version with full awareness of the new score method in BlackjackFacecards.\n\nWhy would the compilation of tallyscores change? Evidently, cards is a Vector{Any}, and this means that tallyscores can't guess what kind of object card might be, and thus it can't guess what kind of objects are passed into score. The crux of the invalidation is thus:\n\nwhen Blackjack is compiled, inference does not know which score method will be called. However, at the time of compilation the only score method is for Int. Thus Julia will reason that anything that isn't an Int is going to trigger an error anyway, and so you might as well optimize tallyscore expecting all cards to be Ints.\nhowever, when BlackjackFacecards is loaded, suddenly there are two score methods supporting both Int and Char. Now Julia's guess that all cards will probably be Ints doesn't seem so likely to be true, and thus tallyscores should be recompiled.\n\nThus, invalidations arise from optimization based on what methods and types are \"in the world\" at the time of compilation (sometimes called world-splitting). This form of optimization can have performance benefits, but it also leaves your code vulnerable to invalidation."},{"title":"Fixing invalidations","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Fixing-invalidations","category":"section","text":"In broad strokes, there are three ways to prevent invalidation."},{"title":"Method 1: defer compilation until the full world is known","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Method-1:-defer-compilation-until-the-full-world-is-known","category":"section","text":"The first and simplest technique is to ensure that the full range of possibilties (the entire \"world of code\") is present before any compilation occurs. In this case, probably the best approach would be to merge the BlackjackFacecards package into Blackjack itself. Or, if you are a maintainer of the \"Blackjack ecosystem\" and have reasons for thinking that keeping the packages separate makes sense, you could alternatively move the PrecompileTools workload to BlackjackFacecards. Either approach should prevent the invalidations from occuring."},{"title":"Method 2: improve inferability","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Method-2:-improve-inferability","category":"section","text":"The second way to prevent invalidations is to improve the inferability of the victim(s). This approach is often applicable to mt_backedges invalidations,  but it can sometimes fix backedges invalidations too. Invalidation classes explains the differences in detail and why inference failures tend to be affiliated with mt_backedges invalidations.\n\nIn our blackjack example, if Int and Char really are the only possible kinds of cards, then in playgame it would be better to declare\n\nmyhand = Union{Int,Char}[]\n\nand similarly for deck itself. That untyped [] is what makes myhand (and thus cards, when passed to tallyscore) a Vector{Any}, and the possibilities for card are endless. By constraining the possible types, we allow inference to know more clearly what methods might be called. More tips on fixing invalidations through improving inference can be found in Techniques for fixing inference problems.\n\nIn this particular case, just annotating Union{Int,Char}[] isn't sufficient on its own, because the score method for Char doesn't yet exist, so Julia doesn't know what to call. However, in most real-world cases this change alone would be sufficient: usually all the needed methods exist, it's just a question of reassuring Julia that no other options are even possible.\n\nnote: Note\nThis fix leverages union-splitting, which is conceptually related to \"world-splitting.\" However, union-splitting is far more effective at fixing inference problems, as it guarantees that no other possibilities will ever exist, no matter how many other methods get defined.\n\ntip: Tip\nMany vulnerabilities can be fixed by improving inference. In complex code, it's easy to unwittingly write things in ways that defeat Julia's type inference. Tools that help you discover inference problems, like SnoopCompile and JET, help you discover these unwitting \"mistakes.\"\n\nWhile in real life it's usually a bad idea to \"blame the victim,\" it's typically the right attitude for fixing invalidations. Keep in mind, though, that the source of the problem may not be the immediate victim: in this case, it was a poor container choice in playgame that put tallyscore in the bad position of having to operate on a Vector{Any}.\n\nImproving inferability is probably the most broadly-applicable technique, and when applicable it usually gives the best outcomes: not only is your code more resistant to invalidation, but it's likely faster and compiles to smaller binaries. However, of the three approaches it is also the one that requires the deepest understanding of Julia's type system, and thus may be difficult for some coders to use.\n\nThere are cases where there is no good way to make the code inferable, in which case other strategies are needed."},{"title":"Method 3: disable Julia's speculative optimization","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Method-3:-disable-Julia's-speculative-optimization","category":"section","text":"The third option is to prevent Julia's speculative optimization: one could replace score(card) with invokelatest(score, card):\n\nfunction tallyscores(cards)\n    s = 0\n    for card in cards\n        s += invokelatest(score, card)\n    end\n    return s\nend\n\nThis forces Julia to always look up the appropriate method of score while the code is running, and thus prevents the speculative optimizations that leave the code vulnerable to invalidation. However, the cost is that your code may run somewhat more slowly, particularly here where the call is inside a loop.\n\nIf you plan to define at least two score methods, another way to turn off this optimization would be to declare\n\nBase.Experimental.@max_methods 1 function score end\n\nbefore defining any score methods. You can read the documentation on @max_methods to learn more about how it works.\n\ntip: Tip\nMost of us learn best by doing. Try at least one of these methods of fixing the invalidation, and use SnoopCompile to verify that it works."},{"title":"Undoing the damage from invalidations","page":"Tutorial on @snoop_invalidations","location":"tutorials/invalidations/#Undoing-the-damage-from-invalidations","category":"section","text":"If you can't prevent the invalidation, an alternative approach is to recompile the invalidated code. For example, one could repeat the precompile workload from Blackjack in BlackjackFacecards. While this will mean that the whole \"stack\" will be compiled twice and cached twice (which is wasteful), it should be effective in reducing latency for users.\n\nPrecompileTools also has a @recompile_invalidations. This isn't generally recommended for use in package (you can end up with long compile times for things you don't need), but it can be useful in personal \"Startup packages\" where you want to reduce latency for a particular project you're working on. See the PrecompileTools documentation for details.\n\nPkg.activate(oldproj)   # hide"},{"title":"Reference","page":"Reference","location":"reference/#Reference","category":"section","text":""},{"title":"Data collection","page":"Reference","location":"reference/#Data-collection","category":"section","text":""},{"title":"GUIs","page":"Reference","location":"reference/#GUIs","category":"section","text":""},{"title":"Analysis of invalidations","page":"Reference","location":"reference/#Analysis-of-invalidations","category":"section","text":""},{"title":"Analysis of @snoop_inference","page":"Reference","location":"reference/#Analysis-of-@snoop_inference","category":"section","text":""},{"title":"Analysis of LLVM","page":"Reference","location":"reference/#Analysis-of-LLVM","category":"section","text":""},{"title":"Demos","page":"Reference","location":"reference/#Demos","category":"section","text":""},{"title":"SnoopCompileCore.@snoop_invalidations","page":"Reference","location":"reference/#SnoopCompileCore.@snoop_invalidations","category":"macro","text":"invs = @snoop_invalidations expr\n\nCapture method cache invalidations triggered by evaluating expr. invs is a sequence of invalidated Core.MethodInstances together with \"explanations,\" consisting of integers (encoding depth) and strings (documenting the source of an invalidation).\n\nUnless you are working at a low level, you essentially always want to pass invs directly to SnoopCompile.invalidation_trees.\n\nExtended help\n\ninvs is in a format where the \"reason\" comes after the items. Method deletion results in the sequence\n\n[zero or more (mi, \"invalidate_mt_cache\") pairs..., zero or more (depth1 tree, loctag) pairs..., method, loctag] with loctag = \"jl_method_table_disable\"\n\nwhere mi means a MethodInstance. depth1 means a sequence starting at depth=1.\n\nMethod insertion results in the sequence\n\n[zero or more (depth0 tree, sig) pairs..., same info as with delete_method except loctag = \"jl_method_table_insert\"]\n\nThe authoritative reference is Julia's own src/gf.c file.\n\n\n\n\n\n"},{"title":"SnoopCompileCore.@snoop_inference","page":"Reference","location":"reference/#SnoopCompileCore.@snoop_inference","category":"macro","text":"tinf = @snoop_inference commands;\n\nProduce a profile of julia's type inference, recording the amount of time spent inferring every MethodInstance processed while executing commands. Each fresh entrance to type inference (whether executed directly in commands or because a call was made by runtime-dispatch) also collects a backtrace so the caller can be identified.\n\ntinf is a tree, each node containing data on a particular inference \"frame\" (the method, argument-type specializations, parameters, and even any constant-propagated values). Each reports the exclusive/inclusive times, where the exclusive time corresponds to the time spent inferring this frame in and of itself, whereas the inclusive time includes the time needed to infer all the callees of this frame.\n\nThe top-level node in this profile tree is ROOT. Uniquely, its exclusive time corresponds to the time spent not in julia's type inference (codegen, llvm_opt, runtime, etc).\n\nWorking with tinf effectively requires loading SnoopCompile.\n\nwarning: Warning\nNote the semicolon ; at the end of the @snoop_inference macro call. Because SnoopCompileCore is not permitted to invalidate any code, it cannot define the Base.show methods that pretty-print tinf. Defer inspection of tinf until SnoopCompile has been loaded.\n\nExample\n\njulia> tinf = @snoop_inference begin\n           sort(rand(100))  # Evaluate some code and profile julia's type inference\n       end;\n\n\n\n\n\n"},{"title":"SnoopCompileCore.@snoop_llvm","page":"Reference","location":"reference/#SnoopCompileCore.@snoop_llvm","category":"macro","text":"@snoop_llvm \"func_names.csv\" \"llvm_timings.yaml\" begin\n    # Commands to execute, in a new process\nend\n\ncauses the julia compiler to log timing information for LLVM optimization during the provided commands to the files \"funcnames.csv\" and \"llvmtimings.yaml\". These files can be used for the input to SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\").\n\nThe logs contain the amount of time spent optimizing each \"llvm module\", and information about each module, where a module is a collection of functions being optimized together.\n\n\n\n\n\n"},{"title":"SnoopCompile.flamegraph","page":"Reference","location":"reference/#SnoopCompile.flamegraph","category":"function","text":"flamegraph(tinf::InferenceTimingNode; include_llvm=true, tmin=0.0, excluded_modules=Set([Main]), mode=nothing)\n\nConvert the call tree of inference timings returned from @snoop_inference into a FlameGraph. Returns a FlameGraphs.FlameGraph structure that represents the timing trace recorded for type inference and, if include_llvm is true, LLVM compilation.\n\nFrames that take less than tmin seconds of inclusive time will not be included in the resultant FlameGraph (meaning total time including it and all of its children). This can be helpful if you have a very big profile, to save on processing time.\n\nNon-precompilable frames are marked in reddish colors. excluded_modules can be used to mark methods defined in modules to which you cannot or do not wish to add precompiles.\n\nmode controls how frames are named in tools like ProfileView. nothing uses the default of just the qualified function name, whereas mode=:spec will print the full specialization of each MethodInstance. Supplying a pre-populated mode=Dict(method => count) will cause count to be included in the frame name of the corresponding method. This can be used, for example, to display the number of specializations of each method.\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.002148974/0.002767166 on Base.Compiler.Timings.ROOT() with 1 direct children\n\njulia> fg = flamegraph(tinf)\nNode(FlameGraphs.NodeData(ROOT() at typeinfer.jl:75, 0x00, 0:3334431))\n\njulia> ProfileView.view(fg);  # Display the FlameGraph in a package that supports it\n\nYou should be able to reconcile the resulting flamegraph to print_tree(tinf) (see flatten).\n\nThe empty horizontal periods in the flamegraph correspond to times when something other than inference is running. The total width of the flamegraph is set from the ROOT node.\n\n\n\n\n\n"},{"title":"SnoopCompile.pgdsgui","page":"Reference","location":"reference/#SnoopCompile.pgdsgui","category":"function","text":"methodref, ax = pgdsgui(tinf::InferenceTimingNode; consts::Bool=true, by=inclusive)\nmethodref     = pgdsgui(ax, tinf::InferenceTimingNode; kwargs...)\n\nCreate a scatter plot comparing:     - (vertical axis) the inference time for all instances of each Method, as captured by tinf;     - (horizontal axis) the run time cost, as estimated by capturing a @profile before calling this function.\n\nEach dot corresponds to a single method. The face color encodes the number of times that method was inferred, and the edge color corresponds to the fraction of the runtime spent on runtime dispatch (black is 0%, bright red is 100%). Clicking on a dot prints the method (or location, if inlined) to the REPL, and sets methodref[] to that method.\n\nax is the pyplot axis of the scatterplot.\n\ncompat: Compat\npgdsgui depends on PyPlot via Julia extensions. You must load both SnoopCompile and PyPlot for this function to be defined.\n\n\n\n\n\n"},{"title":"SnoopCompile.uinvalidated","page":"Reference","location":"reference/#SnoopCompile.uinvalidated","category":"function","text":"umis = uinvalidated(invlist)\n\nReturn the unique invalidated MethodInstances. invlist is obtained from SnoopCompileCore.@snoop_invalidations. This is similar to filtering for MethodInstances in invlist, except that it discards any tagged \"invalidate_mt_cache\". These can typically be ignored because they are nearly inconsequential: they do not invalidate any compiled code, they only transiently affect an optimization of runtime dispatch.\n\n\n\n\n\n"},{"title":"SnoopCompile.invalidation_trees","page":"Reference","location":"reference/#SnoopCompile.invalidation_trees","category":"function","text":"trees = invalidation_trees(list; consolidate=true)\n\nParse list, as captured by SnoopCompileCore.@snoop_invalidations, into a set of invalidation trees, where parents nodes were called by their children.\n\nExample\n\njulia> f(x::Int)  = 1\nf (generic function with 1 method)\n\njulia> f(x::Bool) = 2\nf (generic function with 2 methods)\n\njulia> applyf(container) = f(container[1])\napplyf (generic function with 1 method)\n\njulia> callapplyf(container) = applyf(container)\ncallapplyf (generic function with 1 method)\n\njulia> c = Any[1]\n1-element Array{Any,1}:\n 1\n\njulia> callapplyf(c)\n1\n\njulia> trees = invalidation_trees(@snoop_invalidations f(::AbstractFloat) = 3)\n1-element Array{SnoopCompile.MethodInvalidations,1}:\n inserting f(::AbstractFloat) in Main at REPL[36]:1 invalidated:\n   mt_backedges: 1: signature Tuple{typeof(f),Any} triggered MethodInstance for applyf(::Array{Any,1}) (1 children) more specific\n\nThere are two sources of invalidation: method insertion/deletion (new code invalidating old code) and edge validation (package import validating against the existing session). By default, these two are combined into a single set of trees, but you can disable this by passing consolidate=false. One potential advantage of not consolidating is that edge-validation can bundle multiple methods together into a single invalidation tree, which might reduce the number of trees if a package creates many methods for a single function.\n\nFor more information, see the tutorials in the online documentation.\n\n\n\n\n\n"},{"title":"SnoopCompile.precompile_blockers","page":"Reference","location":"reference/#SnoopCompile.precompile_blockers","category":"function","text":"staletrees = precompile_blockers(invalidations, tinf::InferenceTimingNode)\n\nSelect just those invalidations that contribute to \"stale nodes\" in tinf, and link them together. This can allow one to identify specific blockers of precompilation for particular MethodInstances.\n\nExample\n\nusing SnoopCompileCore\ninvalidations = @snoop_invalidations using PkgA, PkgB;\nusing SnoopCompile\ntrees = invalidation_trees(invalidations)\ntinf = @snoop_inference begin\n    some_workload()\nend\nstaletrees = precompile_blockers(trees, tinf)\n\nIn many cases, this reduces the number of invalidations that require analysis by one or more orders of magnitude.\n\ninfo: Info\nprecompile_blockers is experimental and has not yet been thoroughly vetted by real-world use. Users are encouraged to try it and report any \"misses\" or unnecessary \"hits.\"\n\n\n\n\n\n"},{"title":"SnoopCompile.filtermod","page":"Reference","location":"reference/#SnoopCompile.filtermod","category":"function","text":"modtrigs = filtermod(mod::Module, mtrigs::AbstractVector{MethodTriggers})\n\nSelect just the method-based triggers arising from a particular module.\n\n\n\n\n\nthinned = filtermod(module, trees::AbstractVector{MethodInvalidations}; recursive=false)\n\nSelect just the cases of invalidating a method defined in module.\n\nIf recursive is false, only the roots of trees are examined (i.e., the proximal source of the invalidation must be in module). If recursive is true, then thinned contains all routes to a method in module.\n\n\n\n\n\n"},{"title":"SnoopCompile.findcaller","page":"Reference","location":"reference/#SnoopCompile.findcaller","category":"function","text":"methinvs = findcaller(method::Method, trees)\n\nFind a path through trees that reaches method. Returns a single MethodInvalidations object.\n\nExamples\n\nSuppose you know that loading package SomePkg triggers invalidation of f(data). You can find the specific source of invalidation as follows:\n\nf(data)                             # run once to force compilation\nm = @which f(data)\nusing SnoopCompile\ntrees = invalidation_trees(@snoop_invalidations using SomePkg)\nmethinvs = findcaller(m, trees)\n\nIf you don't know which method to look for, but know some operation that has had added latency, you can look for methods using @snoopi. For example, suppose that loading SomePkg makes the next using statement slow. You can find the source of trouble with\n\njulia> using SnoopCompile\n\njulia> trees = invalidation_trees(@snoop_invalidations using SomePkg);\n\njulia> tinf = @snoopi using SomePkg            # this second `using` will need to recompile code invalidated above\n1-element Array{Tuple{Float64,Core.MethodInstance},1}:\n (0.08518409729003906, MethodInstance for require(::Module, ::Symbol))\n\njulia> m = tinf[1][2].def\nrequire(into::Module, mod::Symbol) in Base at loading.jl:887\n\njulia> findcaller(m, trees)\ninserting ==(x, y::SomeType) in SomeOtherPkg at /path/to/code:100 invalidated:\n   backedges: 1: superseding ==(x, y) in Base at operators.jl:83 with MethodInstance for ==(::Symbol, ::Any) (16 children) more specific\n\n\n\n\n\n"},{"title":"SnoopCompile.report_invalidations","page":"Reference","location":"reference/#SnoopCompile.report_invalidations","category":"function","text":"report_invalidations(\n    io::IO = stdout;\n    invalidations,\n    n_rows::Int = 10,\n    process_filename::Function = x -> x,\n)\n\nPrint a tabular summary of invalidations given:\n\ninvalidations the output of SnoopCompileCore.@snoop_invalidations\n\nand (optionally)\n\nio::IO IO stream. Defaults to stdout\nn_rows::Int the number of rows to be displayed in the truncated table. A value of 0 indicates no truncation. A positive value will truncate the table to the specified number of rows.\nprocess_filename(::String)::String a function to post-process each filename, where invalidations are found\n\nExample usage\n\nimport SnoopCompileCore\ninvalidations = SnoopCompileCore.@snoop_invalidations begin\n\n    # load packages & define any additional methods\n\nend;\n\nusing SnoopCompile\nusing PrettyTables # to load report_invalidations\nreport_invalidations(;invalidations)\n\nUsing report_invalidations requires that you first load the PrettyTables.jl package.\n\n\n\n\n\n"},{"title":"SnoopCompile.flatten","page":"Reference","location":"reference/#SnoopCompile.flatten","category":"function","text":"flatten(tinf; tmin = 0.0, sortby=exclusive)\n\nFlatten the execution graph of InferenceTimingNodes returned from @snoop_inference into a Vector of InferenceTiming frames, each encoding the time needed for inference of a single MethodInstance. By default, results are sorted by exclusive time (the time for inferring the MethodInstance itself, not including any inference of its callees); other options are sortedby=inclusive which includes the time needed for the callees, or nothing to obtain them in the order they were inferred (depth-first order).\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.002148974/0.002767166 on Base.Compiler.Timings.ROOT() with 1 direct children\n\njulia> using AbstractTrees; print_tree(tinf)\nInferenceTimingNode: 0.00242354/0.00303526 on Base.Compiler.Timings.ROOT() with 1 direct children\n└─ InferenceTimingNode: 0.000150891/0.000611721 on SnoopCompile.FlattenDemo.packintype(::Int64) with 2 direct children\n   ├─ InferenceTimingNode: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64) with 0 direct children\n   └─ InferenceTimingNode: 9.43e-5/0.000355512 on SnoopCompile.FlattenDemo.dostuff(::SnoopCompile.FlattenDemo.MyType{Int64}) with 2 direct children\n      ├─ InferenceTimingNode: 6.6458e-5/0.000124716 on SnoopCompile.FlattenDemo.extract(::SnoopCompile.FlattenDemo.MyType{Int64}) with 2 direct children\n      │  ├─ InferenceTimingNode: 3.401e-5/3.401e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, ::Symbol) with 0 direct children\n      │  └─ InferenceTimingNode: 2.4248e-5/2.4248e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, x::Symbol) with 0 direct children\n      └─ InferenceTimingNode: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64) with 0 direct children\n\nNote the printing of getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, x::Symbol): it shows the specific Symbol, here :x, that getproperty was inferred with. This reflects constant-propagation in inference.\n\nThen:\n\njulia> flatten(tinf; sortby=nothing)\n8-element Vector{InferenceTiming}:\n InferenceTiming: 0.002423543/0.0030352639999999998 on Base.Compiler.Timings.ROOT()\n InferenceTiming: 0.000150891/0.0006117210000000001 on SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 9.43e-5/0.00035551200000000005 on SnoopCompile.FlattenDemo.dostuff(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 6.6458e-5/0.000124716 on SnoopCompile.FlattenDemo.extract(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 3.401e-5/3.401e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, ::Symbol)\n InferenceTiming: 2.4248e-5/2.4248e-5 on getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, x::Symbol)\n InferenceTiming: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64)\n\njulia> flatten(tinf; tmin=1e-4)                        # sorts by exclusive time (the time before the '/')\n4-element Vector{InferenceTiming}:\n InferenceTiming: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64)\n InferenceTiming: 0.000150891/0.0006117210000000001 on SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.002423543/0.0030352639999999998 on Base.Compiler.Timings.ROOT()\n\njulia> flatten(tinf; sortby=inclusive, tmin=1e-4)      # sorts by inclusive time (the time after the '/')\n6-element Vector{InferenceTiming}:\n InferenceTiming: 0.000105318/0.000105318 on SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 6.6458e-5/0.000124716 on SnoopCompile.FlattenDemo.extract(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 0.000136496/0.000136496 on SnoopCompile.FlattenDemo.domath(::Int64)\n InferenceTiming: 9.43e-5/0.00035551200000000005 on SnoopCompile.FlattenDemo.dostuff(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 0.000150891/0.0006117210000000001 on SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.002423543/0.0030352639999999998 on Base.Compiler.Timings.ROOT()\n\nAs you can see, sortby affects not just the order but also the selection of frames; with exclusive times, dostuff did not on its own rise above threshold, but it does when using inclusive times.\n\nSee also: accumulate_by_source.\n\n\n\n\n\n"},{"title":"SnoopCompileCore.exclusive","page":"Reference","location":"reference/#SnoopCompileCore.exclusive","category":"function","text":"exclusive(ci::InferenceTimingNode; include_llvm::Bool=true)\n\nReturn the time spent inferring ci, not including the time needed for any of its callees. If include_llvm is true, the LLVM compilation time is added.\n\n\n\n\n\n"},{"title":"SnoopCompileCore.inclusive","page":"Reference","location":"reference/#SnoopCompileCore.inclusive","category":"function","text":"inclusive(ci::InferenceTimingNode; include_llvm::Bool=true)\n\nReturn the time spent inferring ci and its callees. If include_llvm is true, the LLVM compilation time is added as well.\n\n\n\n\n\n"},{"title":"SnoopCompile.accumulate_by_source","page":"Reference","location":"reference/#SnoopCompile.accumulate_by_source","category":"function","text":"accumulate_by_source(flattened; tmin = 0.0, by=exclusive)\n\nAdd the inference timings for all MethodInstances of a single Method together. flattened is the output of flatten. Returns a list of (t, method) tuples.\n\nWhen the accumulated time for a Method is large, but each instance is small, it indicates that it is being inferred for many specializations (which might include specializations with different constants).\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.004978/0.005447 on Base.Compiler.Timings.ROOT() with 1 direct children\n\njulia> accumulate_by_source(flatten(tinf))\n7-element Vector{Tuple{Float64, Union{Method, Core.MethodInstance}}}:\n (4.6294999999999996e-5, getproperty(x, f::Symbol) @ Base Base.jl:37)\n (5.8965e-5, dostuff(y) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:45)\n (6.4141e-5, extract(y::SnoopCompile.FlattenDemo.MyType) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:36)\n (8.9997e-5, (var\"#ctor-self#\"::Type{SnoopCompile.FlattenDemo.MyType{T}} where T)(x) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:35)\n (9.2256e-5, domath(x) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:41)\n (0.000117514, packintype(x) @ SnoopCompile.FlattenDemo ~/.julia/dev/SnoopCompile/src/inference_demos.jl:37)\n (0.004977755, ROOT() @ Core.Compiler.Timings compiler/typeinfer.jl:79)\n\nCompared to the output from flatten, the two inferences passes on getproperty have been consolidated into a single aggregate call.\n\n\n\n\n\nmtrigs = accumulate_by_source(Method, itrigs::AbstractVector{InferenceTrigger})\n\nConsolidate inference triggers via their caller method. mtrigs is a vector of Method=>list pairs, where list is a list of InferenceTriggers.\n\n\n\n\n\nloctrigs = accumulate_by_source(itrigs::AbstractVector{InferenceTrigger})\n\nAggregate inference triggers by location (function, file, and line number) of the caller.\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_demo:\n\njulia> itrigs = inference_triggers(SnoopCompile.itrigs_demo())\n2-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:763)\n Inference triggered to call MethodInstance for double(::Float64) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:763)\n\njulia> accumulate_by_source(itrigs)\n1-element Vector{SnoopCompile.LocationTriggers}:\n    calldouble1 at /pathto/SnoopCompile/src/parcel_snoop_inference.jl:762 (2 callees from 1 callers)\n\n\n\n\n\n"},{"title":"SnoopCompile.collect_for","page":"Reference","location":"reference/#SnoopCompile.collect_for","category":"function","text":"list = collect_for(m::Method, tinf::InferenceTimingNode)\nlist = collect_for(m::MethodInstance, tinf::InferenceTimingNode)\n\nCollect all InferenceTimingNodes (descendants of tinf) that match m.\n\n\n\n\n\n"},{"title":"SnoopCompile.staleinstances","page":"Reference","location":"reference/#SnoopCompile.staleinstances","category":"function","text":"staleinstances(tinf::InferenceTimingNode)\n\nReturn a list of InferenceTimingNodes corresponding to MethodInstances that have \"stale\" code (specifically, CodeInstances with outdated max_world world ages). These may be a hint that invalidation occurred while running the workload provided to @snoop_inference, and consequently an important origin of (re)inference.\n\nwarning: Warning\nstaleinstances only looks retrospectively for stale code; it does not distinguish whether the code became stale while running @snoop_inference from whether it was already stale before execution commenced.\n\nWhile staleinstances is recommended as a useful \"sanity check\" to run before performing a detailed analysis of inference, any serious examination of invalidation should use @snoop_invalidations.\n\nFor more information about world age, see https://docs.julialang.org/en/v1/manual/methods/#Redefining-Methods.\n\n\n\n\n\n"},{"title":"SnoopCompile.inference_triggers","page":"Reference","location":"reference/#SnoopCompile.inference_triggers","category":"function","text":"itrigs = inference_triggers(tinf::InferenceTimingNode; exclude_toplevel=true)\n\nCollect the \"triggers\" of inference, each a fresh entry into inference via a call dispatched at runtime. All the entries in itrigs are previously uninferred, or are freshly-inferred for specific constant inputs.\n\nexclude_toplevel determines whether calls made from the REPL, include, or test suites are excluded.\n\nExample\n\nWe'll use SnoopCompile.itrigs_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.itrigs_demo()\nInferenceTimingNode: 0.004490576/0.004711168 on Base.Compiler.Timings.ROOT() with 2 direct children\n\njulia> itrigs = inference_triggers(tinf)\n2-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/inference_demos.jl:86) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/inference_demos.jl:87)\n Inference triggered to call MethodInstance for double(::Float64) from calldouble1 (/pathto/SnoopCompile/src/inference_demos.jl:86) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/inference_demos.jl:87)\n\njulia> edit(itrigs[1])     # opens an editor at the spot in the caller\n\njulia> using Cthulhu\n\njulia> ascend(itrigs[2])   # use Cthulhu to inspect the stacktrace (caller is the second item in the trace)\nChoose a call for analysis (q to quit):\n >   double(::Float64)\n       calldouble1 at /pathto/SnoopCompile/src/inference_demos.jl:86 => calldouble2(::Vector{Vector{Any}}) at /pathto/SnoopCompile/src/inference_demos.jl:87\n         calleach(::Vector{Vector{Vector{Any}}}) at /pathto/SnoopCompile/src/inference_demos.jl:88\n...\n\n\n\n\n\n"},{"title":"SnoopCompile.trigger_tree","page":"Reference","location":"reference/#SnoopCompile.trigger_tree","category":"function","text":"root = trigger_tree(itrigs)\n\nOrganize inference triggers itrigs in tree format, grouping items via the call tree.\n\nIt is a tree rather than a more general graph due to the fact that caching inference results means that each node gets visited only once.\n\n\n\n\n\n"},{"title":"SnoopCompile.suggest","page":"Reference","location":"reference/#SnoopCompile.suggest","category":"function","text":"suggest(itrig::InferenceTrigger)\n\nAnalyze itrig and attempt to suggest an interpretation or remedy. This returns a structure of type Suggested; the easiest thing to do with the result is to show it; however, you can also filter a list of suggestions.\n\nExample\n\njulia> itrigs = inference_triggers(tinf);\n\njulia> sugs = suggest.(itrigs);\n\njulia> sugs_important = filter(!isignorable, sugs)    # discard the ones that probably don't need to be addressed\n\nwarning: Warning\nSuggestions are approximate at best; most often, the proposed fixes should not be taken literally, but instead taken as a hint about the \"outcome\" of a particular runtime dispatch incident. The suggestions target calls made with non-inferrable argumets, but often the best place to fix the problem is at an earlier stage in the code, where the argument was first computed.You can get much deeper insight via ascend (and Cthulhu generally), and even stacktrace is often useful. Suggestions are intended to be a quick and easier-to-comprehend first pass at analyzing an inference trigger.\n\n\n\n\n\n"},{"title":"SnoopCompile.isignorable","page":"Reference","location":"reference/#SnoopCompile.isignorable","category":"function","text":"isignorable(s::Suggested)\n\nReturns true if s is unlikely to be an inference problem in need of fixing.\n\n\n\n\n\n"},{"title":"SnoopCompile.callerinstance","page":"Reference","location":"reference/#SnoopCompile.callerinstance","category":"function","text":"mi = callerinstance(itrig::InferenceTrigger)\n\nReturn the MethodInstance mi of the caller in the selected stackframe in itrig.\n\n\n\n\n\n"},{"title":"SnoopCompile.callingframe","page":"Reference","location":"reference/#SnoopCompile.callingframe","category":"function","text":"itrigcaller = callingframe(itrig::InferenceTrigger)\n\n\"Step out\" one layer of the stacktrace, referencing the caller of the current frame of itrig.\n\nYou can retrieve the proximal trigger of inference with InferenceTrigger(itrigcaller).\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_demo:\n\njulia> itrig = inference_triggers(SnoopCompile.itrigs_demo())[1]\nInference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:763)\n\njulia> itrigcaller = callingframe(itrig)\nInference triggered to call MethodInstance for double(::UInt8) from calleach (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:764) with specialization MethodInstance for calleach(::Vector{Vector{Vector{Any}}})\n\n\n\n\n\n"},{"title":"SnoopCompile.skiphigherorder","page":"Reference","location":"reference/#SnoopCompile.skiphigherorder","category":"function","text":"itrignew = skiphigherorder(itrig; exact::Bool=false)\n\nAttempt to skip over frames of higher-order functions that take the callee as a function-argument. This can be useful if you're analyzing inference triggers for an entire package and would prefer to assign triggers to package-code rather than Base functions like map!, broadcast, etc.\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_higherorder_demo:\n\njulia> itrig = inference_triggers(SnoopCompile.itrigs_higherorder_demo())[1]\nInference triggered to call MethodInstance for double(::Float64) from mymap! (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:706) with specialization MethodInstance for mymap!(::typeof(SnoopCompile.ItrigHigherOrderDemo.double), ::Vector{Any}, ::Vector{Any})\n\njulia> callingframe(itrig)      # step out one (non-inlined) frame\nInference triggered to call MethodInstance for double(::Float64) from mymap (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:710) with specialization MethodInstance for mymap(::typeof(SnoopCompile.ItrigHigherOrderDemo.double), ::Vector{Any})\n\njulia> skiphigherorder(itrig)   # step out to frame that doesn't have `double` as a function-argument\nInference triggered to call MethodInstance for double(::Float64) from callmymap (/pathto/SnoopCompile/src/parcel_snoop_inference.jl:711) with specialization MethodInstance for callmymap(::Vector{Any})\n\nwarn: Warn\nBy default skiphigherorder is conservative, and insists on being sure that it's the callee being passed to the higher-order function. Higher-order functions that do not get specialized (e.g., with ::Function argument types) will not be skipped over. You can pass exact=false to allow ::Function to also be passed over, but keep in mind that this may falsely skip some frames.\n\n\n\n\n\n"},{"title":"SnoopCompile.InferenceTrigger","page":"Reference","location":"reference/#SnoopCompile.InferenceTrigger","category":"type","text":"InferenceTrigger(callee::MethodInstance, callerframes::Vector{StackFrame}, btidx::Int, bt)\n\nOrganize information about the \"triggers\" of inference. callee is the MethodInstance requiring inference, callerframes, btidx and bt contain information about the caller. callerframes are the frame(s) of call site that triggered inference; it's a Vector{StackFrame}, rather than a single StackFrame, due to the possibility that the caller was inlined into something else, in which case the first entry is the direct caller and the last entry corresponds to the MethodInstance into which it was ultimately inlined. btidx is the index in bt, the backtrace collected upon entry into inference, corresponding to callerframes.\n\nInferenceTriggers are created by calling inference_triggers. See also: callerinstance and callingframe.\n\n\n\n\n\n"},{"title":"SnoopCompile.runtime_inferencetime","page":"Reference","location":"reference/#SnoopCompile.runtime_inferencetime","category":"function","text":"ridata = runtime_inferencetime(tinf::InferenceTimingNode; consts=true, by=inclusive)\nridata = runtime_inferencetime(tinf::InferenceTimingNode, profiledata; lidict, consts=true, by=inclusive)\n\nCompare runtime and inference-time on a per-method basis. ridata[m::Method] returns (trun, tinfer, nspecializations), measuring the approximate amount of time spent running m, inferring m, and the number of type-specializations, respectively. trun is estimated from profiling data, which the user is responsible for capturing before the call. Typically tinf is collected via @snoop_inference on the first call (in a fresh session) to a workload, and the profiling data collected on a subsequent call. In some cases you may need to repeat the workload several times to collect enough profiling samples.\n\nprofiledata and lidict are obtained from Profile.retrieve().\n\n\n\n\n\n"},{"title":"SnoopCompile.parcel","page":"Reference","location":"reference/#SnoopCompile.parcel","category":"function","text":"ttot, pcs = SnoopCompile.parcel(tinf::InferenceTimingNode)\n\nParcel the \"root-most\" precompilable MethodInstances into separate modules. These can be used to generate precompile directives to cache the results of type-inference, reducing latency on first use.\n\nLoosely speaking, and MethodInstance is precompilable if the module that owns the method also has access to all the types it need to precompile the instance. When the root node of an entrance to inference is not itself precompilable, parcel examines the children (and possibly, children's children...) until it finds the first node on each branch that is precompilable. MethodInstances are then assigned to the module that owns the method.\n\nttot is the total inference time; pcs is a list of module => (tmod, pclist) pairs. For each module, tmod is the amount of inference time affiliated with methods owned by that module; pclist is a list of (t, mi) time/MethodInstance tuples.\n\nSee also: SnoopCompile.write.\n\nExample\n\nWe'll use SnoopCompile.itrigs_demo, which runs @snoop_inference on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.itrigs_demo()\nInferenceTimingNode: 0.004490576/0.004711168 on Base.Compiler.Timings.ROOT() with 2 direct children\n\njulia> ttot, pcs = SnoopCompile.parcel(tinf);\n\njulia> ttot\n0.000220592\n\njulia> pcs\n1-element Vector{Pair{Module, Tuple{Float64, Vector{Tuple{Float64, Core.MethodInstance}}}}}:\n SnoopCompile.ItrigDemo => (0.000220592, [(9.8986e-5, MethodInstance for double(::Float64)), (0.000121606, MethodInstance for double(::UInt8))])\n\nSince there was only one module, ttot is the same as tmod. The ItrigDemo module had two precomilable MethodInstances, each listed with its corresponding inclusive time.\n\n\n\n\n\nmodtrigs = SnoopCompile.parcel(mtrigs::AbstractVector{MethodTriggers})\n\nSplit method-based triggers into collections organized by the module in which the methods were defined. Returns a module => list vector, with the module having the most MethodTriggers last.\n\n\n\n\n\n"},{"title":"SnoopCompile.write","page":"Reference","location":"reference/#SnoopCompile.write","category":"function","text":"write(prefix::AbstractString, pc; always::Bool=false, suppress_time::Bool=false)\n\nWrite each modules' precompiles to a separate file.  If always is true, the generated function will always run the precompile statements when called, otherwise the statements will only be called during package precompilation.\n\nWhen writing results from parceling @snoop_inference, by default SnoopCompile appends the time taken to precompile each statement to the generated file.  If suppress_time is true, this information will be omitted.\n\ncompat: SnoopCompile 3.1\nThe suppress_time keyword argument was added in SnoopCompile 3.1.\n\n\n\n\n\n"},{"title":"SnoopCompile.isprecompilable","page":"Reference","location":"reference/#SnoopCompile.isprecompilable","category":"function","text":"isprecompilable(mod::Module, mi::MethodInstance)\nisprecompilable(mi::MethodInstance; excluded_modules=Set([Main::Module]))\n\nDetermine whether mi is able to be precompiled within mod. This requires that all the types in mi's specialization signature are \"known\" to mod. See SnoopCompile.known_type for more information.\n\nisprecompilable(mi) sets mod to the module in which the corresponding method was defined. If mod ∈ excluded_modules, then isprecompilable returns false.\n\nIf mi has been compiled by the time its defining module \"closes\" (the final end of the module definition) and isprecompilable(mi) returns true, then Julia will automatically include this specialization in that module's precompile cache.\n\n!!! tip If mi is a MethodInstance corresponding to f(::T), then calling     f(x::T) before the end of the module definition suffices to force     compilation of mi. Alternatively, use precompile(f, (T,)).\n\nIf you'd like to cache it but isprecompilable(mi) returns false, you need to identify a module mod for which isprecompilable(mod, mi) returns true. However, just ensuring that mi gets compiled within mod may not be sufficient to ensure that it gets retained in the cache: by default, Julia will omit it from the cache if none of the types are \"owned\" by that module. (For example, if mod didn't define the method, and all the types in mi's signature come from other modules imported by mod, then mod does not \"own\" any aspect of mi.) To force it to be retained, ensure it gets called (for the first time) within a PrecompileTools.@compile_workload block. (This is the main purpose of PrecompileTools.)\n\nExamples\n\njulia> module A\n       a(x) = x\n       end\nA\n\njulia> module B\n       using ..A\n       struct BType end    # this type is not known to A\n       b(x) = x\n       end\nB\n\nNow let's run these methods to generate some compiled MethodInstances:\n\njulia> A.a(3.2)          # Float64 is not \"owned\" by A, but A loads Base so A knows about it\n3.2\n\njulia> A.a(B.BType())    # B.BType is not known to A\nB.BType()\n\njulia> B.b(B.BType())    # B knows about B.BType\nB.BType()\n\njulia> mia1, mia2 = Base.specializations(only(methods(A.a)));\n\njulia> @show mia1 SnoopCompile.isprecompilable(mia1);\nmia1 = MethodInstance for A.a(::Float64)\nSnoopCompile.isprecompilable(mia1) = true\n\njulia> @show mia2 SnoopCompile.isprecompilable(mia2);\nmia2 = MethodInstance for A.a(::B.BType)\nSnoopCompile.isprecompilable(mia2) = false\n\njulia> mib = only(Base.specializations(only(methods(B.b))))\nMethodInstance for B.b(::B.BType)\n\njulia> SnoopCompile.isprecompilable(mib)\ntrue\n\njulia> SnoopCompile.isprecompilable(A, mib)\nfalse\n\n\n\n\n\n"},{"title":"SnoopCompile.known_type","page":"Reference","location":"reference/#SnoopCompile.known_type","category":"function","text":"known_type(mod::Module, T::Union{Type,TypeVar})\n\nReturns true if the type T is \"known\" to the module mod, meaning that one could have written a function with signature f(x::T) in mod without getting an error.\n\n\n\n\n\n"},{"title":"SnoopCompile.report_callee","page":"Reference","location":"reference/#SnoopCompile.report_callee","category":"function","text":"To use report_callee do using JET\n\n\n\n\n\n"},{"title":"SnoopCompile.report_callees","page":"Reference","location":"reference/#SnoopCompile.report_callees","category":"function","text":"To use report_callees do using JET\n\n\n\n\n\n"},{"title":"SnoopCompile.report_caller","page":"Reference","location":"reference/#SnoopCompile.report_caller","category":"function","text":"To use report_caller do using JET\n\n\n\n\n\n"},{"title":"SnoopCompile.read_snoop_llvm","page":"Reference","location":"reference/#SnoopCompile.read_snoop_llvm","category":"function","text":"times, info = SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\"; tmin_secs=0.0)\n\nReads the log file produced by the compiler and returns the structured representations.\n\nThe results will only contain modules that took longer than tmin_secs to optimize.\n\nReturn value\n\ntimes contains the time spent optimizing each module, as a Pair from the time to an\n\narray of Strings, one for every MethodInstance in that llvm module.\n\ninfo is a Dict containing statistics for each MethodInstance encountered, from before\n\nand after optimization, including number of instructions and number of basicblocks.\n\nExample\n\njulia> @snoop_llvm \"func_names.csv\" \"llvm_timings.yaml\" begin\n           using InteractiveUtils\n           @eval InteractiveUtils.peakflops()\n       end\nLaunching new julia process to run commands...\ndone.\n\njulia> times, info = SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\", tmin_secs = 0.025);\n\njulia> times\n3-element Vector{Pair{Float64, Vector{String}}}:\n 0.028170923 => [\"Tuple{typeof(LinearAlgebra.copy_transpose!), Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}, Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}}\"]\n 0.031356962 => [\"Tuple{typeof(Base.copyto!), Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}, Array{Float64, 2}, Base.UnitRange{Int64}, Base.UnitRange{Int64}}\"]\n 0.149138788 => [\"Tuple{typeof(LinearAlgebra._generic_matmatmul!), Array{Float64, 2}, Char, Char, Array{Float64, 2}, Array{Float64, 2}, LinearAlgebra.MulAddMul{true, true, Bool, Bool}}\"]\n\njulia> info\nDict{String, NamedTuple{(:before, :after), Tuple{NamedTuple{(:instructions, :basicblocks), Tuple{Int64, Int64}}, NamedTuple{(:instructions, :basicblocks), Tuple{Int64, Int64}}}}} with 3 entries:\n  \"Tuple{typeof(LinearAlgebra.copy_transpose!), Ar… => (before = (instructions = 651, basicblocks = 83), after = (instructions = 348, basicblocks = 40…\n  \"Tuple{typeof(Base.copyto!), Array{Float64, 2}, … => (before = (instructions = 617, basicblocks = 77), after = (instructions = 397, basicblocks = 37…\n  \"Tuple{typeof(LinearAlgebra._generic_matmatmul!)… => (before = (instructions = 4796, basicblocks = 824), after = (instructions = 1421, basicblocks =…\n\n\n\n\n\n"},{"title":"SnoopCompile.flatten_demo","page":"Reference","location":"reference/#SnoopCompile.flatten_demo","category":"function","text":"tinf = SnoopCompile.flatten_demo()\n\nA simple demonstration of @snoop_inference. This demo defines a module\n\nmodule FlattenDemo\n    struct MyType{T} x::T end\n    extract(y::MyType) = y.x\n    function packintype(x)\n        y = MyType{Int}(x)\n        return dostuff(y)\n    end\n    function domath(x)\n        y = x + x\n        return y*x + 2*x + 5\n    end\n    dostuff(y) = domath(extract(y))\nend\n\nIt then \"warms up\" (forces inference on) all of Julia's Base methods needed for domath, to ensure that these MethodInstances do not need to be inferred when we collect the data. It then returns the results of\n\n@snoop_inference FlattenDemo.packintypes(1)\n\nSee flatten for an example usage.\n\n\n\n\n\n"},{"title":"SnoopCompile.itrigs_demo","page":"Reference","location":"reference/#SnoopCompile.itrigs_demo","category":"function","text":"tinf = SnoopCompile.itrigs_demo()\n\nA simple demonstration of collecting inference triggers. This demo defines a module\n\nmodule ItrigDemo\n@noinline double(x) = 2x\n@inline calldouble1(c) = double(c[1])\ncalldouble2(cc) = calldouble1(cc[1])\ncalleach(ccs) = (calldouble2(ccs[1]), calldouble2(ccs[2]))\nend\n\nIt then \"warms up\" (forces inference on) calldouble2(::Vector{Vector{Any}}), calldouble1(::Vector{Any}), double(::Int):\n\ncc = [Any[1]]\nItrigDemo.calleach([cc,cc])\n\nThen it collects and returns inference data using\n\ncc1, cc2 = [Any[0x01]], [Any[1.0]]\n@snoop_inference ItrigDemo.calleach([cc1, cc2])\n\nThis does not require any new inference for calldouble2 or calldouble1, but it does force inference on double with two new types. See inference_triggers to see what gets collected and returned.\n\n\n\n\n\n"},{"title":"SnoopCompile.itrigs_higherorder_demo","page":"Reference","location":"reference/#SnoopCompile.itrigs_higherorder_demo","category":"function","text":"tinf = SnoopCompile.itrigs_higherorder_demo()\n\nA simple demonstration of handling higher-order methods with inference triggers. This demo defines a module\n\nmodule ItrigHigherOrderDemo\ndouble(x) = 2x\n@noinline function mymap!(f, dst, src)\n    for i in eachindex(dst, src)\n        dst[i] = f(src[i])\n    end\n    return dst\nend\n@noinline mymap(f::F, src) where F = mymap!(f, Vector{Any}(undef, length(src)), src)\ncallmymap(src) = mymap(double, src)\nend\n\nThe key feature of this set of definitions is that the function double gets passed as an argument through mymap and mymap! (the latter are higher-order functions).\n\nIt then \"warms up\" (forces inference on) callmymap(::Vector{Any}), mymap(::typeof(double), ::Vector{Any}), mymap!(::typeof(double), ::Vector{Any}, ::Vector{Any}) and double(::Int):\n\nItrigHigherOrderDemo.callmymap(Any[1, 2])\n\nThen it collects and returns inference data using\n\n@snoop_inference ItrigHigherOrderDemo.callmymap(Any[1.0, 2.0])\n\nwhich forces inference for double(::Float64).\n\nSee skiphigherorder for an example using this demo.\n\n\n\n\n\n"},{"title":"Techniques for fixing inference problems","page":"Techniques for fixing inference problems","location":"explanations/fixing_inference/#Techniques-for-fixing-inference-problems","category":"section","text":"Here we assume you've dug into your code with a tool like Cthulhu, and want to know how to fix some of the problems that you discover. Below is a collection of specific cases and some tricks for handling them.\n\nNote that there is also a tutorial on fixing inference that delves into advanced topics."},{"title":"Adding type annotations","page":"Techniques for fixing inference problems","location":"explanations/fixing_inference/#Adding-type-annotations","category":"section","text":""},{"title":"Using concrete types","page":"Techniques for fixing inference problems","location":"explanations/fixing_inference/#Using-concrete-types","category":"section","text":"Defining variables like list = [] can be convenient, but it creates a list of type Vector{Any}. This prevents inference from knowing the type of items extracted from list. Using list = String[] for a container of strings, etc., is an excellent fix. When in doubt, check the type with isconcretetype: a common mistake is to think that list_of_lists = Array{Int}[] gives you a vector-of-vectors, but\n\njulia> isconcretetype(Array{Int})\nfalse\n\nreminds you that Array requires a second parameter indicating the dimensionality of the array. (Or use list_of_lists = Vector{Int}[] instead, as Vector{Int} === Array{Int, 1}.)\n\nMany valuable tips can be found among Julia's performance tips, and readers are encouraged to consult that page."},{"title":"Working with non-concrete types","page":"Techniques for fixing inference problems","location":"explanations/fixing_inference/#Working-with-non-concrete-types","category":"section","text":"In cases where invalidations occur, but you can't use concrete types (there are indeed many valid uses of Vector{Any}), you can often prevent the invalidation using some additional knowledge. One common example is extracting information from an IOContext structure, which is roughly defined as\n\nstruct IOContext{IO_t <: IO} <: AbstractPipe\n    io::IO_t\n    dict::ImmutableDict{Symbol, Any}\nend\n\nThere are good reasons that dict uses a value-type of Any, but that makes it impossible for the compiler to infer the type of any object looked up in an IOContext. Fortunately, you can help! For example, the documentation specifies that the :color setting should be a Bool, and since it appears in documentation it's something we can safely enforce. Changing\n\niscolor = get(io, :color, false)\n\nto\n\niscolor = get(io, :color, false)::Bool     # assert that the rhs is Bool-valued\n\nwill throw an error if it isn't a Bool, and this allows the compiler to take advantage of the type being known in subsequent operations.\n\nIf the return type is one of a small number of possibilities (generally three or fewer), you can annotate the return type with Union{...}. This is generally advantageous only when the intersection of what inference already knows about the types of a variable and the types in the Union results in an concrete type.\n\nAs a more detailed example, suppose you're writing code that parses Julia's Expr type:\n\njulia> ex = :(Array{Float32,3})\n:(Array{Float32, 3})\n\njulia> dump(ex)\nExpr\n  head: Symbol curly\n  args: Vector{Any(3,))\n    1: Symbol Array\n    2: Symbol Float32\n    3: Int64 3\n\nex.args is a Vector{Any}. However, for a :curly expression only certain types will be found among the arguments; you could write key portions of your code as\n\na = ex.args[2]\nif a isa Symbol\n    # inside this block, Julia knows `a` is a Symbol, and so methods called on `a` will be resistant to invalidation\n    foo(a)\nelseif a isa Expr && length((a::Expr).args) > 2\n    a::Expr         # sometimes you have to help inference by adding a type-assert\n    x = bar(a)      # `bar` is now resistant to invalidation\nelseif a isa Integer\n    # even though you've not made this fully-inferrable, you've at least reduced the scope for invalidations\n    # by limiting the subset of `foobar` methods that might be called\n    y = foobar(a)\nend\n\nOther tricks include replacing broadcasting on v::Vector{Any} with Base.mapany(f, v)–mapany avoids trying to narrow the type of f(v[i]) and just assumes it will be Any, thereby avoiding invalidations of many convert methods.\n\nAdding type-assertions and fixing inference problems are the most common approaches for fixing invalidations. You can discover these manually, but using Cthulhu is highly recommended."},{"title":"Inferrable field access for abstract types","page":"Techniques for fixing inference problems","location":"explanations/fixing_inference/#Inferrable-field-access-for-abstract-types","category":"section","text":"When invalidations happen for methods that manipulate fields of abstract types, often there is a simple solution: create an \"interface\" for the abstract type specifying that certain fields must have certain types. Here's an example:\n\nabstract type AbstractDisplay end\n\nstruct Monitor <: AbstractDisplay\n    height::Int\n    width::Int\n    maker::String\nend\n\nstruct Phone <: AbstractDisplay\n    height::Int\n    width::Int\n    maker::Symbol\nend\n\nfunction Base.show(@nospecialize(d::AbstractDisplay), x)\n    str = string(x)\n    w = d.width\n    if length(str) > w  # do we have to truncate to fit the display width?\n        ...\n\nIn this show method, we've deliberately chosen to prevent specialization on the specific type of AbstractDisplay (to reduce the total number of times we have to compile this method). As a consequence, Julia's inference may not realize that d.width returns an Int.\n\nFortunately, you can help by defining an interface for generic AbstractDisplay objects:\n\nfunction Base.getproperty(d::AbstractDisplay, name::Symbol)\n    if name === :height\n        return getfield(d, :height)::Int\n    elseif name === :width\n        return getfield(d, :width)::Int\n    elseif name === :maker\n        return getfield(d, :maker)::Union{String,Symbol}\n    end\n    return getfield(d, name)\nend\n\nJulia's constant propagation will ensure that most accesses of those fields will be determined at compile-time, so this simple change robustly fixes many inference problems."},{"title":"Fixing Core.Box","page":"Techniques for fixing inference problems","location":"explanations/fixing_inference/#Fixing-Core.Box","category":"section","text":"Julia issue 15276 is one of the more surprising forms of inference failure; it is the most common cause of a Core.Box annotation. If other variables depend on the Boxed variable, then a single Core.Box can lead to widespread inference problems. For this reason, these are also among the first inference problems you should tackle.\n\nRead this explanation of why this happens and what you can do to fix it. If you are directed to find Core.Box inference triggers via suggest, you may need to explore around the call site a bit– the inference trigger may be in the closure itself, but the fix needs to go in the method that creates the closure.\n\nUse of ascend is highly recommended for fixing Core.Box inference failures."},{"title":"Handling edge cases","page":"Techniques for fixing inference problems","location":"explanations/fixing_inference/#Handling-edge-cases","category":"section","text":"You can sometimes get invalidations from failing to handle \"formal\" possibilities. For example, operations with regular expressions might return a Union{Nothing, RegexMatch}. You can sometimes get poor type inference by writing code that fails to take account of the possibility that nothing might be returned. For example, a comprehension\n\nms = [m.match for m in match.((rex,), my_strings)]\n\nmight be replaced with\n\nms = [m.match for m in match.((rex,), my_strings) if m !== nothing]\n\nand return a better-typed result."},{"title":"Tutorial on @snoop_llvm","page":"Tutorial on @snoop_llvm","location":"tutorials/snoop_llvm/#Tutorial-on-@snoop_llvm","category":"section","text":"Julia uses the LLVM compiler to generate machine code. Typically, the two main contributors to the overall compile time are inference and LLVM, and thus together @snoop_inference and @snoop_llvm collect fairly comprehensive data on the compiler.\n\n@snoop_llvm has a somewhat different design than @snoop_inference: while @snoop_inference runs in the same session that you'll be using for analysis (and thus requires that you remember to do the data gathering in a fresh session), @snoop_llvm spawns a fresh process to collect the data. The downside is that you get less interactivity, as the data have to be written out in intermediate forms as a text file."},{"title":"Add SnoopCompileCore and SnoopCompile to your environment","page":"Tutorial on @snoop_llvm","location":"tutorials/snoop_llvm/#Add-SnoopCompileCore-and-SnoopCompile-to-your-environment","category":"section","text":"Here, we'll add these packages to your default environment.\n\nusing Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\"]);"},{"title":"Collecting the data","page":"Tutorial on @snoop_llvm","location":"tutorials/snoop_llvm/#Collecting-the-data","category":"section","text":"Here's a simple demonstration of usage:\n\nusing SnoopCompileCore\n@snoop_llvm \"func_names.csv\" \"llvm_timings.yaml\" begin\n    using InteractiveUtils\n    @eval InteractiveUtils.peakflops()\nend\n\nusing SnoopCompile\ntimes, info = SnoopCompile.read_snoop_llvm(\"func_names.csv\", \"llvm_timings.yaml\", tmin_secs = 0.005);\n\nThis will write two files, \"func_names.csv\" and \"llvm_timings.yaml\", in your current working directory. Let's look at what was read from these files:\n\ntimes\ninfo"},{"title":"Understanding SnoopCompile and Julia's compilation pipeline","page":"Understanding SnoopCompile and Julia's compilation pipeline","location":"explanations/basic/#Understanding-SnoopCompile-and-Julia's-compilation-pipeline","category":"section","text":"Julia uses Just-in-time (JIT) compilation to generate the code that runs on your CPU. Broadly speaking, there are two major compilation steps: inference and code generation. Inference is the process of determining the type of each object, which in turn determines which specific methods get called; once type inference is complete, code generation performs optimizations and ultimately generates the assembly language (native code) used on CPUs. Some aspects of this process are documented here.\n\nUsing code that has never been compiled requires that it first be JIT-compiled, and this contributes to the latency of using the package. In some circumstances, you can cache (store) the results of compilation to files to reduce the latency when your package is used. These files are the the *.ji and *.so files that live in the compiled directory of your Julia depot, usually located at ~/.julia/compiled. However, if these files become large, loading them can be another source for latency. Julia needs time both to load and validate the cached compiled code. Minimizing the latency of using a package involves focusing on caching the compilation of code that is both commonly used and takes time to compile.\n\nCaching code for later use is called precompilation. Julia has had some forms of precompilation almost since the very first packages. However, it was Julia 1.9 that first supported \"complete\" precompilation, including the ability to store native code in shared-library cache files.\n\nSnoopCompile is designed to try to allow you to analyze the costs of JIT-compilation, identify key bottlenecks that contribute to latency, and set up precompile directives to see whether it produces measurable benefits."},{"title":"Package precompilation","page":"Understanding SnoopCompile and Julia's compilation pipeline","location":"explanations/basic/#Package-precompilation","category":"section","text":"When a package is precompiled, here's what happens under the hood:\n\nJulia loads all of the package's dependencies (the ones in the [deps] section of the Project.toml file), typically from precompile cache files\nJulia evaluates the source code (text files) that define the package module(s). Evaluating function foo(args...) ... end creates a new method foo. Note that:\nthe source code might also contain statements that create \"data\" (e.g., consts). In some cases this can lead to some subtle precompilation \"gotchas\"\nthe source code might also contain a precompile workload, which forces compilation and tracking of package methods.\nJulia iterates over the module contents and writes the result to disk. Note that the module contents might include compiled code, and if so it is written along with everything else to the cache file.\n\nWhen Julia loads your package, it just loads the \"snapshot\" stored in the cache file: it does not re-evaluate the source-text files that defined your package! It is appropriate to think of the source files of your package as \"build scripts\" that create your module; once the \"build scripts\" are executed, it's the module itself that gets cached, and the job of the build scripts is done."},{"title":"Invalidation classes","page":"Invalidation classes","location":"explanations/invalidation_classes/#Invalidation-classes","category":"section","text":"invalidation_trees returns two broad classes of invalidated targets in backedges and mt_backedges. To understand the difference, let's introduce a new term: we say that a callee method covers a call if it can accept all possible types used in that call. Consider this example:\n\nf(x::Integer) = false\ng1(x::Signed) = f(x)    # `f(::Integer)` always covers this call\ng2(x::Number) = f(x)    # `f(::Integer)` may not cover this call\n\ng1 will only ever be called for Signed inputs, and because Signed <: Integer, the method of f fully covers the call in g1. In contrast, g2 can be called for any Number type, and since Number is not a subtype of Integer, f may not cover the entire call.\n\nWith this understanding, the difference is straightforward: backedges-class invalidations are when there is exactly one applicable method and it fully covers the call. mt_backedges-class invalidations are for anything else. In such cases, Julia may need to scan the method table (the mt in mt_backedges) of the function in order to determine which method, if any, might be applicable.\n\nThis helps explain why mt_backedges invalidations are more likely to arise from poor inference: poor inference \"widens\" the argument types and thus makes it more likely that a call is unlikely to be covered by exactly one method. It's still possible to get a backedges-class invalidation from poor inference:\n\ng3(x::Ref{Any}) = f(x[]::Signed)\n\nguarantees that our method of f covers the call, even though we can't predict with precision what type x[] will return. Thus if you invalidate the compiled code of g3 by defining a new method for f(x::Signed), you'll get a backedges-class invalidation."},{"title":"Using @snoop_inference to emit manual precompile directives","page":"Using @snoop_inference to emit manual precompile directives","location":"tutorials/snoop_inference_parcel/#precompilation","category":"section","text":"In a few cases, it may be inconvenient or impossible to precompile using a workload. Some examples might be:\n\nan application that opens graphical windows\nan application that connects to a database\nan application that creates, deletes, or rewrites files on disk\n\nIn such cases, one alternative is to create a manual list of precompile directives using Julia's precompile(f, argtypes) function.\n\nwarning: Warning\nManual precompile directives are much more likely to \"go stale\" as the package is developed—precompile does not throw an error if a method for the given argtypes cannot be found. They are also more likely to be dependent on the Julia version, operating system, or CPU architecture. Whenever possible, it's safer to use a workload.\n\nprecompile directives have to be emitted by the module that owns the method and/or types. SnoopCompile comes with a tool, parcel, that splits out the \"root-most\" precompilable MethodInstances into their constituent modules. This will typically correspond to the bottom row of boxes in the flame graph. In cases where you have some that are not naively precompilable (see SnoopCompile.isprecompilable), they may include MethodInstances from higher up in the call tree.\n\nLet's use SnoopCompile.parcel on our OptimizeMe demo:\n\nusing SnoopCompileCore, SnoopCompile # here we need the SnoopCompile path for the next line (normally you should wait until after data collection is complete)\ninclude(joinpath(pkgdir(SnoopCompile), \"examples\", \"OptimizeMe.jl\"))\ntinf = @snoop_inference OptimizeMe.main();\nttot, pcs = SnoopCompile.parcel(tinf);\nttot\npcs\n\nttot shows the total amount of time spent on type-inference. parcel discovered precompilable MethodInstances for four modules, Core, Base.Multimedia, Base, and OptimizeMe that might benefit from precompile directives. These are listed in increasing order of inference time.\n\nLet's look specifically at OptimizeMeFixed, since that's under our control:\n\npcmod = pcs[end]\ntmod, tpcs = pcmod.second;\ntmod\ntpcs\n\nThis indicates the amount of time spent specifically on OptimizeMe, plus the list of calls that could be precompiled in that module.\n\nWe could look at the other modules (packages) similarly."},{"title":"SnoopCompile.write","page":"Using @snoop_inference to emit manual precompile directives","location":"tutorials/snoop_inference_parcel/#SnoopCompile.write","category":"section","text":"You can generate files that contain ready-to-use precompile directives using SnoopCompile.write:\n\nSnoopCompile.write(\"/tmp/precompiles_OptimizeMe\", pcs)\n\nYou'll now find a directory /tmp/precompiles_OptimizeMe, and inside you'll find files for modules that could have precompile directives added manually. The contents of the last of these should be recognizable:\n\nfunction _precompile_()\n    ccall(:jl_generating_output, Cint, ()) == 1 || return nothing\n    Base.precompile(Tuple{typeof(main)})   # time: 0.4204474\nend\n\nThe first ccall line ensures we only pay the cost of running these precompile directives if we're building the package; this is relevant mostly if you're running Julia with --compiled-modules=no, which can be a convenient way to disable precompilation and examine packages in their \"native state.\" (It would also matter if you've set __precompile__(false) at the top of your module, but if so why are you reading this?)\n\nThis file is ready to be moved into the OptimizeMe repository and included into your module definition.\n\nYou might also consider submitting some of the other files (or their precompile directives) to the packages you depend on."},{"title":"Information for SnoopCompile developers","page":"Information for SnoopCompile developers","location":"explanations/devs/#Information-for-SnoopCompile-developers","category":"section","text":""},{"title":"Invalidations","page":"Information for SnoopCompile developers","location":"explanations/devs/#Invalidations","category":"section","text":""},{"title":"Capturing invalidation logs","page":"Information for SnoopCompile developers","location":"explanations/devs/#Capturing-invalidation-logs","category":"section","text":"Julia itself handles (in)validation when you define (or delete) methods and load packages. Julia's internal machinery provides the option of recording these invalidation decisions to a log, which is just a Vector{Any}. Currently (as of Julia 1.12) there are two independent logs:\n\nfor method insertion and deletion (i.e., new methods invalidating old code), logging is handled in Julia's src/gf.c. You enable it with logmeths = ccall(:jl_debug_method_invalidation, Any, (Cint,), true) and pass a final argument of false to turn it off.\nfor validating precompiled code during package loading (i.e., \"new\" code being invalidated by old methods), logging is handled in Julia's base/staticdata.jl. You enable it with logedges = SnoopCompile.ReinferUtils.debug_method_invalidation(true) and pass false to turn it off.\n\nIn both cases, the log will initially be empty, but subsequent activity (defining or deleting methods, or loading packages) may add entries.\n\nSnoopCompileCore's @snoop_invalidation just turns on these logging streams, executes the user's block of code, turns off logging, and returns the captured log streams."},{"title":"Interpreting invalidation logs","page":"Information for SnoopCompile developers","location":"explanations/devs/#Interpreting-invalidation-logs","category":"section","text":"The definitive source for interpreting these two logging streams is Julia's own code; the documentation below may be outdated by future changes in Julia. (Such changes have happened repeatedly over the course of Julia's development.) If you have even a shred of doubt about whether any of this is (still) correct, check Julia's code.\n\nFor both logging streams, a single decision typically results in appending multiple entries to the log. These decisions come with a string (the tag) documenting the origin of each entry. In general, each distinct mechanism by which invalidations can occur should have its own unique tag. Often these correspond to specific lines in the source code."},{"title":"method logs","page":"Information for SnoopCompile developers","location":"explanations/devs/#method-logs","category":"section","text":"Let trigger::Method indicate an added or deleted method for function f. If defining/deleting this method would change how one or more caller::MethodInstances of the corresponding function would dispatch, those callers must be invalidated. Such events can result in a cascade of invalidations of code that directly or indirectly called trigger or less-specific methods of the same function. The order in which these invalidations appear in the log stream is as follows:\n\nBackedges of callee below, encoded as a tree where links are specified as (caller::MethodInstance, depth::Int32) pairs.\n\ndepth=1 typically corresponds to an inferrable caller. depth=0 corresponds to a potentially-missing callee (at the time of compilation), and will be followed by calleesig::DataType. (If the called function had potentially-applicable methods, calleesig will not be a subtype of any of their signatures.) corresponds to the root (though no entry with depth=0 is written), and sequential increases in depth indicate a traversal through branches. If depth decreases, this indicates the start of a new branch from the parent with depth depth-1.\n\n(callee::MethodInstance, tag) pairs that were directly affected by change in dispatch.\nPossibly,\n\nAfter all such callee branches are complete, the (trigger::Method, tag) event that initiated the entire set of invalidations pair is logged.\n\nThe interpretation of the tags is as follows:\n\n\"jl_method_table_disable\": the trigger with the same tag was deleted (Base.delete_method)\n\"jl_method_table_insert\": the trigger with the same tag was added (function f(...) end)\n(callee::MethodInstance, \"invalidate_mt_cache\"): a method-table cache for runtime dispatch was invalidated by a method insertion. At sites of runtime dispatch, Julia will maintain local method tables of the most common call targets to make dispatch more efficient. Since runtime dispatch involves real-time method lookup anyway, this form of invalidation is not serious, and a detailed listing is suppressed by SnoopCompile's printing behavior. These are always followed (eventually) by a (trigger::Method, \"jl_method_table_insert\") pair."},{"title":"edge logs","page":"Information for SnoopCompile developers","location":"explanations/devs/#edge-logs","category":"section","text":"Since edge logs are populated during package loading, we'll use PkgDep to indicate a package that is a dependency for PkgUser. (PkgUser's Project.toml might list PkgDep in its [deps] section, or it might be an indirect dependency.) Invalidation events result in the insertion of 3 or 4 items in logedges. The tag is always the second item. They take one of the following forms:\n\n(def::Method, \"method_globalref\", codeinst::CodeInstance, nothing): method def in PkgUser references PkgDep.SomeObject (which might be const data, a type, etc.), but the binding for SomeObject has been modified since PkgUser was compiled. codeinst, which holds a compiled specialization of def, needs to be recompiled.\n(edge::Union{MethodInstance,DataType,Core.Binding}, \"insert_backedges_callee\", codeinst::CodeInstance, matches::Union{Vector{Any},Nothing}): edge was selected as a dispatch target (a \"callee\") of codeinst, but new method(s) listed in matches now supersede it in dispatch specificity. There are 3 or 4 sub-cases:\nedge::MethodInstance indicates a known target at the time of compilation\nedge::DataType represents either\nTuple{typeof(f), argtypes...} for a poorly-inferred or invoked call for which the target selected at compilation time is no longer valid (matches will be nothing)\na signature of a known function for which no appropriate method had yet been defined at the time of compilation. matches lists methods that now apply.\nedge::Core.Binding indicates a target that was unknown at the time of compilation, and matches will be nothing.\n(caller::CodeInstance, \"verify_methods\", callee::CodeInstance): callee is an invalidated dependency of caller. These encode invalidations that cascade from the proximal source."},{"title":"SnoopCompile.jl","page":"SnoopCompile.jl","location":"#SnoopCompile.jl","category":"section","text":"Julia is fast, but its execution speed depends on optimizing code through compilation. Code must be compiled before you can use it, and unfortunately compilation is slow. This can cause latency the first time you use code: this latency is often called time-to-first-plot (TTFP) or more generally time-to-first-execution (TTFX). If something feels slow the first time you use it, and fast thereafter, you're probably experiencing the latency of compilation. Note that TTFX is distinct from time-to-load (TTL, which refers to the time you spend waiting for using MyPkg to finish), even though both contribute to latency.\n\nModern versions of Julia can store compiled code to disk (precompilation) to reduce or eliminate latency. Users and developers who are interested in reducing TTFX should first head to PrecompileTools, read its documentation thoroughly, and try using it to solve latency problems.\n\nThis package, SnoopCompile, should be considered when:\n\nprecompilation doesn't reduce TTFX as much as you wish\nprecompilation \"works,\" but only in isolation: as soon as you load (certain) additional packages, TTFX is bad again\nyou're wondering if you can reduce the amount of time needed to precompile your package and/or the size of the precompilation cache files\n\nIn other words, SnoopCompile is a diagonostic package that helps reveal the causes of latency. Historically, it proceeded PrecompileTools, and indeed PrecompileTools was split out from SnoopCompile. Today, SnoopCompile is generally needed only when PrecompileTools fails to deliver the desired benefits."},{"title":"SnoopCompile analysis modes","page":"SnoopCompile.jl","location":"#SnoopCompile-analysis-modes","category":"section","text":"SnoopCompile \"snoops\" on the Julia compiler, collecting information that may be useful to developers. Here are some of the things you can do with SnoopCompile:\n\ndiagnose invalidations, cases where Julia must throw away previously-compiled code (see Tutorial on @snoop_invalidations)\ntrace inference, to learn what code is being newly (or freshly) analyzed in an early stage of the compilation pipeline (Tutorial on @snoop_inference)\ntrace code generation by LLVM, a late stage in the compilation pipeline (Tutorial on @snoop_llvm)\nreveal methods with excessive numbers of compiler-generated specializations, a.k.a.profile-guided despecialization (Tutorial on PGDS)\nintegrate with tools like JET to further reduce the risk that your lovingly-precompiled code will be invalidated by loading other packages (Tutorial on JET integration)"},{"title":"Background information","page":"SnoopCompile.jl","location":"#Background-information","category":"section","text":"If nothing else, you should know this:\n\ninvalidations occur when you load code (e.g., using MyPkg) or otherwise define new methods\ninference and other stages of compilation occur the first time you run code for a particular combination of input types\n\nThe individual tutorials briefly explain core concepts. More detail can be found in Understanding SnoopCompile and Julia's compilation pipeline."},{"title":"Who should use this package","page":"SnoopCompile.jl","location":"#Who-should-use-this-package","category":"section","text":"SnoopCompile is intended primarily for package developers who want to improve the experience for their users. It is also recommended for users who are willing to \"dig deep\" and understand why packages they depend on have high latency. Your experience with latency may be personal, as it can depend on the specific combination of packages you load. If latency troubles you, don't make the assumption that it must be unfixable: you might be the first person affected by that specific cause of latency."},{"title":"Precompilation \"gotcha\"s","page":"Precompilation \"gotcha\"s","location":"explanations/gotchas/#Precompilation-\"gotcha\"s","category":"section","text":""},{"title":"Running code during module definition","page":"Precompilation \"gotcha\"s","location":"explanations/gotchas/#running-during-pc","category":"section","text":"Suppose you're working on an astronomy package and your source code has a line\n\nconst planets = map(makeplanet, [\"Mercury\", ...])\n\nJulia will dutifully create planets and store it in the package's precompile cache file. This also runs makeplanet, and if this is the first time it gets run, it will compile makeplanet. Assuming that makeplanet is a method defined in the package, the compiled code for makeplanet will be stored in the cache file.\n\nHowever, two circumstances can lead to puzzling omissions from the cache files:\n\nif makeplanet is a method defined in a dependency of your package, it will not be cached in your package. You'd want to add precompilation of makeplanet to the package that creates that method.\nif makeplanet is poorly-infered and uses runtime dispatch, any such callees that are not owned by your package will not be cached. For example, suppose makeplanet ends up calling methods in Base Julia or its standard libraries that are not precompiled into Julia itself: the compiled code for those methods will not be added to the cache file.\n\nOne option to ensure this dependent code gets cached is to create planets inside PrecompileTools.@compile_workload:\n\n@compile_workload begin\n    global planets\n    const planet = map(makeplanet, [\"Mercury\", ...])\nend\n\nNote that your package definition can have multiple @compile_workload blocks."},{"title":"Using @snoop_inference results to improve inferrability","page":"Using @snoop_inference results to improve inferrability","location":"tutorials/snoop_inference_analysis/#inferrability","category":"section","text":"Throughout this page, we'll use the OptimizeMe demo, which ships with SnoopCompile.\n\nnote: Note\nTo understand what follows, it's essential to refer to OptimizeMe source code as you follow along.\n\nusing SnoopCompileCore, SnoopCompile # here we need the SnoopCompile path for the next line (normally you should wait until after data collection is complete)\ninclude(joinpath(pkgdir(SnoopCompile), \"examples\", \"OptimizeMe.jl\"))\ntinf = @snoop_inference OptimizeMe.main();\nfg = flamegraph(tinf)\n\nIf you visualize fg with ProfileView, you may see something like this:\n\n(Image: flamegraph-OptimizeMe)\n\nFrom the standpoint of precompilation, this has some obvious problems:\n\neven though we called a single method, OptimizeMe.main(), there are many distinct flames separated by blank spaces. This indicates that many calls are being made by runtime dispatch:  each separate flame is a fresh entrance into inference.\nseveral of the flames are marked in red, indicating that they are not naively precompilable (see the Tutorial on @snoop_inference). While @compile_workload can handle these flames, an even more robust solution is to eliminate them altogether.\n\nOur goal will be to improve the design of OptimizeMe to make it more readily precompilable."},{"title":"Analyzing inference triggers","page":"Using @snoop_inference results to improve inferrability","location":"tutorials/snoop_inference_analysis/#Analyzing-inference-triggers","category":"section","text":"We'll first extract the \"triggers\" of inference, which is just a repackaging of part of the information contained within tinf. Specifically an InferenceTrigger captures callee/caller relationships that straddle a fresh entrance to type-inference, allowing you to identify which calls were made by runtime dispatch and what MethodInstance they called.\n\nitrigs = inference_triggers(tinf)\n\nThe number of elements in this Vector{InferenceTrigger} tells you how many calls were (1) made by runtime dispatch and (2) the callee had not previously been inferred.\n\ntip: Tip\nIn the REPL, SnoopCompile displays InferenceTriggers with yellow coloration for the callee, red for the caller method, and blue for the caller specialization. This makes it easier to quickly identify the most important information.\n\nIn some cases, this might indicate that you'll need to fix each case separately; fortunately, in many cases fixing one problem addresses many other."},{"title":"Method triggers","page":"Using @snoop_inference results to improve inferrability","location":"tutorials/snoop_inference_analysis/#methtrigs","category":"section","text":"Most often, it's most convenient to organize them by the method triggering the need for inference:\n\nmtrigs = accumulate_by_source(Method, itrigs)\n\nThe methods triggering the largest number of inference runs are shown at the bottom. You can also select methods from a particular module:\n\nmodtrigs = filtermod(OptimizeMe, mtrigs)\n\nRather than filter by a single module, you can alternatively call SnoopCompile.parcel(mtrigs) to split them out by module. In this case, most of the triggers came from Base, not OptimizeMe. However, many of the failures in Base were nevertheless indirectly due to OptimizeMe: our methods in OptimizeMe call Base methods with arguments that trigger internal inference failures. Fortunately, we'll see that using more careful design in OptimizeMe can avoid many of those problems.\n\ntip: Tip\nIf you have a longer list of inference triggers than you feel comfortable tackling, filtering by your package's module or using precompile_blockers can be a good way to start. Fixing issues in the package itself can end up resolving many of the \"indirect\" triggers too. Also be sure to note the ability to filter out likely \"noise\" from test suites.\n\nYou can get an overview of each Method trigger with summary:\n\nmtrig = modtrigs[1]\nsummary(mtrig)\n\nYou can also say edit(mtrig) and be taken directly to the method you're analyzing in your editor. You can still \"dig deep\" into individual triggers:\n\nitrig = mtrig.itrigs[1]\n\nThis is useful if you want to analyze with Cthulhu.ascend. Method-based triggers, which may aggregate many different individual triggers, can be useful because tools like Cthulhu.jl show you the inference results for the entire MethodInstance, allowing you to fix many different inference problems at once."},{"title":"Trigger trees","page":"Using @snoop_inference results to improve inferrability","location":"tutorials/snoop_inference_analysis/#Trigger-trees","category":"section","text":"While method triggers are probably the most useful way of organizing these inference triggers, for learning purposes here we'll use a more detailed scheme, which organizes inference triggers in a tree:\n\nitree = trigger_tree(itrigs)\nusing AbstractTrees\nprint_tree(itree)\n\nThis gives you a big-picture overview of how the inference failures arose. The parent-child relationships are based on the backtraces at the entrance to inference, and the nodes are organized in the order in which inference occurred. Inspection of these trees can be informative; for example, here we notice a lot of method specializations for Container{T} for different T.\n\nWe're going to march through these systematically."},{"title":"suggest and fixing Core.Box","page":"Using @snoop_inference results to improve inferrability","location":"tutorials/snoop_inference_analysis/#suggest-and-fixing-Core.Box","category":"section","text":"You may have noticed above that summary(mtrig) generated a red has Core.Box message. Assuming that itrig is still the first (and it turns out, only) trigger from this method, let's look at this again, explicitly using suggest, the tool that generated this hint:\n\nsuggest(itrig)\n\nYou can see that SnoopCompile recommends tackling this first; depending on how much additional code is affected, fixing a  Core.Box allows inference to work better and may resolve other triggers.\n\nThis message also directs readers to a section of this documentation that links to a page of the Julia manual describing the underlying problem. The Julia documentation suggests a couple of fixes, of which the best (in this case) is to use the let statement to rebind the variable and end any \"conflict\" with the closure:\n\nfunction abmult(r::Int, ys)\n    if r < 0\n        r = -r\n    end\n    let r = r    # Julia #15276\n        return map(x -> howbig(r * x), ys)\n    end\nend"},{"title":"suggest and a fix involving manual eltype specification","page":"Using @snoop_inference results to improve inferrability","location":"tutorials/snoop_inference_analysis/#suggest-and-a-fix-involving-manual-eltype-specification","category":"section","text":"Let's look at the other Method-trigger rooted in OptimizeMe:\n\nmtrig = modtrigs[2]\nsummary(mtrig)\nitrig = mtrig.itrigs[1]\n\nIf you use Cthulhu's ascend(itrig) you might see something like this:\n\n(Image: ascend-lotsa)\n\nThe first thing to note here is that cs is inferred as an AbstractVector–fixing this to make it a concrete type should be our next goal. There's a second, more subtle hint: in the call menu at the bottom, the selected call is marked < semi-concrete eval >. This is a hint that a method is being called with a non-concrete type.\n\nWhat might that non-concrete type be?\n\nisconcretetype(OptimizeMe.Container)\n\nThe statement Container.(list) is thus creating an AbstractVector with a non-concrete element type. You can seem in greater detail what happens, inference-wise, in this snippet from print_tree(itree):\n\n   ├─ similar(::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Tuple{Base.OneTo{Int64}}, Type{Main.OptimizeMe.Container}, Tuple{Base.Broadcast.Extruded{Vector{Any}, Tuple{Bool}, Tuple{Int64}}}}, ::Type{Main.OptimizeMe.Container{Int64}})\n   ├─ setindex!(::Vector{Main.OptimizeMe.Container{Int64}}, ::Main.OptimizeMe.Container{Int64}, ::Int64)\n   ├─ Base.Broadcast.copyto_nonleaf!(::Vector{Main.OptimizeMe.Container{Int64}}, ::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Tuple{Base.OneTo{Int64}}, Type{Main.OptimizeMe.Container}, Tuple{Base.Broadcast.Extruded{Vector{Any}, Tuple{Bool}, Tuple{Int64}}}}, ::Base.OneTo{Int64}, ::Int64, ::Int64)\n   │  ├─ similar(::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Tuple{Base.OneTo{Int64}}, Type{Main.OptimizeMe.Container}, Tuple{Base.Broadcast.Extruded{Vector{Any}, Tuple{Bool}, Tuple{Int64}}}}, ::Type{Main.OptimizeMe.Container})\n   │  └─ Base.Broadcast.restart_copyto_nonleaf!(::Vector{Main.OptimizeMe.Container}, ::Vector{Main.OptimizeMe.Container{Int64}}, ::Base.Broadcast.Broadcasted\n\nIn rough terms, what this means is the following:\n\nsince the first item in list is an Int, the output initially gets created as a Vector{Container{Int}}\nhowever, copyto_nonleaf! runs into trouble when it goes to copy the second item, which is a Container{UInt8}\nhence, copyto_nonleaf! re-allocates the output array to be a generic Vector{Container} and then calls restart_copyto_nonleaf!.\n\nWe can prevent all this hassle with one simple change: rewrite that line as\n\ncs = Container{Any}.(list)\n\nWe use Container{Any} here because there is no more specific element type–other than an unreasonably-large Union–that can hold all the items in list.\n\nIf you make these edits manually, you'll see that we've gone from dozens of itrigs (38 on Julia 1.10, you may get a different number on other Julia versions) down to about a dozen (13 on Julia 1.10). Real progress!"},{"title":"Replacing hard-to-infer calls with lower-level APIs","page":"Using @snoop_inference results to improve inferrability","location":"tutorials/snoop_inference_analysis/#Replacing-hard-to-infer-calls-with-lower-level-APIs","category":"section","text":"We note that many of the remaining triggers are somehow related to show, for example:\n\nInference triggered to call show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMe.Container{Any}}) from #55 (/cache/build/builder-amdci4-0/julialang/julia-release-1-dot-10/usr/share/julia/stdlib/v1.10/REPL/src/REPL.jl:273) with specialization (::REPL.var\"#55#56\"{REPL.REPLDisplay{REPL.LineEditREPL}, MIME{Symbol(\"text/plain\")}, Base.RefValue{Any}})(::Any)\n\nIn this case we see that the calling method is #55.  This is a gensym, or generated symbol, indicating that the method was generated during Julia's lowering pass, and might indicate a macro, a do block or other anonymous function, the generator for a @generated function, etc.\n\nedit(itrig) (or equivalently, edit(node) where node is a child of itree) takes us to this method in Base, for which key lines are\n\nfunction display(d::REPLDisplay, mime::MIME\"text/plain\", x)\n    x = Ref{Any}(x)\n    with_repl_linfo(d.repl) do io\n        ⋮\n        show(io, mime, x[])\n        ⋮\nend\n\nThe generated method corresponds to the do block here. The call to show comes from show(io, mime, x[]). This implementation uses a clever trick, wrapping x in a Ref{Any}(x), to prevent specialization of the method defined by the do block on the specific type of x. This trick is designed to limit the number of MethodInstances inferred for this display method.\n\nA great option is to replace the call to display with an explicit\n\nshow(stdout, MIME(\"text/plain\"), cs)\n\nThere's one extra detail: the type of stdout is not fixed (and therefore not known), because one can use a terminal, a file, devnull, etc., as stdout. If you want to prevent all runtime dispatch from this call, you'd need to supply an io::IO object of known type as the first argument. It could, for example, be passed in to lotsa_containers from main:\n\nfunction lotsa_containers(io::IO)\n    ⋮\n    println(io, \"lotsa containers:\")\n    show(io, MIME(\"text/plain\"), cs)\nend\n\nHowever, if you want it to go to stdout–and to allow users to redirect stdout to a target of their choosing–then an io argument may have to be of unknown type when called from main."},{"title":"When you need to rely on @compile_workload","page":"Using @snoop_inference results to improve inferrability","location":"tutorials/snoop_inference_analysis/#When-you-need-to-rely-on-@compile_workload","category":"section","text":"Most of the remaining triggers are difficult to fix because they occur in deliberately-@nospecialized portions of Julia's internal code for displaying arrays. In such cases, adding a PrecompileTools.@compile_workload is your best option. Here we use an interesting trick:\n\n@compile_workload begin\n    lotsa_containers(devnull)  # use `devnull` to suppress output\n    abmult(rand(-5:5), rand(3))\nend\nprecompile(lotsa_containers, (Base.TTY,))\n\nDuring the workload, we pass devnull as the io object to lotsa_containers: this suppresses the output so you don't see anything during precompilation. However, devnull is not a Base.TTY, the standard type of stdout. Nevertheless, this is effective because we can see that many of the callees in the remaining inference-triggers do not depend on the io object.\n\nTo really ice the cake, we also add a manual precompile directive. (precompile doesn't execute the method, it just compiles it.) This doesn't \"step through\" runtime dispatch, but at least it precompiles the entry point. Thus, at least lotsa_containers will be precompiled for the most likely IO type encountered in practice.\n\nWith these changes, we've fixed nearly all the latency problems in OptimizeMe, and made it much less vulnerable to invalidation as well. You can see the final code in the OptimizeMeFixed source code. Note that this would have to be turned into a real package for the @compile_workload to have any effect."},{"title":"A note on analyzing test suites","page":"Using @snoop_inference results to improve inferrability","location":"tutorials/snoop_inference_analysis/#test-suites","category":"section","text":"If you're doing a package analysis, it's convenient to use the package's runtests.jl script as a way to cover much of the package's functionality. SnoopCompile has a couple of enhancements designed to make it easier to ignore inference triggers that come from the test suite itself. First, suggest.(itrigs) may show something like this:\n\n ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)\n\nThis indicates a broadcasting operation in the @testset itself. Second, while it's a little dangerous (because suggest cannot entirely be trusted), you can filter these out:\n\njulia> itrigsel = [itrig for itrig in itrigs if !isignorable(suggest(itrig))];\n\njulia> length(itrigs)\n222\n\njulia> length(itrigsel)\n71\n\nWhile there is some risk of discarding triggers that provide clues about the origin of other triggers (e.g., they would have shown up in the same branch of the trigger_tree), the shorter list may help direct your attention to the \"real\" issues."},{"title":"Tutorial on @snoop_inference","page":"Tutorial on @snoop_inference","location":"tutorials/snoop_inference/#Tutorial-on-@snoop_inference","category":"section","text":"Inference may occur when you run code. Inference is the first step of type-specialized compilation. @snoop_inference collects data on what inference is doing, giving you greater insight into what is being inferred and how long it takes.\n\nCompilation is needed only for \"fresh\" code; running the demos below on code you've already used will yield misleading results. When analyzing inference, you're advised to always start from a fresh session. See also the comparison between SnoopCompile and JET."},{"title":"Add SnoopCompileCore, SnoopCompile, and helper packages to your environment","page":"Tutorial on @snoop_inference","location":"tutorials/snoop_inference/#Add-SnoopCompileCore,-SnoopCompile,-and-helper-packages-to-your-environment","category":"section","text":"Here, we'll add these packages to your default environment. (With the exception of AbstractTrees, these \"developer tool\" packages should not be added to the Project file of any real packages unless you're extending the tool itself.)\n\nusing Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\", \"AbstractTrees\", \"ProfileView\"]);"},{"title":"Setting up the demo","page":"Tutorial on @snoop_inference","location":"tutorials/snoop_inference/#Setting-up-the-demo","category":"section","text":"To see @snoop_inference in action, we'll use the following demo:\n\nmodule FlattenDemo\n    struct MyType{T} x::T end\n\n    extract(y::MyType) = y.x\n\n    function domath(x)\n        y = x + x\n        return y*x + 2*x + 5\n    end\n\n    dostuff(y) = domath(extract(y))\n\n    function packintype(x)\n        y = MyType{Int}(x)\n        return dostuff(y)\n    end\nend\n\n# output\n\nMain.var\"Main\".FlattenDemo\n\nThe main call, packintype, stores the input in a struct, and then calls functions that extract the field value and performs arithmetic on the result."},{"title":"Collecting the data","page":"Tutorial on @snoop_inference","location":"tutorials/snoop_inference/#sccshow","category":"section","text":"To profile inference on this call, do the following:\n\njulia> using SnoopCompileCore\n\njulia> tinf = @snoop_inference FlattenDemo.packintype(1);\n\njulia> using SnoopCompile\n\njulia> tinf\nInferenceTimingNode: 0.000010/0.002443 on MethodInstance for Base.Compiler.Timings.ROOT() with 1 direct children\n\ntip: Tip\nDon't omit the semicolon on the tinf = @snoop_inference ... line, or you may get an enormous amount of output. The compact display on the final line is possible only because SnoopCompile defines nice Base.show methods for the data returned by @snoop_inference. These methods cannot be defined in SnoopCompileCore because it has a fundamental design constraint: loading SnoopCompileCore is not allowed to invalidate any code. Moving those Base.show methods to SnoopCompileCore would violate that guarantee.\n\nThis may not look like much, but there's a wealth of information hidden inside tinf."},{"title":"A quick check for potential invalidations","page":"Tutorial on @snoop_inference","location":"tutorials/snoop_inference/#A-quick-check-for-potential-invalidations","category":"section","text":"After running @snoop_inference, it's generally recommended to check the output of staleinstances:\n\njulia> staleinstances(tinf)\nSnoopCompileCore.InferenceTiming[]\n\nIf you see this, all's well. A non-empty list might indicate method invalidations, which can be checked (in a fresh session) using the tools described in Tutorial on @snoop_invalidations.\n\nIf you do have a lot of invalidations, precompile_blockers may be an effective way to reveal those invalidations that affect your particular package and workload."},{"title":"Viewing the results","page":"Tutorial on @snoop_inference","location":"tutorials/snoop_inference/#flamegraph","category":"section","text":"Let's start unpacking the output of @snoop_inference and see how to get more insight. First, notice that the output is an InferenceTimingNode: it's the root element of a tree of such nodes, all connected by caller-callee relationships. Indeed, this particular node is for Base.Compiler.Timings.ROOT(), a \"dummy\" node that is the root of all such trees.\n\nYou may have noticed that this ROOT node prints with two numbers. It will be easier to understand their meaning if we first display the whole tree. We can do that with the AbstractTrees package:\n\njulia> using AbstractTrees\n\njulia> print_tree(tinf, maxdepth=100)\nInferenceTimingNode: 0.000010/0.002443 on MethodInstance for Base.Compiler.Timings.ROOT() with 1 direct children\n└─ InferenceTimingNode: 0.001744/0.002433 on MethodInstance for Main.var\"Main\".FlattenDemo.packintype(::Int64) with 2 direct children\n   ├─ InferenceTimingNode: 0.000067/0.000070 on MethodInstance for MyType{Int64}(::Int64) with 0 direct children\n   └─ InferenceTimingNode: 0.000140/0.000415 on MethodInstance for Main.var\"Main\".FlattenDemo.dostuff(::MyType{Int64}) with 2 direct children\n      ├─ InferenceTimingNode: 0.000090/0.000124 on MethodInstance for Main.var\"Main\".FlattenDemo.extract(::MyType{Int64}) with 1 direct children\n      │  └─ InferenceTimingNode: 0.000027/0.000027 on MethodInstance for getproperty(::MyType{Int64}, ::Symbol) with 0 direct children\n      └─ InferenceTimingNode: 0.000073/0.000073 on MethodInstance for Main.var\"Main\".FlattenDemo.domath(::Int64) with 0 direct children\n\nThis tree structure reveals the caller-callee relationships, showing the specific types that were used for each MethodInstance. Indeed, as the calls to getproperty reveal, it goes beyond the types and even shows the results of constant propagation; the getproperty(::MyType{Int64}, x::Symbol) corresponds to y.x in the definition of extract.\n\nnote: Note\nGenerally we speak of call graphs rather than call trees. But because inference results are cached (a.k.a., we only \"visit\" each node once), we obtain a tree as a depth-first-search of the full call graph.\n\nYou can extract the MethodInstance with\n\njulia> Core.MethodInstance(tinf)\nMethodInstance for Base.Compiler.Timings.ROOT()\n\njulia> Core.MethodInstance(tinf.children[1])\nMethodInstance for Main.var\"Main\".FlattenDemo.packintype(::Int64)\n\nEach node in this tree is accompanied by a pair of numbers. The first number is the exclusive inference time (in seconds), meaning the time spent inferring the particular MethodInstance, not including the time spent inferring its callees. The second number is the inclusive time, which is the exclusive time plus the time spent on the callees. Therefore, the inclusive time is always at least as large as the exclusive time.\n\nThe ROOT node is a bit different: its exclusive time measures the time spent on all operations except inference. In this case, we see that the entire call took approximately 3.3ms, of which 2.7ms was spent on activities besides inference. Almost all of that was code-generation, but it also includes the time needed to run the code. Just 0.55ms was needed to run type-inference on this entire series of calls. As you will quickly discover, inference takes much more time on more complicated code.\n\nWe can also display this tree as a flame graph, using the ProfileView.jl package:\n\njulia> fg = flamegraph(tinf)\nNode(FlameGraphs.NodeData(ROOT() at typeinfer.jl:75, 0x00, 0:10080857))\n\njulia> using ProfileView\n\njulia> ProfileView.view(fg)\n\nYou should see something like this:\n\n(Image: flamegraph)\n\nUsers are encouraged to read the ProfileView documentation to understand how to interpret this, but briefly:\n\nthe horizontal axis is time (wide boxes take longer than narrow ones), the vertical axis is call depth\nhovering over a box displays the method that was inferred\nleft-clicking on a box causes the full MethodInstance to be printed in your REPL session\nright-clicking on a box opens the corresponding method in your editor\nctrl-click can be used to zoom in\nempty horizontal spaces correspond to activities other than type-inference\nany boxes colored red (there are none in this particular example, but you'll see some later) correspond to naively non-precompilable MethodInstances, in which the method is owned by one module but the types are from another unrelated module. Such MethodInstances are omitted from the precompile cache file unless they've been \"marked\" by PrecompileTools.@compile_workload or an explicit precompile directive. See SnoopCompile.isprecompilable for further explanation and some examples.\nany boxes colored orange-yellow (there is one in this demo) correspond to methods inferred for specific constants (constant propagation).\n\nYou can explore this flamegraph and compare it to the output from print_tree.\n\nnote: Note\nOrange-yellow boxes that appear at the base of a flame are worth special attention, and may represent something that you thought you had precompiled. For example, suppose your workload \"exercises\" myfun(args...; warn=true), so you might think you have myfun covered for the corresponding argument types. But constant-propagation (as indicated by the orange-yellow coloration) results in (re)compilation for specific values: if Julia has decided that myfun merits constant-propagation, a call myfun(args...; warn=false) might need to be compiled separately.When you want to prevent constant-propagation from hurting your TTFX, you have two options:precompile for all relevant argument values as well as types. The most common argument types to trigger Julia's constprop heuristics are numbers (Bool/Int/etc) and Symbol.\nDisable constant-propagation for this method by adding Base.@constprop :none in front of your definition of myfun. Constant-propagation can be a big performance boost when it changes how performance-sensitive code is optimized for specific input values, but when this doesn't apply you can safely disable it.\n\nFinally, flatten, on its own or together with accumulate_by_source, allows you to get an sense for the cost of individual MethodInstances or Methods.\n\nThe tools here allow you to get an overview of where inference is spending its time. This gives you insight into the major contributors to latency."},{"title":"Profile-guided despecialization","page":"Profile-guided despecialization","location":"tutorials/pgdsgui/#pgds","category":"section","text":"Julia's multiple dispatch allows developers to create methods for specific argument types. On top of this, the Julia compiler performs automatic specialization:\n\nfunction countnonzeros(A::AbstractArray)\n    ...\nend\n\nwill be compiled separately for Vector{Int}, Matrix{Float64}, SubArray{...}, and so on, if it gets called for each of these types. Each specialization (each MethodInstance with different argument types) costs extra inference and code-generation time, so while specialization often improves runtime performance, that has to be weighed against the cost in latency. There are also cases in which overspecialization can hurt both run-time and compile-time performance. Consequently, an analysis of specialization can be a powerful tool for improving package quality.\n\nSnoopCompile ships with an interactive tool, pgdsgui, short for \"Profile-guided despecialization.\" The name is a reference to a related technique, profile-guided optimization (PGO). Both PGO and PGDS use runtime profiling to help guide decisions about code optimization. PGO is often used in languages whose default mode is to avoid specialization, whereas PGDS seems more appropriate for a language like Julia which specializes by default. While PGO is sometimes an automatic part of the compiler that optimizes code midstream during execution, SnoopCompile's PGDS is a tool for making static changes (edits) to code. Again, this seems appropriate for a language where specialization typically happens prior to the first execution of the code."},{"title":"Add SnoopCompileCore, SnoopCompile, and helper packages to your environment","page":"Profile-guided despecialization","location":"tutorials/pgdsgui/#Add-SnoopCompileCore,-SnoopCompile,-and-helper-packages-to-your-environment","category":"section","text":"We'll add these packages to your default environment so you can use them while in the package environment:\n\nusing Pkg\nPkg.add([\"SnoopCompileCore\", \"SnoopCompile\", \"PyPlot\"]);\n\nPyPLot is used for the PGDS interface in part to reduce interference with native-Julia plotting packages like Makie–it's a little awkward to depend on a package that you might be simultaneously modifying!"},{"title":"Using the PGDS graphical user interface","page":"Profile-guided despecialization","location":"tutorials/pgdsgui/#Using-the-PGDS-graphical-user-interface","category":"section","text":"To illustrate the use of PGDS, we'll examine an example in which some methods get specialized for hundreds of types. To keep this example short, we'll create functions that operate on types themselves.\n\nnote: Note\nAs background to this example, for a DataType T, T.name returns a Core.TypeName, and T.name.name returns the name as a Symbol. Base.unwrap_unionall(T) preserves DataTypes as-is, but converts a UnionAll type into a DataType.\n\n\"\"\"\n    spelltype(T::Type)\n\nSpell out a type's name, one character at a time.\n\"\"\"\nfunction spelltype(::Type{T}) where T\n    name = Base.unwrap_unionall(T).name.name\n    str = \"\"\n    for c in string(name)\n        str *= c\n    end\n    return str\nend\n\n\"\"\"\n    mappushes!(f, dest, src)\n\nLike `map!` except it grows `dest` by one for each element in `src`.\n\"\"\"\nfunction mappushes!(f, dest, src)\n    for item in src\n        push!(dest, f(item))\n    end\n    return dest\nend\n\nmappushes(f, src) = mappushes!(f, [], src)\n\nThere are two stages to PGDS: first (and preferrably starting in a fresh Julia session), we profile type-inference:\n\njulia> using SnoopCompileCore\n\njulia> Ts = subtypes(Any);  # get a long list of different types\n\njulia> tinf = @snoop_inference mappushes(spelltype, Ts);\n\nThen, in the same session, profile the runtime:\n\njulia> using Profile\n\njulia> @profile mappushes(spelltype, Ts);\n\nTypically, it's best if the workload here is reflective of a \"real\" workload (test suites often are not), so that you get a realistic view of where your code spends its time during actual use.\n\nNow let's launch the PDGS GUI:\n\njulia> using SnoopCompile\n\njulia> import PyPlot        # the GUI is dependent on PyPlot, must load it before the next line\n\njulia> mref, ax = pgdsgui(tinf);\n\nYou should see something like this:\n\n(Image: pgdsgui)\n\nIn this graph, each dot corresponds to a single method; for this method, we plot inference time (vertical axis) against the run time (horizontal axis). The coloration of each dot encodes the number of specializations (the number of distinct MethodInstances) for that method; by default it even includes the number of times the method was inferred for specific constants (constant propagation), although you can exclude those cases using the consts=false keyword. Finally, the edge of each dot encodes the fraction of time spent on runtime dispatch (aka, type-instability), with black indicating 0% and bright red indicating 100%.\n\nIn this plot, we can see that no method runs for more than 0.01 seconds, whereas some methods have an aggregate inference time of up to 1s. Overall, inference-time dominates this plot. Moreover, for the most expensive cases, the number of specializations is in the hundreds or thousands.\n\nTo learn more about what is being specialized, just click on one of the dots; if you choose the upper-left dot (the one with highest inference time), you should see something like this in your REPL:\n\nspelltype(::Type{T}) where T in Main at REPL[1]:6 (586 specializations)\n\nThis tells you the method corresponding to this dot. Moreover, mref (one of the outputs of pgdsgui) holds this method:\n\njulia> mref[]\nspelltype(::Type{T}) where T in Main at REPL[1]:6\n\nWhat are the specializations, and how costly was each?\n\njulia> collect_for(mref[], tinf)\n586-element Vector{SnoopCompileCore.InferenceTimingNode}:\n InferenceTimingNode: 0.003486/0.020872 on InferenceFrameInfo for spelltype(::Type{T}) where T with 7 direct children\n InferenceTimingNode: 0.003281/0.003892 on InferenceFrameInfo for spelltype(::Type{AbstractArray}) with 2 direct children\n InferenceTimingNode: 0.003349/0.004023 on InferenceFrameInfo for spelltype(::Type{AbstractChannel}) with 2 direct children\n InferenceTimingNode: 0.000827/0.001154 on InferenceFrameInfo for spelltype(::Type{AbstractChar}) with 5 direct children\n InferenceTimingNode: 0.003326/0.004070 on InferenceFrameInfo for spelltype(::Type{AbstractDict}) with 2 direct children\n InferenceTimingNode: 0.000833/0.001159 on InferenceFrameInfo for spelltype(::Type{AbstractDisplay}) with 5 direct children\n⋮\n InferenceTimingNode: 0.000848/0.001160 on InferenceFrameInfo for spelltype(::Type{YAML.Span}) with 5 direct children\n InferenceTimingNode: 0.000838/0.001148 on InferenceFrameInfo for spelltype(::Type{YAML.Token}) with 5 direct children\n InferenceTimingNode: 0.000833/0.001150 on InferenceFrameInfo for spelltype(::Type{YAML.TokenStream}) with 5 direct children\n InferenceTimingNode: 0.000809/0.001126 on InferenceFrameInfo for spelltype(::Type{YAML.YAMLDocIterator}) with 5 direct children\n\nSo we can see that one MethodInstance for each type in Ts was generated.\n\nIf you see a list of MethodInstances, and the first is extremely costly in terms of inclusive time, but all the rest are not, then you might not need to worry much about over-specialization: your inference time will be dominated by that one costly method (often, the first time the method was called), and the fact that lots of additional specializations were generated may not be anything to worry about. However, in this case, the distribution of time is fairly flat, each contributing a small portion to the overall time. In such cases, over-specialization may be a problem."},{"title":"Reducing specialization with @nospecialize","page":"Profile-guided despecialization","location":"tutorials/pgdsgui/#Reducing-specialization-with-@nospecialize","category":"section","text":"How might we change this? To reduce the number of specializations of spelltype, we use @nospecialize in its definition:\n\nfunction spelltype(@nospecialize(T::Type))\n    name = Base.unwrap_unionall(T).name.name\n    str = \"\"\n    for c in string(name)\n        str *= c\n    end\n    return str\nend\n\nwarning: Warning\nwhere type-parameters force specialization: in spelltype(@nospecialize(::Type{T})) where T, the @nospecialize has no impact and you'll get full specialization on T. Instead, use @nospecialize(T::Type) (without the where statement) as shown.\n\nIf we now rerun that demo, you should see a plot of the same kind as shown above, but with different costs for each dot. The differences are best appreciated comparing them side-by-side (pgdsgui allows you to specify a particular axis into which to plot):\n\n(Image: pgdsgui-compare)\n\nThe results with @nospecialize are shown on the right. You can see that:\n\nNow, the most expensive-to-infer method is <0.01s (formerly it was ~1s)\nNo method has more than 2 specializations\n\nMoreover, our runtimes (post-compilation) really aren't very different, both in the ballpark of a few millseconds (you can check with @btime from BenchmarkTools to be sure).\n\nIn total, we've reduced compilation time approximately 50× without appreciably hurting runtime performance. Reducing specialization, when appropriate, can often yield your biggest reductions in latency.\n\ntip: Tip\nWhen you add @nospecialize, sometimes it's beneficial to compensate for the loss of inferrability by adding some type assertions. This topic will be discussed in greater detail in the next section, but for the example above we can improve runtime performance by annotating the return type of Base.unwrap_unionall(T): name = (Base.unwrap_unionall(T)::DataType).name.name. Then, later lines in spelltype know that name is a Symbol.With this change, the unspecialized variant outperforms the specialized variant in both compile-time and run-time. The reason is that the specialized variant of spell needs to be called by runtime dispatch, whereas for the unspecialized variant there's only one MethodInstance, so its dispatch is handled at compile time."},{"title":"Blocking inference: Base.@nospecializeinfer","page":"Profile-guided despecialization","location":"tutorials/pgdsgui/#Blocking-inference:-Base.@nospecializeinfer","category":"section","text":"Perhaps surprisingly, @nospecialize doesn't prevent Julia's type-inference from inspecting a method. The reason is that it's sometimes useful if the caller knows what type will be returned, even if the callee doesn't exploit this information. In our mappushes example, this isn't an issue, because Ts is a Vector{Any} and this already defeats inference. But in other cases, the caller may be inferable but (to save inference time) you'd prefer to block inference from inspecting the method.\n\nBeginning with Julia 1.10, you can prevent even inference from \"looking at\" @nospecialized arguments with Base.@nospecializeinfer:\n\nBase.@nospecializeinfer function spelltype(@nospecialize(T::Type))\n    name = (Base.unwrap_unionall(T)::DataType).name.name\n    str = \"\"\n    for c in string(name)\n        str *= c\n    end\n    return str\nend\n\nNote that the ::DataType annotation described in the tip above is still effective and recommended. @nospecializeinfer directly affects only arguments that are marked with @nospecialize, and in this case the type-assertion prevents type uncertainty from propagating to the remainder of the function."},{"title":"Argument standardization","page":"Profile-guided despecialization","location":"tutorials/pgdsgui/#Argument-standardization","category":"section","text":"While not immediately relevant to the example above, a very important technique that falls within the domain of reducing specialization is argument standardization: instead of\n\nfunction foo(x, y)\n    # some huge function, slow to compile, and you'd prefer not to compile it many times for different types of x and y\nend\n\nconsider whether you can safely write this as\n\nfunction foo(x::X, y::Y)   # X and Y are concrete types\n    # some huge function, but the concrete typing ensures you only compile it once\nend\nfoo(x, y) = foo(convert(X, x)::X, convert(Y, y)::Y)   # this allows you to still call it with any argument types\n\nThe \"standardizing method\" foo(x, y) is short and therefore quick to compile, so it doesn't really matter if you compile many different instances.\n\ntip: Tip\nIn convert(X, x)::X, the final ::X guards against a broken convert method that fails to return an object of type X. Without it, foo(x, y) might call itself in an infinite loop, ultimately triggering a StackOverflowError. StackOverflowErrors are a particularly nasty form of error, and the typeassert ensures that you get a simple TypeError instead.In other contexts, such typeasserts would also have the effect of fixing inference problems even if the type of x is not well-inferred, but in this case dispatch to foo(x::X, y::Y) would have ensured the same outcome.\n\nThere are of course cases where you can't implement your code in this way: after all, part of the power of Julia is the ability of generic methods to \"do the right thing\" for a wide variety of types. But in cases where you're doing a standard task, e.g., writing some data to a file, there's really no good reason to recompile your save method for a filename encoded as a String and again for a SubString{String} and again for a SubstitutionString and again for an AbstractString and ...: after all, the core of the save method probably isn't sensitive to the precise encoding of the filename.  In such cases, it should be safe to convert all filenames to String, thereby reducing the diversity of input arguments for expensive-to-compile methods.\n\nIf you're using pgdsgui, the cost of inference and the number of specializations may guide you to click on specific dots; collect_for(mref[], tinf) then allows you to detect and diagnose cases where argument standardization might be helpful.\n\nYou can do the same analysis without pgdsgui. The opportunity for argument standardization is often facilitated by looking at, e.g.,\n\njulia> tms = accumulate_by_source(flatten(tinf));  # collect all MethodInstances that belong to the same Method\n\njulia> t, m = tms[end-1]        # the ones towards the end take the most time, maybe they are over-specialized?\n(0.4138147, save(filename::AbstractString, data) in SomePkg at /pathto/SomePkg/src/SomePkg.jl:23)\n\njulia> methodinstances(m)       # let's see what specializations we have\n7-element Vector{Core.MethodInstance}:\n MethodInstance for save(::String, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::SubString{String}, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::AbstractString, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::String, ::Vector{SomePkg.SomeDataType{SubString{String}}})\n MethodInstance for save(::SubString{String}, ::Array)\n MethodInstance for save(::String, ::Vector{var\"#s92\"} where var\"#s92\"<:SomePkg.SomeDataType)\n MethodInstance for save(::String, ::Array)\n\nIn this case we have 7 MethodInstances (some of which are clearly due to poor inferrability of the caller) when one might suffice."}]
}
