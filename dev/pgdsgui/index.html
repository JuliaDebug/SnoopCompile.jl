<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Profile-guided despecialization · SnoopCompile</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">SnoopCompile</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">SnoopCompile.jl</a></li><li><a class="tocitem" href="../snoop_pc/">SnoopPrecompile</a></li><li><a class="tocitem" href="../tutorial/">Tutorial on the foundations</a></li><li><span class="tocitem">Modern tools</span><ul><li><a class="tocitem" href="../snoopr/">Snooping on and fixing invalidations: <code>@snoopr</code></a></li><li><a class="tocitem" href="../snoopi_deep/">Snooping on inference: <code>@snoopi_deep</code></a></li><li class="is-active"><a class="tocitem" href>Profile-guided despecialization</a><ul class="internal"><li><a class="tocitem" href="#Using-the-PGDS-graphical-user-interface"><span>Using the PGDS graphical user interface</span></a></li></ul></li><li><a class="tocitem" href="../snoopi_deep_analysis/">Using <code>@snoopi_deep</code> results to improve inferrability</a></li><li><a class="tocitem" href="../snoopi_deep_parcel/">Using <code>@snoopi_deep</code> results for precompilation</a></li><li><a class="tocitem" href="../jet/">JET integration</a></li></ul></li><li><span class="tocitem">Older tools</span><ul><li><a class="tocitem" href="../snoopi/">Snooping on inference: <code>@snoopi</code></a></li><li><a class="tocitem" href="../snoopc/">Snooping on code generation: <code>@snoopc</code></a></li></ul></li><li><a class="tocitem" href="../userimg/">Creating <code>userimg.jl</code> files</a></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Modern tools</a></li><li class="is-active"><a href>Profile-guided despecialization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Profile-guided despecialization</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/timholy/SnoopCompile.jl/blob/master/docs/src/pgdsgui.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="pgds"><a class="docs-heading-anchor" href="#pgds">Profile-guided despecialization</a><a id="pgds-1"></a><a class="docs-heading-anchor-permalink" href="#pgds" title="Permalink"></a></h1><p>As indicated in the <a href="../#workflow">workflow</a>, one of the important early steps is to evaluate and potentially adjust method specialization. Each specialization (each <code>MethodInstance</code> with different argument types) costs extra inference and code-generation time, so while specialization often improves runtime performance, that has to be weighed against the cost in latency. There are also cases in which <a href="https://docs.julialang.org/en/v1/manual/performance-tips/#The-dangers-of-abusing-multiple-dispatch-(aka,-more-on-types-with-values-as-parameters)">overspecialization can hurt both run-time and compile-time performance</a>. Consequently, an analysis of specialization can be a powerful tool for improving package quality.</p><p><code>SnoopCompile</code> ships with an interactive tool, <a href="../reference/#SnoopCompile.pgdsgui"><code>pgdsgui</code></a>, short for &quot;Profile-guided despecialization.&quot; The name is a reference to a related technique, <a href="https://en.wikipedia.org/wiki/Profile-guided_optimization">profile-guided optimization</a> (PGO). Both PGO and PGDS use runtime profiling to help guide decisions about code optimization. PGO is often used in languages whose default mode is to avoid specialization, whereas PGDS seems more appropriate for a language like Julia which specializes by default. While PGO is sometimes an automatic part of the compiler that optimizes code midstream during execution, PGDS is a tool for making static changes in code. Again, this seems appropriate for a language where specialization typically happens prior to the first execution of the code.</p><h2 id="Using-the-PGDS-graphical-user-interface"><a class="docs-heading-anchor" href="#Using-the-PGDS-graphical-user-interface">Using the PGDS graphical user interface</a><a id="Using-the-PGDS-graphical-user-interface-1"></a><a class="docs-heading-anchor-permalink" href="#Using-the-PGDS-graphical-user-interface" title="Permalink"></a></h2><p>To illustrate the use of PGDS, we&#39;ll examine an example in which some methods get specialized for hundreds of types. To keep this example short, we&#39;ll create functions that operate on types themselves.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>For a <code>DataType</code> <code>T</code>, <code>T.name</code> returns a <code>Core.TypeName</code>, and <code>T.name.name</code> returns the name as a <code>Symbol</code>. <code>Base.unwrap_unionall(T)</code> preserves <code>DataType</code>s as-is, but converts a <code>UnionAll</code> type into a <code>DataType</code>.</p></div></div><pre><code class="language-julia hljs">&quot;&quot;&quot;
    spelltype(T::Type)

Spell out a type&#39;s name, one character at a time.
&quot;&quot;&quot;
function spelltype(::Type{T}) where T
    name = Base.unwrap_unionall(T).name.name
    str = &quot;&quot;
    for c in string(name)
        str *= c
    end
    return str
end

&quot;&quot;&quot;
    mappushes!(f, dest, src)

Like `map!` except it grows `dest` by one for each element in `src`.
&quot;&quot;&quot;
function mappushes!(f, dest, src)
    for item in src
        push!(dest, f(item))
    end
    return dest
end

mappushes(f, src) = mappushes!(f, [], src)</code></pre><p>There are two stages to PGDS: first (and preferrably starting in a fresh Julia session), we profile type-inference:</p><pre><code class="language-julia hljs">julia&gt; using SnoopCompile

julia&gt; Ts = subtypes(Any);  # get a long list of different types

julia&gt; tinf = @snoopi_deep mappushes(spelltype, Ts)
InferenceTimingNode: 4.476700/5.591207 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 587 direct children</code></pre><p>Then, <em>in the same session</em>, profile the runtime:</p><pre><code class="nohighlight hljs">julia&gt; using Profile

julia&gt; @profile mappushes(spelltype, Ts);</code></pre><p>Typically, it&#39;s best if the workload here is reflective of a &quot;real&quot; workload (test suites often are not), so that you get a realistic view of where your code spends its time during actual use.</p><p>Now let&#39;s launch the PDGS GUI:</p><pre><code class="language-julia hljs">julia&gt; import PyPlot        # the GUI is dependent on PyPlot, must load it before the next line

julia&gt; mref, ax = pgdsgui(tinf);</code></pre><p>You should see something like this:</p><p><img src="../assets/pgds_spec.png" alt="pgdsgui"/></p><p>In this graph, each dot corresponds to a single method; for this method, we plot inference time (vertical axis) against the run time (horizontal axis). The coloration of each dot encodes the number of specializations (the number of distinct <code>MethodInstance</code>s) for that method; by default it even includes the number of times the method was inferred for specific constants (<a href="https://en.wikipedia.org/wiki/Constant_folding">constant propagation</a>), although you can exclude those cases using the <code>consts=false</code> keyword. Finally, the edge of each dot encodes the fraction of time spent on runtime dispatch (aka, type-instability), with black indicating 0% and bright red indicating 100%.</p><p>In this plot, we can see that no method runs for more than 0.01 seconds, whereas some methods have an aggregate inference time of up to 1s. Overall, inference-time dominates this plot. Moreover, for the most expensive cases, the number of specializations is in the hundreds or thousands.</p><p>To learn more about <em>what</em> is being specialized, just click on one of the dots; if you choose the upper-left dot (the one with highest inference time), you should see something like this in your REPL:</p><pre><code class="language-julia hljs">spelltype(::Type{T}) where T in Main at REPL[1]:6 (586 specializations)</code></pre><p>This tells you the method corresponding to this dot. Moreover, <code>mref</code> (one of the outputs of <code>pgdsgui</code>) holds this method:</p><pre><code class="language-julia hljs">julia&gt; mref[]
spelltype(::Type{T}) where T in Main at REPL[1]:6</code></pre><p>What are the specializations, and how costly was each?</p><pre><code class="language-julia hljs">julia&gt; collect_for(mref[], tinf)
586-element Vector{SnoopCompileCore.InferenceTimingNode}:
 InferenceTimingNode: 0.003486/0.020872 on InferenceFrameInfo for spelltype(::Type{T}) where T with 7 direct children
 InferenceTimingNode: 0.003281/0.003892 on InferenceFrameInfo for spelltype(::Type{AbstractArray}) with 2 direct children
 InferenceTimingNode: 0.003349/0.004023 on InferenceFrameInfo for spelltype(::Type{AbstractChannel}) with 2 direct children
 InferenceTimingNode: 0.000827/0.001154 on InferenceFrameInfo for spelltype(::Type{AbstractChar}) with 5 direct children
 InferenceTimingNode: 0.003326/0.004070 on InferenceFrameInfo for spelltype(::Type{AbstractDict}) with 2 direct children
 InferenceTimingNode: 0.000833/0.001159 on InferenceFrameInfo for spelltype(::Type{AbstractDisplay}) with 5 direct children
⋮
 InferenceTimingNode: 0.000848/0.001160 on InferenceFrameInfo for spelltype(::Type{YAML.Span}) with 5 direct children
 InferenceTimingNode: 0.000838/0.001148 on InferenceFrameInfo for spelltype(::Type{YAML.Token}) with 5 direct children
 InferenceTimingNode: 0.000833/0.001150 on InferenceFrameInfo for spelltype(::Type{YAML.TokenStream}) with 5 direct children
 InferenceTimingNode: 0.000809/0.001126 on InferenceFrameInfo for spelltype(::Type{YAML.YAMLDocIterator}) with 5 direct children</code></pre><p>So we can see that one <code>MethodInstance</code> for each type in <code>Ts</code> was generated.</p><p>If you see a list of <code>MethodInstance</code>s, and the first is extremely costly in terms of inclusive time, but all the rest are not, then you might not need to worry much about over-specialization: your inference time will be dominated by that one costly method (often, the first time the method was called), and the fact that lots of additional specializations were generated may not be anything to worry about. However, in this case, the distribution of time is fairly flat, each contributing a small portion to the overall time. In such cases, over-specialization may be a problem.</p><h3 id="Reducing-specialization-with-@nospecialize"><a class="docs-heading-anchor" href="#Reducing-specialization-with-@nospecialize">Reducing specialization with <code>@nospecialize</code></a><a id="Reducing-specialization-with-@nospecialize-1"></a><a class="docs-heading-anchor-permalink" href="#Reducing-specialization-with-@nospecialize" title="Permalink"></a></h3><p>How might we change this? To reduce the number of specializations of <code>spelltype</code>, we use <code>@nospecialize</code> in its definition:</p><pre><code class="language-julia hljs">function spelltype(@nospecialize(T::Type))
    name = Base.unwrap_unionall(T).name.name
    str = &quot;&quot;
    for c in string(name)
        str *= c
    end
    return str
end</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>where</code> type-parameters force specialization, regardless of <code>@nospecialize</code>: in <code>spelltype(@nospecialize(::Type{T})) where T</code>, the <code>@nospecialize</code> has no impact and you&#39;ll get full specialization on <code>T</code>. Instead, use <code>@nospecialize(T::Type)</code> as shown.</p></div></div><p>If we now rerun that demo, you should see a plot of the same kind as shown above, but with different costs for each dot. The differences are best appreciated comparing them side-by-side (<a href="../reference/#SnoopCompile.pgdsgui"><code>pgdsgui</code></a> allows you to specify a particular axis into which to plot):</p><p><img src="../assets/pgds_compareplots.png" alt="pgdsgui-compare"/></p><p>The results with <code>@nospecialize</code> are shown on the right. You can see that:</p><ul><li>Now, the most expensive-to-infer method is &lt;0.01s (formerly it was ~1s)</li><li>No method has more than 2 specializations</li></ul><p>Moreover, our runtimes (post-compilation) really aren&#39;t very different, both in the ballpark of a few millseconds (you can check with <code>@btime</code> from BenchmarkTools to be sure).</p><p>In total, we&#39;ve reduced compilation time approximately 50× without appreciably hurting runtime performance. Reducing specialization, when appropriate, can often yield your biggest reductions in latency.</p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>When you add <code>@nospecialize</code>, sometimes it&#39;s beneficial to compensate for the loss of inferrability by adding some type assertions. This topic will be discussed in greater detail in the next section, but for the example above we can improve runtime performance by annotating the return type of <code>Base.unwrap_unionall(T)</code>: <code>name = (Base.unwrap_unionall(T)::DataType).name.name</code>. Then, later lines in <code>spell</code> know that <code>name</code> is a <code>Symbol</code>.</p><p>With this change, the unspecialized variant outperforms the specialized variant in <em>both compile-time and run-time</em>. The reason is that the specialized variant of <code>spell</code> needs to be called by runtime dispatch, whereas for the unspecialized variant there&#39;s only one <code>MethodInstance</code>, so its dispatch is handled at compile time.</p></div></div><h3 id="Argument-standardization"><a class="docs-heading-anchor" href="#Argument-standardization">Argument standardization</a><a id="Argument-standardization-1"></a><a class="docs-heading-anchor-permalink" href="#Argument-standardization" title="Permalink"></a></h3><p>While not immediately relevant to the example above, a very important technique that falls within the domain of reducing specialization is <em>argument standardization</em>: instead of</p><pre><code class="language-julia hljs">function foo(x, y)
    # some huge function, slow to compile, and you&#39;d prefer not to compile it many times for different types of x and y
end</code></pre><p>consider whether you can safely write this as</p><pre><code class="language-julia hljs">function foo(x::X, y::Y)   # X and Y are concrete types
    # some huge function, but the concrete typing ensures you only compile it once
end
foo(x, y) = foo(convert(X, x)::X, convert(Y, y)::Y)   # this allows you to still call it with any argument types</code></pre><p>The &quot;standardizing method&quot; <code>foo(x, y)</code> is short and therefore quick to compile, so it doesn&#39;t really matter if you compile many different instances.</p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>In <code>convert(X, x)::X</code>, the final <code>::X</code> guards against a broken <code>convert</code> method that fails to return an object of type <code>X</code>. Without it, <code>foo(x, y)</code> might call itself in an infinite loop, ultimately triggering a StackOverflowError. StackOverflowErrors are a particularly nasty form of error, and the typeassert ensures that you get a simple <code>TypeError</code> instead.</p><p>In other contexts, such typeasserts would also have the effect of fixing inference problems even if the type of <code>x</code> is not well-inferred (this will be discussed in more detail <a href="../snoopi_deep_analysis/#typeasserts">later</a>), but in this case dispatch to <code>foo(x::X, y::Y)</code> would have ensured the same outcome.</p></div></div><p>There are of course cases where you can&#39;t implement your code in this way: after all, part of the power of Julia is the ability of generic methods to &quot;do the right thing&quot; for a wide variety of types. But in cases where you&#39;re doing a standard task, e.g., writing some data to a file, there&#39;s really no good reason to recompile your <code>save</code> method for a filename encoded as a <code>String</code> and again for a <code>SubString{String}</code> and again for a <code>SubstitutionString</code> and again for an <code>AbstractString</code> and ...: after all, the core of the <code>save</code> method probably isn&#39;t sensitive to the precise encoding of the filename.  In such cases, it should be safe to convert all filenames to <code>String</code>, thereby reducing the diversity of input arguments for expensive-to-compile methods.</p><p>If you&#39;re using <code>pgdsgui</code>, the cost of inference and the number of specializations may guide you to click on specific dots; <code>collect_for(mref[], tinf)</code> then allows you to detect and diagnose cases where argument standardization might be helpful.</p><p>You can do the same analysis without <code>pgdsgui</code>. The opportunity for argument standardization is often facilitated by looking at, e.g.,</p><pre><code class="language-julia hljs">julia&gt; tms = accumulate_by_source(flatten(tinf));  # collect all MethodInstances that belong to the same Method

julia&gt; t, m = tms[end-1]        # the ones towards the end take the most time, maybe they are over-specialized?
(0.4138147, save(filename::AbstractString, data) in SomePkg at /pathto/SomePkg/src/SomePkg.jl:23)

julia&gt; methodinstances(m)       # let&#39;s see what specializations we have
7-element Vector{Core.MethodInstance}:
 MethodInstance for save(::String, ::Vector{SomePkg.SomeDataType})
 MethodInstance for save(::SubString{String}, ::Vector{SomePkg.SomeDataType})
 MethodInstance for save(::AbstractString, ::Vector{SomePkg.SomeDataType})
 MethodInstance for save(::String, ::Vector{SomePkg.SomeDataType{SubString{String}}})
 MethodInstance for save(::SubString{String}, ::Array)
 MethodInstance for save(::String, ::Vector{var&quot;#s92&quot;} where var&quot;#s92&quot;&lt;:SomePkg.SomeDataType)
 MethodInstance for save(::String, ::Array)</code></pre><p>In this case we have 7 <code>MethodInstance</code>s (some of which are clearly due to poor inferrability of the caller) when one might suffice.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../snoopi_deep/">« Snooping on inference: <code>@snoopi_deep</code></a><a class="docs-footer-nextpage" href="../snoopi_deep_analysis/">Using <code>@snoopi_deep</code> results to improve inferrability »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Monday 22 January 2024 10:02">Monday 22 January 2024</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
